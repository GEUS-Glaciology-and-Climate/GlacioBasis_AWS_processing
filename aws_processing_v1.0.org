#+TITLE: AWS processing
#+AUTHOR: Signe Hillerup Larsen
#+EMAIL: shl@geus.dk
#+DATE: {{{time(%Y-%m-%d)}}}
#+DESCRIPTION: Getting the GlacioBasis raw data into usefull formats
#+KEYWORDS:
#+OPTIONS:   H:4 num:4 toc:nil \n:nil ::t |:t ^:{} -:t f:t *:t <:t
# g#+EXCLUDE_TAGS: noexport
#+ARCHIVE: ::* Archive

#+PROPERTY: header-args :session :noweb yes :eval yes
#+PROPERTY: header-args:python :session aws_python :noweb yes 

Kernel:
#+BEGIN_SRC sh
ipython kernel install --name "aws_processing" --user

#+END_SRC

ipython kernel install --name "aws_processing" --user

* Code to process all downloaded data

Overall workflow:

Updates done in the raw data, then everything needs to be done again.

#+BEGIN_SRC python
# Raw to L0
<<run_all_L0_zac_l>>
<<run_all_L0_zac_u>>
<<run_all_L0_zac_a>>
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC bash :results verbatim
# L0 to L0M
<<make_L0M_zac_l>>
<<make_L0M_zac_u>>
<<make_L0M_zac_a>>
#+END_SRC

#+RESULTS:
: ZAC_L data now has nead headers
: ZAC_U data now has nead headers
: ZAC_A data now has nead headers




#+BEGIN_SRC python 
# L0M to L1
<<convert_to_physical_values_workflow>>

# L1 to QC + databases
<<run_all_QC_filtering_steps_for_meterological_observations>>
#+END_SRC

#+RESULTS:
: None

TODO
- Clear out figures created in the QC process
- update nead

** Create L0 files: extract raw data and unify headers and correct special cases

Reading in raw files and writing them out as L0 files with nice headers and special case issues solved
*** Workflow for generating all L0 files

#+BEGIN_SRC python
<<run_all_L0_zac_l>>
<<run_all_L0_zac_u>>
<<run_all_L0_zac_a>>

#+END_SRC

#+RESULTS:
: Index(['rec', 'p', 't_1', 't_2', 'rh', 'wspd', 'wdir', 'wd_std', 'dsr', 'usr',
:        'dlr', 'ulr', 't_rad', 'z_boom', 'z_boom_q', 'z_stake', 'z_stake_q',
:        'z_pt', 't_i_1', 't_i_2', 't_i_3', 't_i_4', 't_i_5', 't_i_6', 't_i_7',
:        't_i_8', 'tilt_x', 'tilt_y', 'gps_time', 'gps_lat', 'gps_lon',
:        'gps_alt', 'gps_geoid', 'gps_q', 'gps_numsat', 'gps_hdop', 't_log',
:        'fan_dc'],
:       dtype='object'

*** Libraries
#+NAME: load_libraries
#+BEGIN_SRC python
from glob import glob
import pandas as pd
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
import os as os
#+END_SRC


*** Constants

#+NAME: define_constants
#+BEGIN_SRC python
mmHg2hPa = 1.33322368
#+END_SRC

*** working folders

#+NAME: define_working_folders
#+BEGIN_SRC python
raw = '/home/shl@geus.dk/OneDrive/projects/glaciobasis/data/aws_raw/'
destination = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0/'
#+END_SRC


*** Define which headers to pass forward
#+NAME: headers_to_pass_forward
#+BEGIN_SRC python
new_headers = ['rec', 'p', 't_1', 't_2', 'rh', 'wspd', 'wdir', 'wd_std', 'dsr', 'usr', 'dlr', 'ulr', 't_rad', 'z_boom', 'z_boom_q', 'z_stake', 'z_stake_q', 'z_pt', 't_i_1', 't_i_2', 't_i_3', 't_i_4', 't_i_5', 't_i_6', 't_i_7', 't_i_8', 'tilt_x', 'tilt_y', 'gps_time', 'gps_lat', 'gps_lon', 'gps_alt', 'gps_geoid', 'gps_q', 'gps_numsat', 'gps_hdop', 't_log', 'fan_dc']


#+END_SRC


*** Each station, year by year

**** Take a look at all the table mem files and find the long one

#+BEGIN_SRC python
<<load_libraries>>
station_id = 'zac_l'
ystart = '2017'
yend = '2018'

filelist = glob(raw + station_id + '/' + ystart + '-' + yend +'/*TableMem*')
for f in filelist:
    df = pd.read_csv(f, header=1,skiprows=[2,3],sep=',',engine='python')
    df.index = pd.to_datetime(df.TIMESTAMP)
    df = df.drop(['TIMESTAMP'], axis=1)
    df.index.name = 'time'
    df.sort_index(inplace=True)
    df = df.replace('NAN',np.NaN)
    #print(f)
    #print(df['AS_T_Avg'])
    #df['AS_T_Avg'].plot()
    
#+END_SRC



**** zac_l

#+NAME: run_all_L0_zac_l
#+BEGIN_SRC python
<<zac_l_2008_2010>>
<<zac_l_2010_2011>>
<<zac_l_2011_2014>>
<<zac_l_2012_2013>>
<<zac_l_2014_2015>>
<<zac_l_2015_2016>>
<<zac_l_2016_2017>>
<<zac_l_2017_2018>>
<<zac_l_2018_2020>>
<<zac_l_2020_2021>>
<<zac_l_2021_2022>>
#+END_SRC


***** zac_l_2008_2010


#+NAME: zac_l_2008_2010
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2008'
yend = '2010'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'ZACK M_TableMem.dat.backup'
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<read_named_TableMem_file>>
<<fix_headers_type0_SR50_switched>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 

df['p'] = df['p']*mmHg2hPa
# Radiation from mV to 10^-5 V
#variables = ['dsr','usr','dlr','ulr']
#df[variables] = df[variables].astype(float)
#sel = df.index > '2010-May-12 20:50:00' 
#df.loc[sel,variables] = df.loc[sel,variables]/100

df.to_csv(outfilename,sep=',', index=True)
#+END_SRC


Read in and concatenate the raw files

***** zac_l_2010_2011

#+NAME: zac_l_2010_2011
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2010'
yend = '2011'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13745.TableMem.dat'
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type0_SR50_switched>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
#df['tilt_x'] = df['tilt_x'].astype(float)/10 
#df['tilt_y'] = df['tilt_y'].astype(float)/10 
# Even though the header in the raw data says that the unit is in mV, the data looks like it is in V/100

df['p'] = df['p']*mmHg2hPa
# Radiation from mV to 10^-5 V
variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100

df.to_csv(outfilename,sep=',', index=True)
#+END_SRC


***** zac_l_2011_2014

#+NAME: zac_l_2011_2014
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2011'
yend = '2014'

yend1 = '2012'
ystart2 = '2013'

outfilename1 = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend1 + '.csv'
outfilename2 = destination+station_id + '/' + station_id + '-' + ystart2 +'-' + yend + '.csv'
filename = 'ZACK M_TableMem.dat.7.backup'
<<read_named_TableMem_file>>

#<<read_and_concat_all_TableMem_files_in_raw_folder>>
df_orig = df

df=df_orig[:'17-April-2012'].copy()
<<fix_headers_type0_SR50_switched>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 
df['p'] = df['p']*mmHg2hPa
df.to_csv(outfilename1,sep=',', index=True)


df=df_orig['1-May-2013':].copy()
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 

df['p'] = df['p']*mmHg2hPa
df.to_csv(outfilename2,sep=',', index=True)


#+END_SRC


***** zac_l_2012_2013

#+NAME: zac_l_2012_2013
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2012'
yend = '2013'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13745.TableMem.dat'
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type0_SR50_switched>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 
df['p'] = df['p']*mmHg2hPa

df.to_csv(outfilename,sep=',', index=True)
#+END_SRC


***** zac_l_2014_2015

#+NAME: zac_l_2014_2015
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2014'
yend = '2015'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'ZACK M_TableMem.dat.14.backup'
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

df['p'] = df['p']*mmHg2hPa
# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 

df.to_csv(outfilename,sep=',', index=True)
#+END_SRC


***** zac_l_2015_2016

#+NAME: zac_l_2015_2016
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2015'
yend = '2016'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13745.TableMem.dat'
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

df['p'] = df['p']*mmHg2hPa
# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 
df.to_csv(outfilename,sep=',', index=True)
#+END_SRC


***** zac_l_2016_2017

#+NAME: zac_l_2016_2017
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2016'
yend = '2017'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13745.TableMem.dat'
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

df['p'] = df['p']*mmHg2hPa
# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 
df.to_csv(outfilename,sep=',', index=True)
#+END_SRC


***** zac_l_2017_2018

#+NAME: zac_l_2017_2018
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2017'
yend = '2018'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13745.TableMem.dat'
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

df['p'] = df['p']*mmHg2hPa
# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 
df.to_csv(outfilename,sep=',', index=True)
#+END_SRC


***** zac_l_2018_2020

#+NAME: zac_l_2018_2020
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2018'
yend = '2020'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13745.TableMem.dat'                                                               
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df['p'] = df['p']*mmHg2hPa
# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 

df.to_csv(outfilename,sep=',', index=True)
#+END_SRC


***** zac_l_2020_2021

#+NAME: zac_l_2020_2021
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2020'
yend = '2021'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'CR1000_nn_TableMem.dat.backup'
#<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<read_named_TableMem_file>>
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 
df['p'] = df['p']*mmHg2hPa

#print(df.keys())
df.to_csv(outfilename,sep=',', index=True)
#+END_SRC



***** zac_l_2021_2022

#+NAME: zac_l_2021_2022
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2021'
yend = '2022'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'zac-l_TableMem.dat'
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<read_named_TableMem_file>>
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 
df['p'] = df['p']*mmHg2hPa

#print(df.keys())
df.to_csv(outfilename,sep=',', index=True)
#+END_SRC



**** zac_u
#+NAME: run_all_L0_zac_u
#+BEGIN_SRC python
<<zac_u_2008_2010>>
<<zac_u_2010_2011>>
<<zac_u_2011_2012>>
<<zac_u_2012_2013>>
<<zac_u_2013_2014>>
<<zac_u_2014_2015>>
<<zac_u_2015_2016>>
<<zac_u_2016_2017>>
<<zac_u_2017_2019>>
<<zac_u_2019_2020>>
<<zac_u_2020_2021>>
<<zac_u_2021_2022>>
#+END_SRC

***** zac_u_2008_2010
#+NAME: zac_u_2008_2010
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2008'
yend = '2010'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'

<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type0>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data


df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC


***** zac_u_2010_2011
#+NAME: zac_u_2010_2011
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2010'
yend = '2011'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13744.TableMem.dat'

<<read_named_TableMem_file>>
<<fix_headers_type0>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC


***** zac_u_2011_2012
#+NAME: zac_u_2011_2012
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2011'
yend = '2012'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'ZACK S_TableMem.dat'

<<read_named_TableMem_file>>

df.index = df.index - pd.to_timedelta('1 day')

<<fix_headers_type0>>

df = df[new_headers] # pressure is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

# the unit of mmHg is converted to hPa
df['p'] = df['p']*mmHg2hPa

df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC


***** zac_u_2012_2013
#+NAME: zac_u_2012_2013
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2012'
yend = '2013'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
df.index = df.index - pd.to_timedelta('1 day')
<<fix_headers_type1>>

df = df[new_headers] # radiation and tilt is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC


***** zac_u_2013_2014
#+NAME: zac_u_2013_2014
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2013'
yend = '2014'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
df.index = df.index - pd.to_timedelta('1 day')

<<fix_headers_type1>>

df = df[new_headers] # radiation and tilt is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df['tilt_x'] = df['tilt_x'].astype(float)*10
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC


***** zac_u_2014_2015
#+NAME: zac_u_2014_2015
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2014'
yend = '2015'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type1>>

df = df[new_headers] # radiation and tilt is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data


variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC



***** zac_u_2015_2016
#+NAME: zac_u_2015_2016
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2015'
yend = '2016'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_E2101.TableMem.dat'
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<read_named_TableMem_file>>

<<fix_headers_type1>>

df = df[new_headers] # radiation and tilt is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df['tilt_x'] = df['tilt_x'].astype(float)*10
df['tilt_y'] = df['tilt_y'].astype(float)*10
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC


***** zac_u_2016_2017
#+NAME: zac_u_2016_2017
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2016'
yend = '2017'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type1>>

df = df[new_headers] # radiation and tilt is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC



***** zac_u_2017_2019
#+NAME: zac_u_2017_2019
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2017'
yend = '2019'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type1>>

df = df[new_headers] # pressure transducer is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC


***** zac_u_2019_2020
#+NAME: zac_u_2019_2020
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2019'
yend = '2020'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type1>>

df = df[new_headers] # thermistor is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC


***** zac_u_2020_2021
#+NAME: zac_u_2020_2021
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2020'
yend = '2021'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_E2101.TableMem.dat'

<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type1>>

df = df[new_headers] # thermistor is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC


***** zac_u_2021_2022
#+NAME: zac_u_2021_2022
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2021'
yend = '2022'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'zac-u_TableMem.dat'

<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type1>>

df = df[new_headers] # thermistor is added

new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC




**** zac_a
#+NAME: run_all_L0_zac_a
#+BEGIN_SRC python
<<zac_a_2009_2010>>
<<zac_a_2010_2011>>
<<zac_a_2011_2012>>
<<zac_a_2012_2013>>
<<zac_a_2013_2014>>
<<zac_a_2014_2015>>
<<zac_a_2015_2016>>
<<zac_a_2016_2017>>
<<zac_a_2017_2018>>
<<zac_a_2018_2019>>
#+END_SRC



***** zac_a_2009_2010
#+NAME: zac_a_2009_2010
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2009'
yend = '2010'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 

df.to_csv(outfilename,sep=',', index=True)

#+END_SRC


***** zac_a_2010_2011
#+NAME: zac_a_2010_2011
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2010'
yend = '2011'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)

df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC


***** zac_a_2011_2012
#+NAME: zac_a_2011_2012
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2011'
yend = '2012'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>
df.index = df.index - pd.to_timedelta('1 day')

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC


***** zac_a_2012_2013
#+NAME: zac_a_2012_2013
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2012'
yend = '2013'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)
#+END_SRC



***** zac_a_2013_2014
#+NAME: zac_a_2013_2014
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2013'
yend = '2014'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC


***** zac_a_2014_2015
#+NAME: zac_a_2014_2015
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2014'
yend = '2015'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

***** zac_a_2015_2016
#+NAME: zac_a_2015_2016
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2015'
yend = '2016'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC


***** zac_a_2016_2017
#+NAME: zac_a_2016_2017
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2016'
yend = '2017'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

***** zac_a_2017_2018
#+NAME: zac_a_2017_2018
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2017'
yend = '2018'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
#print(df.keys())
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC



***** zac_a_2018_2019
#+NAME: zac_a_2018_2019
#+BEGIN_SRC python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2018'
yend = '2019'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1a>>


df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC





**** Code
#+NAME: read_and_concat_all_TableMem_files_in_raw_folder
#+BEGIN_SRC python

filelist = glob(raw + station_id + '/' + ystart + '-' + yend +'/*TableMem*')
df = pd.concat((pd.read_csv(f, header=1,skiprows=[2,3],sep=',',engine='python') for f in filelist))
df.index = pd.to_datetime(df.TIMESTAMP)
df = df.drop(['TIMESTAMP'], axis=1)
df.index.name = 'time'
df.sort_index(inplace=True)
df = df.replace('NAN',np.NaN)


#+END_SRC

#+NAME: read_named_TableMem_file
#+BEGIN_SRC python
df = pd.read_csv(raw + station_id + '/' + ystart + '-' + yend +'/'+filename, header=1,skiprows=[2,3],sep=',',engine='python')
df.index = pd.to_datetime(df.TIMESTAMP)
df = df.drop(['TIMESTAMP'], axis=1)
df.index.name = 'time'
df.sort_index(inplace=True)
df = df.replace('NAN',np.NaN)
#+END_SRC


#+NAME: fix_headers_type0_SR50_switched
#+BEGIN_SRC python

df = df.rename(columns = {'TIMESTAMP':'time', 'RECORD':'rec', 'BP_mmHg_Avg':'p','AS_Pt100_Avg':'t_1', 'AS_T_Avg':'t_2', 'AS_RH_Avg':'rh', 'WS_ms_S_WVT':'wspd', 'WindDir_D1_WVT':'wdir', 'WindDir_SD1_WVT':'wd_std', 'CNR1_SWin_Avg':'dsr', 'CNR1_SWout_Avg':'usr', 'CNR1_LWin_Avg':'dlr', 'CNR1_LWout_Avg':'ulr','CNR1_Pt100_Avg':'t_rad', 'SnowHeight':'z_stake', 'SnowHeightQuality':'z_stake_q', 'Ablation':'z_boom', 'AblationQuality':'z_boom_q', 'Ablation_meter_Avg':'z_pt', 'Thermistor_1':'t_i_1', 'Thermistor_2':'t_i_2', 'Thermistor_3':'t_i_3', 'Thermistor_4':'t_i_4', 'Thermistor_5':'t_i_5', 'Thermistor_6':'t_i_6','Thermistor_7':'t_i_7', 'Thermistor_8':'t_i_8', 'Xtilt_Avg':'tilt_x', 'Ytilt_Avg':'tilt_y', 'TIME':'gps_time', 'LAT':'gps_lat', 'LONGI':'gps_lon', 'ALTDE':'gps_alt', 'GIODAL':'gps_geoid', 'QUAL':'gps_q', 'NUMSATS':'gps_numsat', 'HDP':'gps_hdop', 'PTemp_C_Avg':'t_log', 'Fan_current_avg':'fan_dc' })

#df = df.drop(columns = ['HEMINS','HEMIEW','ALTUNIT','GEOUNIT'])
#+END_SRC

#+NAME: fix_headers_type0
#+BEGIN_SRC python

df = df.rename(columns = {'TIMESTAMP':'time', 'RECORD':'rec', 'BP_mmHg_Avg':'p','AS_Pt100_Avg':'t_1', 'AS_T_Avg':'t_2', 'AS_RH_Avg':'rh', 'WS_ms_S_WVT':'wspd', 'WindDir_D1_WVT':'wdir', 'WindDir_SD1_WVT':'wd_std', 'CNR1_SWin_Avg':'dsr', 'CNR1_SWout_Avg':'usr', 'CNR1_LWin_Avg':'dlr', 'CNR1_LWout_Avg':'ulr','CNR1_Pt100_Avg':'t_rad', 'SnowHeight':'z_boom', 'SnowHeightQuality':'z_boom_q', 'Ablation':'z_stake', 'AblationQuality':'z_stake_q', 'Ablation_meter_Avg':'z_pt', 'Thermistor_1':'t_i_1', 'Thermistor_2':'t_i_2', 'Thermistor_3':'t_i_3', 'Thermistor_4':'t_i_4', 'Thermistor_5':'t_i_5', 'Thermistor_6':'t_i_6','Thermistor_7':'t_i_7', 'Thermistor_8':'t_i_8', 'Xtilt_Avg':'tilt_x', 'Ytilt_Avg':'tilt_y', 'TIME':'gps_time', 'LAT':'gps_lat', 'LONGI':'gps_lon', 'ALTDE':'gps_alt', 'GIODAL':'gps_geoid', 'QUAL':'gps_q', 'NUMSATS':'gps_numsat', 'HDP':'gps_hdop', 'PTemp_C_Avg':'t_log', 'Fan_current_avg':'fan_dc' })

#df = df.drop(columns = ['HEMINS','HEMIEW','ALTUNIT','GEOUNIT'])
#+END_SRC

#+NAME: fix_headers_type1
#+BEGIN_SRC python
df = df.rename(columns = {'RECORD':'rec', 'AirPressure_Avg':'p','Temperature_Avg':'t_1', 'Temperature2_Avg':'t_2', 'RelativeHumidity_Avg':'rh', 'WindSpeed':'wspd', 'WindDirection':'wdir', 'WindDirection_SD':'wd_std', 'ShortwaveRadiationIn_Avg':'dsr', 'ShortwaveRadiationOut_Avg':'usr','LongwaveRadiationIn_Avg':'dlr', 'LongwaveRadiationOut_Avg':'ulr','TemperatureRadSensor_Avg':'t_rad', 'SnowHeight':'z_boom', 'SnowHeightQuality':'z_boom_q', 'SurfaceHeight':'z_stake', 'SurfaceHeightQuality':'z_stake_q', 'IceHeight_Avg':'z_pt', 'TemperatureIce1m_Avg':'t_i_1', 'TemperatureIce2m_Avg':'t_i_2', 'TemperatureIce3m_Avg':'t_i_3', 'TemperatureIce4m_Avg':'t_i_4', 'TemperatureIce5m_Avg':'t_i_5', 'TemperatureIce6m_Avg':'t_i_6','TemperatureIce7m_Avg':'t_i_7', 'TemperatureIce10m_Avg':'t_i_8', 'TiltX_Avg':'tilt_x', 'TiltY_Avg':'tilt_y', 'TimeGPS':'gps_time', 'Latitude':'gps_lat', 'Longitude':'gps_lon', 'Altitude':'gps_alt', 'Giodal':'gps_geoid', 'Quality':'gps_q', 'NumberSatellites':'gps_numsat', 'HDOP':'gps_hdop', 'TemperatureLogger_Avg':'t_log', 'Fan_current_avg':'fan_dc' })
#+END_SRC

#+NAME: fix_headers_type1a
#+BEGIN_SRC python
df = df.rename(columns = {'RECORD':'rec', 'AirPressure_Avg':'p','Temperature_Avg':'t_1', 'Temperature2_Avg':'t_2', 'RelativeHumidity_Avg':'rh', 'WindSpeed':'wspd', 'WindDirection':'wdir', 'WindDirection_SD':'wd_std', 'ShortwaveRadiationIn_Avg':'dsr', 'ShortwaveRadiationOut_Avg':'usr','LongwaveRadiationIn_Avg':'dlr', 'LongwaveRadiationOut_Avg':'ulr','TemperatureRadSensor_Avg':'t_rad', 'SnowHeight':'z_boom', 'SnowHeightQuality':'z_boom_q', 'SurfaceHeight':'z_stake', 'SurfaceHeightQuality':'z_stake_q', 'IceHeight_Avg':'z_pt', 'TemperatureIce1m_Avg':'t_i_1', 'TemperatureIce2m_Avg':'t_i_2', 'TemperatureIce3m_Avg':'t_i_3', 'TemperatureIce4m_Avg':'t_i_4', 'TemperatureIce5m_Avg':'t_i_5', 'TemperatureIce6m_Avg':'t_i_6','TemperatureIce7m_Avg':'t_i_7', 'TemperatureIce10m_Avg':'t_i_8', 'TiltX_Avg':'tilt_x', 'TiltY_Avg':'tilt_y', 'TimeGPS':'gps_time', 'Latitude':'gps_lat', 'Longitude':'gps_lon', 'Altitude':'gps_alt', 'Giodal':'gps_geoid', 'Quality':'gps_q', 'NumberSatellites':'gps_numsat', 'HDOP':'gps_hdop', 'TemperatureLogger_Avg':'t_log', 'FanCurrent_Avg':'fan_dc' })
#+END_SRC


*** Take a look at L0 data 


**** zac_l
#+BEGIN_SRC python
<<load_libraries>>
folder = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0/'
station_id = 'zac_l'
variables = ['t_1', 'rh', 'usr']
fig, ax = plt.subplots(3,1, figsize = (10,10))
filelist = glob(folder+station_id+'/*')
df = pd.concat((pd.read_csv(f,index_col = 0, parse_dates = True,low_memory=False) for f in filelist))
df.sort_index(inplace=True)
df = df.drop(df.index[df.index < datetime(2007,1,1)])


for index,key in enumerate(variables):
    ax[index].plot(df[key])
    ax[index].set_title(key)
    ax[index].plot(df[key])
    ax[index].set_title(key)
    ax[index].plot(df[key])
    ax[index].set_title(key)


    
    

#+END_SRC



**** zac_u
#+BEGIN_SRC python
<<load_libraries>>
folder = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0/'
station_id = 'zac_u'
variables = ['t_1','p', 'rh', 'usr']
fig, ax = plt.subplots(4,1, figsize = (10,10))
filelist = glob(folder+station_id+'/*')
df = pd.concat((pd.read_csv(f,index_col = 0, parse_dates = True,low_memory=False) for f in filelist))
df.sort_index(inplace=True)
#df = df.drop(df.index[df.index < datetime(2007,1,1)])


for index,key in enumerate(variables):
    ax[index].plot(df[key])
    ax[index].set_title(key)
    ax[index].plot(df[key])
    ax[index].set_title(key)
    ax[index].plot(df[key])
    ax[index].set_title(key)


    
    

#+END_SRC


** Create L0M files: add nead header
L0 files are converted into files with the nead header - this is in order to ensure the correct meta data follows each file
The nead header template is filled out manually as 01-nead header files

# fields: time, rec, p, t_1, t_2, rh, wspd, wdir, wd_std, dsr, usr,dlr, ulr, t_rad, z_boom, z_boom_q, z_stake, z_stake_q, z_pt, t_i_1, t_i_2, t_i_3, t_i_4, t_i_5, t_i_6, t_i_7,t_i_8, tilt_x, tilt_y, gps_time, gps_lat, gps_lon,gps_alt, gps_geoid, gps_q, gps_numsat, gps_hdop, t_log, fan_dc


*** zac_l

concatenate the data with the nead header file, rename and put in L0M folder
#+NAME: make_L0M_zac_l
#+BEGIN_SRC bash :eval yes
station_id=zac_l
sourcedir=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0/$station_id
destdir=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/$station_id

years=2008-2010
endyear=2010 
<<concat_nead_and_data>>

years=2010-2011
endyear=2011 
<<concat_nead_and_data>>

years=2011-2012
endyear=2012 
<<concat_nead_and_data>>

years=2012-2013
endyear=2013 
<<concat_nead_and_data>>

years=2013-2014
endyear=2014 
<<concat_nead_and_data>>

years=2014-2015
endyear=2015 
<<concat_nead_and_data>>

years=2015-2016
endyear=2016 
<<concat_nead_and_data>>

years=2016-2017
endyear=2017 
<<concat_nead_and_data>>

years=2017-2018
endyear=2018 
<<concat_nead_and_data>>

years=2018-2020
endyear=2020 
<<concat_nead_and_data>>

years=2020-2021
endyear=2021 
<<concat_nead_and_data>>

years=2021-2022
endyear=2022 
<<concat_nead_and_data>>
echo "ZAC_L data now has nead headers"
#+END_SRC


#+NAME: concat_nead_and_data
#+BEGIN_SRC bash :eval yes
cp $sourcedir/$station_id-${years}.csv $destdir/$station_id-${years}.csv
sed -i '1d' $destdir/$station_id-${years}.csv
cat $destdir/01_nead_$station_id-$years.csv $destdir/$station_id-${years}.csv > $destdir/$station_id-$endyear.csv
rm $destdir/$station_id-${years}.csv

#+END_SRC


*** zac_u
I start by taking a look at the pngs of the files to figure out the quality of each file and if any of them should be split up.

None of them needs to be split-up.

Then I manually copy the files to the L0M folder 
#+NAME: make_L0M_zac_u
#+BEGIN_SRC bash :eval yes

station_id=zac_u
sourcedir=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0/$station_id
destdir=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/$station_id

years=2008-2010
endyear=2010 
<<concat_nead_and_data>>

years=2010-2011
endyear=2011 
<<concat_nead_and_data>>

years=2011-2012
endyear=2012 
<<concat_nead_and_data>>

years=2012-2013
endyear=2013 
<<concat_nead_and_data>>

years=2013-2014
endyear=2014 
<<concat_nead_and_data>>

years=2014-2015
endyear=2015 
<<concat_nead_and_data>>

years=2015-2016
endyear=2016 
<<concat_nead_and_data>>

years=2016-2017
endyear=2017 
<<concat_nead_and_data>>

years=2017-2019
endyear=2019 
<<concat_nead_and_data>>

years=2019-2020
endyear=2020 
<<concat_nead_and_data>>

years=2020-2021
endyear=2021 
<<concat_nead_and_data>>

years=2021-2022
endyear=2022 
<<concat_nead_and_data>>
echo "ZAC_U data now has nead headers"

#+END_SRC




*** zac_a

time,rec,p,t_1,t_2,rh,wspd,wdir,wd_std,dsr,usr,dlr,ulr,t_rad,z_boom,z_boom_q,z_stake,z_stake_q,t_i_1,t_i_2,t_i_3,t_i_4,t_i_5,t_i_6,t_i_7,t_i_8,tilt_x,tilt_y,gps_time,gps_lat,gps_lon,gps_alt,gps_geoid,gps_q,gps_numsat,gps_hdop,t_log,fan_dc
#+NAME: make_L0M_zac_a
#+BEGIN_SRC bash
station_id=zac_a
sourcedir=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0/$station_id
destdir=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/$station_id

years=2009-2010
endyear=2010 

<<concat_nead_and_data>>

years=2010-2011
endyear=2011 
<<concat_nead_and_data>>

years=2011-2012
endyear=2012 
<<concat_nead_and_data>>

years=2012-2013
endyear=2013 
<<concat_nead_and_data>>


years=2013-2014
endyear=2014 
<<concat_nead_and_data>>


years=2014-2015
endyear=2015 
<<concat_nead_and_data>>

years=2015-2016
endyear=2016 
<<concat_nead_and_data>>

years=2016-2017
endyear=2017 
<<concat_nead_and_data>>

years=2017-2018
endyear=2018 
<<concat_nead_and_data>>

years=2018-2019
endyear=2019 
<<concat_nead_and_data>>

echo "ZAC_A data now has nead headers"

#+END_SRC




** Get best guess for PTA calibration coefficient for 2015-2022

#+BEGIN_SRC python
import nead
import pandas as pd
import numpy as np
import os
import glob
import re
from datetime import datetime

workingdir ='/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/'  
station = 'zac_l'
#<<convert_to_physical_values>>

filenamestart = workingdir+'data_v1.0/L0M/'+station+'/'+station

infile = filenamestart +'-2016.csv'
<<read_infile_from_filelist>>
<<add_variable_metadata>>
ds = add_variable_metadata(ds)
fig, ax = plt.subplots(1,1)
ds['z_pt'].to_pandas().resample('D').mean().loc['1-July-2015':'9-July-2015'].plot(ax = ax)


icemeltstart = datetime(2015,7,5)

pta = -(ds['z_pt']-ds['z_pt'].loc[icemeltstart].mean())
sr50 = ds['z_stake']-ds['z_stake'].loc[icemeltstart].mean()

rho_af = 1092
pt_z_coef = 0.51
pt_z_factor = 2.5
pta_corr = ds['z_pt'] * pt_z_coef * pt_z_factor * 998.0 / rho_af \
        + 100 * (pt_z_coef - ds['p']) / (rho_af * 9.81)

pta_corr = -(pta_corr-pta_corr.loc[icemeltstart].mean())

fig, ax = plt.subplots(1,1)
pta.plot(ax=ax, label = 'pta')
pta_corr.plot(ax=ax, label = 'pta_corr')
sr50.plot(ax=ax, label = 'sr50')
ax.set_ylim(-1,2)
ax.set_xlim(datetime(2015,7,1),datetime(2015,9,1))
ax.legend()

#+END_SRC


#+BEGIN_SRC python

infile = filenamestart +'-2017.csv'
<<read_infile_from_filelist>>
<<add_variable_metadata>>
ds = add_variable_metadata(ds)
fig, ax = plt.subplots(1,1)
ds['z_pt'].to_pandas().resample('D').mean().loc['1-June-2016':'31-July-2016'].plot(ax = ax)


icemeltstart = datetime(2016,7,5)

pta = -(ds['z_pt']-ds['z_pt'].loc[icemeltstart].mean())
sr50 = ds['z_stake']-ds['z_stake'].loc[icemeltstart].mean()

rho_af = 1092
pt_z_coef = 0.51
pt_z_factor = 2.5
pta_corr = ds['z_pt'] * pt_z_coef * pt_z_factor * 998.0 / rho_af \
        + 100 * (pt_z_coef - ds['p']) / (rho_af * 9.81)

pta_corr = -(pta_corr-pta_corr.loc[icemeltstart].mean())

fig, ax = plt.subplots(1,1)
pta.plot(ax=ax, label = 'pta')
pta_corr.plot(ax=ax, label = 'pta_corr')
sr50.plot(ax=ax, label = 'sr50')
ax.set_ylim(-1,2)
ax.set_xlim(datetime(2016,7,1),datetime(2016,9,1))
ax.legend()

#ds['z_stake'].plot(ax = ax)
#+END_SRC


** Create L1 files: convert to physical values 


*** Debugging: check raw tilt values to make sure the unit is correct  

#+BEGIN_SRC python :tangle convert_to_physical_values.py
import nead
import pandas as pd
import numpy as np
import os
import glob
import re
workingdir ='/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/'  

station = 'zac_l'
filelist = glob.glob(workingdir+'data_v1.0/L0M/'+station+'/'+station+'**.csv')
fig, ax = plt.subplots(3,1, figsize = (10,15))
for infile in filelist:
    print(str(infile))
    <<read_infile_from_filelist>>
    <<add_variable_metadata>>
    ds = add_variable_metadata(ds)
    ds_raw = ds.copy()

    ds_raw[['tilt_x','tilt_y']].to_dataframe().plot()
#+END_SRC


#+BEGIN_SRC python :tangle convert_to_physical_values.py
station = 'zac_u'
filelist = glob.glob(workingdir+'data_v1.0/L0M/'+station+'/'+station+'**.csv')

for infile in filelist:
    print(str(infile))
    <<read_infile_from_filelist>>
    <<add_variable_metadata>>
    ds = add_variable_metadata(ds)
    ds_raw = ds.copy()
ds_raw[['tilt_x','tilt_y']].to_dataframe().plot(ax = ax[1])

station = 'zac_a'
filelist = glob.glob(workingdir+'data_v1.0/L0M/'+station+'/'+station+'**.csv')

for infile in filelist:
    print(str(infile))
    <<read_infile_from_filelist>>
    <<add_variable_metadata>>
    ds = add_variable_metadata(ds)
    ds_raw = ds.copy()
ds_raw[['tilt_x','tilt_y']].to_dataframe().plot(ax = ax[2])

#+END_SRC


*** Workflow

NB I use the NEAD data format for L0 data. Program installed via:
pip install --upgrade git+https://github.com/GEUS-PROMICE/pyNEAD.git


#+NAME: convert_to_physical_values_workflow
#+BEGIN_SRC python :tangle convert_to_physical_values.py
import nead
import pandas as pd
import numpy as np
import os
import glob
import re

workingdir ='/home/shl/OneDrive/projects/aws_processing_v1.0/'  
station = 'zac_l'
<<convert_to_physical_values>>

station = 'zac_u'
<<convert_to_physical_values>>

station = 'zac_a'
<<convert_to_physical_values>>


<<Make_a_single_netcdf_per_station>>
#+END_SRC


#+NAME: convert_to_physical_values
#+BEGIN_SRC python
filelist = glob.glob(workingdir+'data_v1.0/L0M/'+station+'/'+station+'**.csv')

for infile in filelist:
    print(str(infile))
    <<read_infile_from_filelist>>
    <<add_variable_metadata>>
    ds = add_variable_metadata(ds)
    #ds_raw = ds.copy()
    <<raw_to_phys>> 
    #ds_phys = ds.copy()
    <<correct_rh_and_rad>>
    <<write_out_L1_nc>>
    
   
#+END_SRC


*** code
**** Debugging

#+NAME: Debugging
#+BEGIN_SRC python
import nead
import pandas as pd
import numpy as np
import os
import glob
import matplotlib.pyplot as plt


station = 'zac_l'

workingdir ='/home/shl@geus.dk/Dropbox/GEUS/projects/glaciobasis/aws_processing/'  

#filelist = glob.glob(workingdir+'data/L0M/'+station+'/'+station+'**.csv')

infile = workingdir+'data/L0M/'+station+'/'+station+'-2011.csv'
print(str(infile))
<<read_infile_from_filelist>>
<<add_variable_metadata>>
ds = add_variable_metadata(ds)

<<raw_to_phys>>

#+END_SRC


#+BEGIN_SRC python

<<plot_dsr_usr>>

<<correct_rh_and_rad>>
<<plot_dsr_usr>>

<<write_out_L1_nc>>
    
   
#+END_SRC

#+NAME: plot_dsr_usr
#+BEGIN_SRC python
fig, ax = plt.subplots(1,2)
ds.usr.plot(ax = ax[0])
ds.dsr.plot(ax = ax[1])
#+END_SRC

#+BEGIN_SRC python
workingdir ='/home/shl@geus.dk/Dropbox/GEUS/projects/glaciobasis/aws_processing/'  
station = 'zac_u'
infile = workingdir+'data/L0M/'+station+'/'+station+'-2013.csv'

ds = nead.read(infile)
liste = list(ds.keys())
print(liste)
#+END_SRC


#+NAME: plot_ds_all_variab
#+BEGIN_SRC python
liste = list(ds.keys())
data = {}
for variab in liste:
    data[variab] = ds[variab]


data_df = pd.DataFrame(data)    
data_df.index = pd.to_datetime(data_df['time'])


#data_df.loc[data_df.index > '2014-April-25' ,'t_i_1'].plot()
data_df.plot(subplots=True, layout=(6,7), figsize=(30,30))
#print(ds['z_pt'].dtype)
#ds['z_pt_corr'].plot()
#+END_SRC



**** read files and add metadata
#+NAME: read_infile_from_filelist
#+BEGIN_SRC python
#infile = filelist[0]
ds = nead.read(infile)
ds = ds.set_coords(['time'])
ds = ds.set_index({'index':'time'})
ds = ds.rename({'index':'time'})
ds['time'] = pd.to_datetime(ds.time.values)
ds['n'] = (('time'), np.arange(ds.time.size)+1)

# Remove duplicate dates
_, index_dublicates = np.unique(ds['time'], return_index=True)
ds = ds.isel(time=index_dublicates)

# Remove inf
for column in ds.keys():
    ds[column][ds[column]==np.inf] = np.nan


#+END_SRC


#+NAME: add_variable_metadata
#+BEGIN_SRC python
def add_variable_metadata(ds):
    """Uses the variable DB (variables.csv) to add metadata to the xarray dataset."""
    df = pd.read_csv("./variables.csv", index_col=0, comment="#")

    for v in df.index:
        if v == 'time': continue # coordinate variable, not normal var
        if v not in list(ds.variables): continue
        for c in ['standard_name', 'long_name', 'units']:
            if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
            ds[v].attrs[c] = df[c][v]
            
    return ds
#+END_SRC


**** Dataprocessing code

***** raw_to_phys
#+NAME: raw_to_phys
#+BEGIN_SRC python
T_0 = 273.15

# Calculate pressure transducer fluid density

if 'z_pt' in ds:
    if ds.attrs['pt_antifreeze'] == 50:
        rho_af = 1092
    elif ds.attrs['pt_antifreeze'] == 100:
        rho_af = 1145
    else:
        rho_af = np.nan
        print("ERROR: Incorrect metadata: 'pt_antifreeze =' ", ds.attrs['pt_antifreeze'])
        print("Antifreeze mix only supported at 50 % or 100%")
        # assert(False)
    
for v in ['gps_geounit','min_y']:
    if v in list(ds.variables): ds = ds.drop_vars(v)


# convert radiation from engineering to physical units
if 'dsr' in ds:
        
    ds['dsr'] = (ds['dsr']*10) / ds.attrs['dsr_eng_coef'] * 100
    ds['usr'] = (ds['usr']*10) / ds.attrs['usr_eng_coef'] * 100
    ds['dlr'] = ((ds['dlr']*1000) / ds.attrs['dlr_eng_coef']) + 5.67*10**(-8)*(ds['t_rad'] + T_0)**4 
    ds['ulr'] = ((ds['ulr']*1000) / ds.attrs['ulr_eng_coef']) + 5.67*10**(-8)*(ds['t_rad'] + T_0)**4
    
    ds['tilt_x'] = ds['tilt_x'].astype(float) / 100
    ds['tilt_y'] = ds['tilt_y'].astype(float) / 100

# Adjust sonic ranger readings for sensitivity to air temperature
if 'z_boom' in ds:
    ds['z_boom'] = ds['z_boom'] * ((ds['t_1'] + T_0)/T_0)**0.5 
if 'z_stake' in ds:
    ds['z_stake'] = ds['z_stake'] * ((ds['t_1'] + T_0)/T_0)**0.5

# Adjust pressure transducer due to fluid properties
if 'z_pt' in ds:
    #print('z_pt_corr is produced in' + str(infile) )
    #ds['z_pt'] = ds['z_pt'] * ds.attrs['pt_z_coef'] * ds.attrs['pt_z_factor'] * 998.0 / rho_af

    # Calculate pressure transducer depth
    ds['z_pt_corr'] = ds['z_pt'] * np.nan # new 'z_pt_corr' copied from 'z_pt'
    ds['z_pt_corr'].attrs['long_name'] = ds['z_pt'].long_name + " corrected"
    ds['z_pt_corr'] = ds['z_pt'] * ds.attrs['pt_z_coef'] * ds.attrs['pt_z_factor'] * 998.0 / rho_af \
        + 100 * (ds.attrs['pt_z_p_coef'] - ds['p']) / (rho_af * 9.81)


# Decode GPS
if 'gps_lat' in ds:
    if ds['gps_lat'].dtype.kind == 'O': # not a float. Probably has "NH"
        #assert('NH' in ds['gps_lat'].dropna(dim='time').values[0])
        for v in ['gps_lat','gps_lon','gps_time']:
            a = ds[v].attrs # store
            str2nums = [re.findall(r"[-+]?\d*\.\d+|\d+", _) if isinstance(_, str) else [np.nan] for _ in ds[v].values]
            ds[v][:] = pd.DataFrame(str2nums).astype(float).T.values[0]
            ds[v] = ds[v].astype(float)
            ds[v].attrs = a # restore

    if np.any((ds['gps_lat'] <= 90) & (ds['gps_lat'] > 0)):  # Some stations only recorded minutes, not degrees
        xyz = np.array(re.findall("[-+]?[\d]*[.][\d]+", ds.attrs['geometry'])).astype(float)
        x=xyz[0]; y=xyz[1]; z=xyz[2] if len(xyz) == 3 else 0
        p = shapely.geometry.Point(x,y,z)
        # from IPython import embed; embed()
        # assert(False) # should p be ints rather than floats here?
        # ds['gps_lat'] = ds['gps_lat'].where(
        ds['gps_lat'] = ds['gps_lat'] + 100*p.y
    if np.any((ds['gps_lon'] <= 90) & (ds['gps_lon'] > 0)):
        ds['gps_lon'] = ds['gps_lon'] + 100*p.x

    for v in ['gps_lat','gps_lon']:
        a = ds[v].attrs # store
        ds[v] = np.floor(ds[v] / 100) + (ds[v] / 100 - np.floor(ds[v] / 100)) * 100 / 60
        ds[v].attrs = a # restore


# Correct winddir due to boom_azimuth

# ds['ws'].

# tilt-o-meter voltage to degrees
# if transmitted ne 'yes' then begin
#    tiltX = smooth(tiltX,7,/EDGE_MIRROR,MISSING=-999) & tiltY = smooth(tiltY,7,/EDGE_MIRROR, MISSING=-999)
# endif

# Should just be
# if ds.attrs['PROMICE_format'] != 'TX': dstxy = dstxy.rolling(time=7, win_type='boxcar', center=True).mean()
# but the /EDGE_MIRROR makes it a bit more complicated...

if 'tilt_x' in ds:
    win_size=7
    s = np.int(win_size/2)
    tdf = ds['tilt_x'].to_dataframe()
    ds['tilt_x'] = (('time'), tdf.iloc[:s][::-1].append(tdf).append(tdf.iloc[-s:][::-1]).rolling(win_size, win_type='boxcar',     center=True).mean()[s:-s].values.flatten())
    tdf = ds['tilt_y'].to_dataframe()
    ds['tilt_y'] = (('time'), tdf.iloc[:s][::-1].append(tdf).append(tdf.iloc[-s:][::-1]).rolling(win_size, win_type='boxcar',    center=True).mean()[s:-s].values.flatten())

    # # notOKtiltX = where(tiltX lt -100, complement=OKtiltX) & notOKtiltY = where(tiltY lt -100, complement=OKtiltY)
    notOKtiltX = (ds['tilt_x'] < -100)
    OKtiltX = (ds['tilt_x'] >= -100)
    notOKtiltY = (ds['tilt_y'] < -100)
    OKtiltY = (ds['tilt_y'] >= -100)

    # tiltX = tiltX/10.
    #ds['tilt_x'] = ds['tilt_x'] / 10
    #ds['tilt_y'] = ds['tilt_y'] / 10

    # tiltnonzero = where(tiltX ne 0 and tiltX gt -40 and tiltX lt 40)
    # if n_elements(tiltnonzero) ne 1 then tiltX[tiltnonzero] = tiltX[tiltnonzero]/abs(tiltX[tiltnonzero])*(-0.49*(abs(tiltX[tiltnonzero]))^4 +   3.6*(abs(tiltX[tiltnonzero]))^3 - 10.4*(abs(tiltX[tiltnonzero]))^2 +21.1*(abs(tiltX[tiltnonzero])))

    # tiltY = tiltY/10.
    # tiltnonzero = where(tiltY ne 0 and tiltY gt -40 and tiltY lt 40)
    # if n_elements(tiltnonzero) ne 1 then tiltY[tiltnonzero] = tiltY[tiltnonzero]/abs(tiltY[tiltnonzero])*(-0.49*(abs(tiltY[tiltnonzero]))^4 + 3.6*(abs(tiltY[tiltnonzero]))^3 - 10.4*(abs(tiltY[tiltnonzero]))^2 +21.1*(abs(tiltY[tiltnonzero])))

    dstx = ds['tilt_x']
    nz = (dstx != 0) & (np.abs(dstx) < 40)
    dstx = dstx.where(~nz, other = dstx / np.abs(dstx) * (-0.49 * (np.abs(dstx))**4 + 3.6 * (np.abs(dstx))**3 - 10.4 * (np.abs(dstx))**2 + 21.1 * (np.abs(dstx))))
    ds['tilt_x'] = dstx

    dsty = ds['tilt_y']
    nz = (dsty != 0) & (np.abs(dsty) < 40)
    dsty = dsty.where(~nz, other = dsty / np.abs(dsty) * (-0.49 * (np.abs(dsty))**4 + 3.6 * (np.abs(dsty))**3 - 10.4 * (np.abs(dsty))**2 + 21.1 * (np.abs(dsty))))
    ds['tilt_y'] = dsty

    # if n_elements(OKtiltX) gt 1 then tiltX[notOKtiltX] = interpol(tiltX[OKtiltX],OKtiltX,notOKtiltX) ; Interpolate over gaps for radiation correction; set to -999 again below.
    # if n_elements(OKtiltY) gt 1 then tiltY[notOKtiltY] = interpol(tiltY[OKtiltY],OKtiltY,notOKtiltY) ; Interpolate over gaps for radiation correction; set to -999 again below.

    ds['tilt_x'] = ds['tilt_x'].where(~notOKtiltX)
    ds['tilt_y'] = ds['tilt_y'].where(~notOKtiltY)
    ds['tilt_x'] = ds['tilt_x'].interpolate_na(dim='time')
    ds['tilt_y'] = ds['tilt_y'].interpolate_na(dim='time')

# ds['tilt_x'] = ds['tilt_x'].ffill(dim='time')
# ds['tilt_y'] = ds['tilt_y'].ffill(dim='time')


deg2rad = np.pi / 180
ds['wdir'] = ds['wdir'].where(ds['wspd'] != 0)
ds['wspd_x'] = ds['wspd'] * np.sin(ds['wdir'] * deg2rad)
ds['wspd_y'] = ds['wspd'] * np.cos(ds['wdir'] * deg2rad)
    
 #+END_SRC




#+BEGIN_SRC python
import matplotlib.pyplot as plt
#sel = (dates.index > pd.to_datetime('2009-05-01', format = '%Y-%m-%d')) & (dates.index < pd.to_datetime('2009-05-04', format = '%Y-%m-%d'))
sel = (dates.index.year==2009) & (dayofyear > 200) & (dayofyear < 203)
#plt.plot(dates.index.values[sel],theta_sensor_rad[sel])
plt.plot(dates.index.values[sel],ds['tilt_x'][sel])

#+END_SRC



***** Correct rh and rad

 #+NAME: correct_rh_and_rad
 #+BEGIN_SRC python
<<correct_RH>>
<<calc_t_surf>>
if 'dsr' in ds:
    <<correct_short_wave_radiation_for_tilt>>
    

 #+END_SRC


****** Correct RH water to be relative to ice when T_air is freezing :noexport:
  Relative humidity is recorded as over water - it needs to be corrected as it is over ice when the surface is freezing

#+NAME: correct_RH
#+BEGIN_SRC python :tangle correct_RH.py
below = ds['t_1'].values < 0
T = ds['t_1'].values + 273.15

ew = 10**(-7.90298*(373.16/T-1)+5.02808*np.log10(373.16/T)-(1.3816*10**(-7))*(10**(11.344*(1-T/373.16))-1)+(8.1328*10**(-3))*(10**(-349149*(373.16/T-1))-1)+np.log10(1013.246))

#ew = 10**(-7.90298*(373.16/T-1)+5.02808*np.log10(373.16/T)-(1.3816*10**(-7))*(10**(11.344*(1-T/373.16))-1)+(8.1328*10**(-3))*(10**(-349149*(373.16/T-1))-1)+np.log10(1013.246))

ei = 10**(-9.09718*(273.16/T-1)-3.56654*np.log10(273.16/T)+0.876793*(1-T/273.16)+np.log10(6.1071))

rh_ice = ds['rh'].values*ew/ei

rh = ds['rh'].copy()
rh[below] = rh_ice[below]
#rh[rh>100] = 100
rh[rh<0] = 0
ds['rh_corr'] = rh.copy()
ds['rh_corr'].attrs['long_name'] = ds['rh'].long_name + " corrected"          

  #+END_SRC


****** calc_t_surf                                                 :noexport:


#+NAME: calc_t_surf
#+BEGIN_SRC python :tagle calculate_t_surface.py
T_0 = 273.15
epsilon = 0.97
sigma = 5.67*10**(-8)
Tsurf = ((ds['ulr']-(1-epsilon)*ds['dlr'])/(epsilon*sigma))**0.25 -T_0
ds = ds.assign({'t_surf':Tsurf})

 #+END_SRC


****** Correct short wave radiation for tilt                       :noexport:
 Correcting Short wave radiation for tilt according to MacWhorter and Weller 1990
 This code is adapted from the promice processing idl code version 2012.

 #+NAME: correct_short_wave_radiation_for_tilt
 #+BEGIN_SRC python :tagle correct_shortwave_radiation_for_tilt.py
<<cloud_cover_tsurf>>
<<tilt_angle_rotate>>
<<zenith_and_hour_angle>>
<<correction_factor_for_direct_beam_radiation>>
<<corr_srin_for_tilt>>
<<ok_albedos>>
<<corr_sr_diffuse_radiation>>
<<corr_large_zenith_angles>>
<<corr_srin_upper_sensor_not_in_sight>>
<<removing_spikes>> #The spike removal can disquise problems
<<adding_columns>>

 #+END_SRC



******* Code                                                       :noexport:
 Calculate cloud cover for SRin correction and surface temperature
 #+NAME: cloud_cover_tsurf
 #+BEGIN_SRC python :tangle correct_sw_for_tilt.py
# Calculate cloud cover
T_0 = 273.15
eps_overcast = 1
eps_clear = 9.36508e-6
LR_overcast = eps_overcast*5.67*10**(-8)*(ds['t_1']+T_0)**4 # assumption
LR_clear = eps_clear*5.67*10**(-8)*(ds['t_1']+T_0)**6 # Swinbank (1963)
CloudCov = (ds['dlr'].values-LR_clear)/(LR_overcast-LR_clear)

overcast = CloudCov > 1
Clear = CloudCov < 0
CloudCov[overcast] = 1
CloudCov[Clear] = 0
DifFrac = 0.2+0.8*CloudCov


 #+END_SRC



#+BEGIN_SRC python
plt.plot(CloudCov)
#+END_SRC

 Calculating the tilt angle and direction of senson and rotating to a north-south aligned coordinate system

#+NAME: tilt_angle_rotate
#+BEGIN_SRC python :tangle correct_sw_for_tilt.py
# Calculating the tilt angle and direction of senson and rotating to a north-south aligned coordinate system
deg2rad = np.pi / 180
tiltX_rad = ds['tilt_x'].values*deg2rad
tiltY_rad = ds['tilt_y'].values*deg2rad

X = np.sin(tiltX_rad)*np.cos(tiltX_rad)*(np.sin(tiltY_rad))**2 + np.sin(tiltX_rad)*(np.cos(tiltY_rad))**2 # Cartesian coordinate
Y = np.sin(tiltY_rad)*np.cos(tiltY_rad)*(np.sin(tiltX_rad))**2 + np.sin(tiltY_rad)*(np.cos(tiltX_rad))**2 # Cartesian coordinate
Z = np.cos(tiltX_rad)*np.cos(tiltY_rad) + (np.sin(tiltX_rad))**2*(np.sin(tiltY_rad))**2 # Cartesian coordinate
phi_sensor_rad = -np.pi/2-np.arctan(Y/X) # spherical coordinate

phi_sensor_rad[X > 0] = phi_sensor_rad[X > 0]+np.pi
phi_sensor_rad[(X == 0) & (Y < 0)] = np.pi
phi_sensor_rad[(X == 0) & (Y >= 0)] = 0
phi_sensor_rad[phi_sensor_rad < 0] = phi_sensor_rad[phi_sensor_rad < 0]+2*np.pi

phi_sensor_deg = phi_sensor_rad*180/np.pi # radians to degrees
theta_sensor_rad = np.arccos(Z/(X**2+Y**2+Z**2)**0.5) # spherical coordinate (or actually total tilt of the sensor, i.e. 0 when horizontal)
theta_sensor_deg = theta_sensor_rad*180/np.pi # radians to degrees



 #+END_SRC



 #+BEGIN_SRC python 
import matplotlib.pyplot as plt
#sel = (dates.index > pd.to_datetime('2009-05-01', format = '%Y-%m-%d')) & (dates.index < pd.to_datetime('2009-05-04', format = '%Y-%m-%d'))
sel = (dates.index.year==2009) & (dayofyear > 200) & (dayofyear < 203)
#plt.plot(dates.index.values[sel],theta_sensor_rad[sel])
plt.plot(dates.index.values,ds['tilt_x'])

 #+END_SRC

 Calculating zenith and hour angle of the sun
 #+NAME: zenith_and_hour_angle
 #+BEGIN_SRC python :tangle correct_sw_for_tilt.py
# Calculating zenith and hour angle of the sun
lat = float(ds.geometry[13:19]) #ds['gps_lat'].median().values
lon = float(ds.geometry[6:12]) #ds['gps_lon'].median().values
dates = ds.time.to_dataframe()
dates.index = pd.to_datetime(dates['time'])
dayofyear =dates.index.dayofyear.values
hour = dates.index.hour.values
minute = dates.index.minute.values

d0_rad = 2*np.pi*(dayofyear+(hour+minute/60)/24-1)/365
Declination_rad = np.arcsin(0.006918-0.399912*np.cos(d0_rad)+0.070257*np.sin(d0_rad)-0.006758*np.cos(2*d0_rad)+0.000907*np.sin(2*d0_rad)-0.002697*np.cos(3*d0_rad)+0.00148*np.sin(3*d0_rad))

HourAngle_rad = 2*np.pi*(((hour+minute/60.)/24-0.5))# - lon/360) #- 15.*timezone/360.) ; NB: Make sure time is in UTC and longitude is positive when west! Hour angle should be 0 at noon.
DirectionSun_deg = HourAngle_rad*180/np.pi-180 # This is 180 deg at noon (NH), as opposed to HourAngle.
DirectionSun_deg[DirectionSun_deg < 0] = DirectionSun_deg[DirectionSun_deg < 0]+360
DirectionSun_deg[DirectionSun_deg < 0] = DirectionSun_deg[DirectionSun_deg < 0]+360

ZenithAngle_rad = np.arccos(np.cos(lat*np.pi/180)*np.cos(Declination_rad)*np.cos(HourAngle_rad) + np.sin(lat*np.pi/180)*np.sin(Declination_rad))
ZenithAngle_deg = ZenithAngle_rad*180/np.pi
sundown = ZenithAngle_deg >= 90
SRtoa = 1372*np.cos(ZenithAngle_rad) # SRin at the top of the atmosphere
SRtoa[sundown] = 0




 #+END_SRC




 http://solardat.uoregon.edu/SolarRadiationBasics.html


% Calculating the correction factor for direct beam radiation (http://solardat.uoregon.edu/SolarRadiationBasics.html)
CorFac = sin(Declination_rad).*sin(lat*pi/180.).*cos(theta_sensor_rad)...
        -sin(Declination_rad).*cos(lat*pi/180.).*sin(theta_sensor_rad).*cos(phi_sensor_rad+pi) ...
        +cos(Declination_rad).*cos(lat*pi/180.).*cos(theta_sensor_rad).*cos(HourAngle_rad) ...
        +cos(Declination_rad).*sin(lat*pi/180.).*sin(theta_sensor_rad).*cos(phi_sensor_rad+pi).*cos(HourAngle_rad) ...
        +cos(Declination_rad).*sin(theta_sensor_rad).*sin(phi_sensor_rad+pi).*sin(HourAngle_rad);





 #+NAME: correction_factor_for_direct_beam_radiation
 #+BEGIN_SRC python :tangle correct_sw_for_tilt.py
# correction factor for direct beam radiation
CorFac = np.sin(Declination_rad) * np.sin(lat*np.pi/180.) * np.cos(theta_sensor_rad) \
         -np.sin(Declination_rad) * np.cos(lat*np.pi/180.) * np.sin(theta_sensor_rad) * np.cos(phi_sensor_rad+np.pi) \
        +np.cos(Declination_rad) * np.cos(lat*np.pi/180.) * np.cos(theta_sensor_rad) * np.cos(HourAngle_rad) \
        +np.cos(Declination_rad) * np.sin(lat*np.pi/180.) * np.sin(theta_sensor_rad) * np.cos(phi_sensor_rad+np.pi) * np.cos(HourAngle_rad) \
        +np.cos(Declination_rad) * np.sin(theta_sensor_rad)*np.sin(phi_sensor_rad+np.pi)*np.sin(HourAngle_rad)

CorFac = np.cos(ZenithAngle_rad)/CorFac
no_correction = (CorFac <= 0) | ( ZenithAngle_deg > 90) # sun out of field of view upper sensor
CorFac[no_correction] = 1
  #+END_SRC


 Calculating SRin over a horizontal surface corrected for station/sensor tilt
 #+NAME: corr_srin_for_tilt
 #+BEGIN_SRC python :tangle correct_sw_for_tilt.py
 # Calculating SRin over a horizontal surface corrected for station/sensor tilt
CorFac_all = CorFac/(1-DifFrac+CorFac*DifFrac)
SRin_cor = ds['dsr']*CorFac_all
#srin_tilt_cor = SRin_cor.copy() # Debuggin
 #+END_SRC




 Calculating albedo based on albedo values when sun is in sight of the upper sensor
 #+NAME: ok_albedos
 #+BEGIN_SRC python :tangle correct_sw_for_tilt.py
# Calculating albedo based on albedo values when sun is in sight of the upper sensor
AngleDif_deg = 180/np.pi*np.arccos(np.sin(ZenithAngle_rad)*np.cos(HourAngle_rad+np.pi)*np.sin(theta_sensor_rad)*np.cos(phi_sensor_rad) \
                             +np.sin(ZenithAngle_rad)*np.sin(HourAngle_rad+np.pi)*np.sin(theta_sensor_rad)*np.sin(phi_sensor_rad) \
                             +np.cos(ZenithAngle_rad)*np.cos(theta_sensor_rad)) # angle between sun and sensor


albedo = ds['usr']/SRin_cor


OKalbedos = (AngleDif_deg < 70) & (ZenithAngle_deg < 70) & (albedo < 1) & (albedo > 0)

notOKalbedos = (AngleDif_deg >= 70) | (ZenithAngle_deg >= 70) | (albedo >= 1) | (albedo <= 0)

albedo[notOKalbedos] = np.nan
albedo = albedo.ffill('time')
#albedo = interp1(datenumber,albedo,datenumber,'pchip') # interpolate over gaps - gives problems for discontinuous data sets, but is not the end of the world

 #+END_SRC



 Correcting SR using SRin when sun is in field of view of lower sensor assuming sensor measures only diffuse radiation

 #+NAME: corr_sr_diffuse_radiation
 #+BEGIN_SRC python :tangle correct_sw_for_tilt.py
# Correcting SR using SRin when sun is in field of view of lower sensor assuming sensor measures only diffuse radiation
sunonlowerdome = (AngleDif_deg >= 90) & (ZenithAngle_deg <= 90)
SRin_cor[sunonlowerdome] = ds['dsr'][sunonlowerdome].values/DifFrac[sunonlowerdome]


SRout_cor = ds['usr']
SRout_cor[sunonlowerdome] = albedo[sunonlowerdome]*ds['dsr'][sunonlowerdome].values/DifFrac[sunonlowerdome]
#srin_cor_dome = SRin_cor.copy() # debugging
 #+END_SRC

 Setting SRin and SRout to zero for solar zenith angles larger than 95 deg or either SRin or SRout are (less than) zero

 #+NAME: corr_large_zenith_angles
 #+BEGIN_SRC python :tangle correct_sw_for_tilt.py
# Setting SRin and SRout to zero for solar zenith angles larger than 95 deg or either SRin or SRout are (less than) zero
no_SR = (ZenithAngle_deg > 95) | (SRin_cor <= 0) | (SRout_cor <= 0)

SRin_cor[no_SR] = 0
SRout_cor[no_SR] = 0

 #+END_SRC


 % Correcting SRin using more reliable SRout when sun not in sight of upper sensor



 This gives problems for some  of the years
 #+NAME: corr_srin_upper_sensor_not_in_sight
 #+BEGIN_SRC python :tangle correct_sw_for_tilt.py
# Correcting SRin using more reliable SRout when sun not in sight of upper sensor

#SRin_cor[no_correction] = SRout_cor[no_correction]/albedo[no_correction]
SRin_cor[~notOKalbedos] = SRout_cor[~notOKalbedos]/albedo[~notOKalbedos]
#SRin_cor_alb = SRin_cor.copy() # Debugging
#SRin_cor = SRout_cor/albedo # What is done in the IDL code
#albedo[notOKalbedos] = -999
 #+END_SRC



 % Removing spikes by interpolation based on a simple top-of-the-atmosphere limitation

 #+NAME: removing_spikes
 #+BEGIN_SRC python :tangle correct_sw_for_tilt.py
# Removing spikes by interpolation based on a simple top-of-the-atmosphere limitation

SRin_cor_mark = SRin_cor.copy()
TOA_crit_nopass = SRin_cor > 0.9*SRtoa+10

SRin_cor[TOA_crit_nopass] = np.nan
SRout_cor[TOA_crit_nopass] = np.nan


SRin_cor_final = SRin_cor.copy()

 #+END_SRC


 #+NAME: adding_columns
 #+BEGIN_SRC python :tangle correct_sw_for_tilt.py
# Assign columns to ds file

ds = ds.assign({'albedo':albedo, 'dsr_corr':SRin_cor, 'usr_corr':SRout_cor, 'cloud_cov':CloudCov})
ds['I'] = ('time', SRtoa)
ds['solar_zenith_angle'] = ('time', ZenithAngle_deg )



#ds_test = ds.assign({'albedo':albedo,'srin_tilt_cor':srin_tilt_cor,'srin_cor_dome':srin_cor_dome, 'SRin_cor_alb':SRin_cor_alb ,'SRin_cor_final':SRin_cor_final,'SRin_cor_mark':SRin_cor_mark , 'dsr_corr':SRin_cor, 'usr_corr':SRout_cor, 'cloud_cov':CloudCov}) # debugging
#ds['dsr_corr']=SRin_cor
ds['dsr_corr'].attrs['long_name'] = ds['dsr'].long_name + " corrected"   
#ds['usr_corr']=SRout_cor.copy()
ds['usr_corr'].attrs['long_name'] = ds['usr'].long_name + " corrected"   

#ds['cloud_cover']=CloudCov.copy()
 #+END_SRC


**** Write out L1 files
 #+NAME: write_out_L1_nc
 #+BEGIN_SRC python

outpath = 'data_v1.0/L1/'+station+'/'
outfile = infile[-14:-4]
ds = ds.sel(time=ds.time.notnull())
#ds_test = ds_test.sel(time=ds.time.notnull()) # debugging

outpathfile = outpath + outfile + ".nc"
#outpathfile_test = outpath + outfile + "_test.nc" #debug
if os.path.exists(outpathfile): os.remove(outpathfile)
ds.to_netcdf(outpathfile, mode='w', format='NETCDF4', compute=True)
#ds_test.to_netcdf(outpathfile_test, mode='w', format='NETCDF4', compute=True) #debug

 #+END_SRC


#+BEGIN_SRC python
print(ds.time[0].data)
#+END_SRC

#+RESULTS:
:results:
# Out [78]: 
# output
2010-05-13T03:10:00.000000000

:end:
**** Testing what part of the tilt correction that matters in the 'bad' years like 2016

#+BEGIN_SRC python

import xarray as xr
ds = xr.load_dataset('data_v1.0/L1/zac_l/zac_l-2016_test.nc')
ds_test = ds[['dsr','dsr_corr','srin_tilt_cor','srin_cor_dome','SRin_cor_alb','SRin_cor_final', 'SRin_cor_mark' ]].to_pandas().loc['June-2015']

fig,ax = plt.subplots(1,1)
#ds_test['srin_tilt_cor'].resample('D').sum().plot(ax = ax, label = 'tilt')
#ds_test['srin_cor_dome'].resample('D').sum().plot(ax = ax, label = 'dome')
#ds_test['SRin_cor_mark'].resample('D').sum().plot(ax = ax, label = 'mark')

#ds_test['SRin_cor_final'].resample('D').sum().plot(ax = ax, label = 'final')
ds_test['dsr'].resample('D').sum().plot(ax = ax, label = 'dsr')
ds_test['dsr_corr'].resample('D').sum().plot(ax = ax, label = 'dsr_corr')
ax.legend()
#ax.set_ylim(35000,45000)

#+END_SRC

#+BEGIN_SRC python
ds = xr.load_dataset('data_v1.0/L1/zac_l/zac_l-2017.nc')
ds_sel = ds[['dsr','dsr_corr']].to_pandas().loc['June-2016']

fig,ax = plt.subplots(1,1)

ds_sel['dsr'].resample('D').sum().plot(ax = ax, label = 'dsr')
ds_sel['dsr_corr'].resample('D').sum().plot(ax = ax, label = 'dsr_corr')
ax.legend()
#ax.set_ylim(35000,45000)



 #+END_SRC


**** Make one single netcdf per station

#+NAME: Make_a_single_netcdf_per_station
#+BEGIN_SRC python

import pandas as pd
import xarray as xr
from glob import glob
import matplotlib.pyplot as plt
import os

datadir = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/'
outfile = datadir+'zac_l/zac_l-2008-2022.nc'
with xr.open_mfdataset(datadir+'zac_l/zac_l-20??.nc') as ds:
    ds.to_netcdf(outfile)
    ds.to_dataframe().to_csv(datadir+'zac_l/zac_l-2008-2022.txt')

outfile = datadir+'zac_u/zac_u-2008-2022.nc'
with xr.open_mfdataset(datadir+'zac_u/zac_u-20??.nc') as ds:
    ds.to_netcdf(outfile)
    ds.to_dataframe().to_csv(datadir+'zac_u/zac_u-2008-2022.txt')

outfile = datadir+'zac_a/zac_a-2009-2020.nc'
with xr.open_mfdataset(datadir+'zac_a/zac_a-20??.nc') as ds:
    ds.to_netcdf(outfile)
    ds.to_dataframe().to_csv(datadir+'zac_a/zac_a-2009-2020.txt')


 #+END_SRC

 #+RESULTS:

 #+BEGIN_SRC python

 if os.path.isfile(outfile):
     os.remove(outfile)

 ds.to_netcdf(outfile)


 infile = glob(datadir+'zac_u/zac_u-20??.nc')
 print(infile)
 outfile = datadir+'zac_u/zac_u-2008-2022.nc'

 ds = xr.open_dataset(infile[0]).load().dropna(dim='time', how='all')
 for f in infile[1:]:
     tmp = xr.open_mfdataset(f).load().dropna(dim='time', how='all')
     ds = ds.combine_first(tmp)

 if os.path.isfile(outfile):
     os.remove(outfile)

 ds.to_netcdf(outfile)    



 infile = glob(datadir+'zac_a/zac_a-20??.nc')
 outfile = datadir+'zac_a/zac_a-2009-2020.nc'

 ds = xr.open_mfdataset(infile[0]).load().dropna(dim='time', how='all')
 for f in infile[1:]:
     tmp = xr.open_mfdataset(f).load().dropna(dim='time', how='all')
     ds = ds.combine_first(tmp)

 if os.path.isfile(outfile):
     os.remove(outfile)

 ds.to_netcdf(outfile)    


 #+END_SRC



*** Quality check plots


**** dataseries examples
#+BEGIN_SRC python 
import xarray as xr
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from glob import glob

filename = 'data_v1.0/L1/zac_a/zac_a-2009-2020.nc'

ds = xr.open_dataset(filename)
ds.tilt_x.plot()

    
ds_filtered = ds.copy()
ds_filtered['p'] = ds['p'].copy()
ds_filtered['p'] = ds['p'].where(ds['p'] > (ds['p'].median() - 100))
ds_filtered['t_1'] = ds['t_1'].where(ds['t_1'] > -50)
ds_filtered['t_2'] = ds['t_2'].where(ds['t_2'] > -50)


fig, ax = plt.subplots(7,1,figsize=(20,20))
ds['dsr_corr'].plot(ax = ax[0])
ds['usr_corr'].plot(ax = ax[0])
ds['dlr'].plot(ax = ax[1])
ds['ulr'].plot(ax = ax[1])
ax[1].set_ylim([0,400])
ds_filtered['p'].plot(ax=ax[2])
ds_filtered['t_1'].plot(ax=ax[3])
ds_filtered['t_2'].plot(ax=ax[3])
ds['rh_corr'].plot(ax=ax[4])
#+END_SRC



#+BEGIN_SRC python 
import xarray as xr
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from glob import glob

filename = 'data_v1.0/L1/zac_l/zac_l-2008-2022.nc'

ds = xr.open_dataset(filename)

ds_filtered = ds.copy()
ds_filtered['p'] = ds['p'].copy()
ds_filtered['p'] = ds['p'].where(ds['p'] > (ds['p'].median() - 100))
ds_filtered['t_1'] = ds['t_1'].where((ds['t_1'] > -50) & (ds['t_1'] < 50))
ds_filtered['t_2'] = ds['t_2'].where((ds['t_2'] > -50) & (ds['t_2'] < 50))


fig, ax = plt.subplots(5,1,figsize=(20,10))
ds['dsr_corr'].plot(ax = ax[0])
ds['usr_corr'].plot(ax = ax[0])
ds['dlr'].plot(ax = ax[1])
ds['ulr'].plot(ax = ax[1])
ax[1].set_ylim([0,400])
ds_filtered['p'].plot(ax=ax[2])
ds_filtered['t_1'].plot(ax=ax[3])
#ds_filtered['t_2'].plot(ax=ax[3])
ds['rh_corr'].plot(ax=ax[4])

#+END_SRC


**** Get positions for all three stations
#+BEGIN_SRC python 
import xarray as xr
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from glob import glob

filename = 'data_v1.0/L1/zac_l/zac_l-2008-2020.nc'
with xr.open_dataset(filename) as ds:
    lat_l = ds.gps_lat.mean().values
    lon_l = ds.gps_lon.mean().values
    alt_l = ds.gps_alt.mean().values
    

filename = 'data_v1.0/L1/zac_u/zac_u-2008-2020.nc'
with xr.open_dataset(filename) as ds:
    lat_u = ds.gps_lat.mean().values
    lon_u = ds.gps_lon.mean().values
    alt_u = ds.gps_alt.mean().values
filename = 'data_v1.0/L1/zac_a/zac_a-2009-2020.nc'
with xr.open_dataset(filename) as ds:
    lat_a = ds.gps_lat.mean().values
    lon_a = ds.gps_lon.mean().values
    alt_a = ds.gps_alt.mean().values



stations = ['zac_l','zac_u','zac_a']
lat = [+lat_l,+lat_u,+lat_a]
lon = [-lon_l,-lon_u,-lon_a]
alt = [+alt_l,+alt_u, +alt_a]

positions = pd.DataFrame({'station':stations, 'lat':lat, 'lon':lon, 'elev':alt})
positions.set_index('station', inplace= True)
#positions = pd.to_numeric(positions)
positions.to_csv('GlacioBasis_Zackenberg_station_positions.csv', index=True, float_format = '%.4f')
print(positions)
#+END_SRC

#+RESULTS:
:results:
# Out [30]: 
# output
               lat        lon         elev
station                                   
zac_l    74.624558 -21.375218   644.513575
zac_u    74.644020 -21.469426   877.588662
zac_a    74.647475 -21.651841  1476.043991

:end:

#+BEGIN_SRC python

    
ds_filtered = ds.copy()
ds_filtered['p'] = ds['p'].copy()
ds_filtered['p'] = ds['p'].where(ds['p'] > (ds['p'].median() - 100))
ds_filtered['t_1'] = ds['t_1'].where(ds['t_1'] > -50)
ds_filtered['t_2'] = ds['t_2'].where(ds['t_2'] > -50)


fig, ax = plt.subplots(7,1,figsize=(20,20))
ds['dsr_corr'].plot(ax = ax[0])
ds['usr_corr'].plot(ax = ax[0])
ds['dlr'].plot(ax = ax[1])
ds['ulr'].plot(ax = ax[1])
ax[1].set_ylim([0,400])
ds_filtered['p'].plot(ax=ax[2])
ds_filtered['t_1'].plot(ax=ax[3])
ds_filtered['t_2'].plot(ax=ax[3])
ds['rh_corr'].plot(ax=ax[4])
#+END_SRC


**** Gradients
#+BEGIN_SRC python 
import xarray as xr
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from glob import glob


zac_l= xr.open_dataset('data_v1.0/L1/zac_l/zac_l-2008-2021.nc')
zac_u = xr.open_dataset('data_v1.0/L1/zac_u/zac_u-2008-2020.nc')
zac_a = xr.open_dataset('data_v1.0/L1/zac_a/zac_a-2009-2020.nc')



fig, ax = plt.subplots(2,1,figsize=(10,10))
zac_l['t_1'].plot(ax = ax[0])
zac_u['t_1'].plot(ax = ax[0])
zac_a['t_1'].plot(ax = ax[0])
zac_l['p'].plot(ax = ax[1])
zac_u['p'].plot(ax = ax[1])
zac_a['p'].plot(ax = ax[1])


#+END_SRC





* QC and database delivery

** Python libs

#+NAME: load_libs
#+BEGIN_SRC python :session
import numpy as np
import pandas as pd
import xarray as xr
import matplotlib.pyplot as plt
import datetime
#+END_SRC


** Loading raw data and filtering data based on QC below

*** Meterological observations
Workflow

#+NAME: run_all_QC_filtering_steps_for_meterological_observations
#+BEGIN_SRC python :tangle QC_filtering_of_meterological_data.py

<<run_all_preQC_for_zac_a>>
<<run_all_postQC_for_zac_a>>

<<run_all_preQC_for_zac_l>>
<<run_all_postQC_for_zac_l>>

<<run_all_preQC_for_zac_u>>
<<run_all_postQC_for_zac_u>>
#+END_SRC


**** zac_a

***** Pre QC
#+NAME: run_all_preQC_for_zac_a
#+BEGIN_SRC python
<<zac_a_preQC_temperature>>
<<zac_a_preQC_radiation>>
<<zac_a_preQC_tilt>>
<<zac_a_preQC_relative_humidity>>
<<zac_a_preQC_wind_speed>>
<<zac_a_preQC_pressure>>
<<zac_a_preQC_boom_height>>
<<zac_a_preQC_stake_height>>
#+END_SRC


 There are so many quality issues with the transmitted data - that I will omit it for now...
****** Transmitted data - what data do we have
 Plotting  all the transmitted data:
   #+BEGIN_SRC python
 <<load_libs>>
 <<Load_transmitted_data>>

 transmitted_reduced = transmitted.drop(' timestamp', axis = 1)
 transmitted_reduced = transmitted_reduced.dropna(axis = 1, how = 'all')
 transmitted_reduced.astype(float).plot(subplots=True, figsize = (20,30), layout = (25,2))
   #+END_SRC

 Plotting the transmitted data that makes sense:

   #+BEGIN_SRC python
 fig, ax = plt.subplots(6,1,figsize=(10,10))
 transmitted['temperature.1'].plot(ax=ax[0], label = 'temperature.1')
 transmitted['temperature2'].plot(ax=ax[0] , label = 'temperature2')
 transmitted['temperature'].plot(ax=ax[0], label = 'temperature')
 ax[0].legend()
 #ax[0].set_ylim(-40,20)
 transmitted['airpressure'].plot(ax=ax[1], label = 'airpressure')
 ax[1].legend()
 transmitted['relativehumidity'].plot(ax=ax[2])
 ax[2].legend()

 transmitted['shortwaveradiationin'].plot(ax=ax[3], label = 'shortwaveradiationin')
 transmitted['shortwaveradiationout'].plot(ax=ax[3], label = 'shortwaveradiationout')
 ax[3].legend()

 transmitted['windspeed_1'].plot(ax=ax[4], label = 'windspeed')
 ax[4].legend()

 transmitted['windspeed_2'].plot(ax=ax[5], label = 'windspeed_2 believed to be direction')
 ax[5].legend()

   #+END_SRC



****** Transmitted data what is actually usable (when was the station buried)
  We look at the incoming shortwave radiation to define when the station is buried
  #+BEGIN_SRC python
  #transmitted_trusted = transmitted[:'2019-August-30']
  <<load_libs>>
  <<data_file_paths>>
  <<Load_transmitted_data>>

  fig, ax = plt.subplots(1,1,figsize = (10,5))
  with xr.open_dataset(zac_a_path) as ds:
      dsr = ds['dsr'].where(ds['dsr'] != -9999.).to_dataframe()
       #print(t_1)

  new_index = pd.date_range(dsr.index[0],dsr.index[-1], freq = '10min')
  dsr = dsr.reindex(new_index)
  ax.plot(dsr.index.dayofyear, dsr)
  ax.plot(transmitted.index.dayofyear,transmitted['shortwaveradiationin'].astype(float))
  dayofinterest=242 # 30 August 2019
  ax.set_ylim(0,1100)
  ymin,ymax = ax.get_ylim()
  ax.vlines(dayofinterest,ymin,ymax, color = 'black', linestyle = '--' )


  #+END_SRC


****** Temperature
#+NAME: zac_a_preQC_temperature
 #+BEGIN_SRC python

<<load_libs>>
<<data_file_paths>>
<<load_transmitted_trusted>>
  

fig, ax = plt.subplots(1,1,figsize = (10,5))
with xr.open_dataset(zac_a_path) as ds:
    df = ds[['t_1']].to_dataframe()

count10min = df.resample('H').count()
temp_hour = df.resample('H').mean()
temp_hour[count10min<6] = np.nan
#count_hours = temp_hour.resample('D').count()
#count_hours.plot()
#count10min.plot()
#temp_day = temp_hour.resample('D').mean()
#temp_day[count_hours<24 ] = np.nan

#temp_day.plot()
#temp_hour.plot(ax=ax)
temp_hour = pd.DataFrame(temp_hour)
temp_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_temperature.csv', index = True, float_format = '%g')
#temp_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_temperature.csv', index = True, float_format = '%g')

  #+END_SRC



******* Adding the transmitted data 


 So we trust that the data for now and we can concatenate the hourly and the daily temperature record

 Merging the data
    #+BEGIN_SRC python
 temperature_hour_full = pd.concat((t_1_hour['t_1'],transmitted['temperature']), axis=1)
 temperature_hour_full['merged'] = temperature_hour_full[['t_1','temperature']].sum(axis=1, min_count=1)
 temp_hour = pd.DataFrame(temperature_hour_full['merged'].copy())
 temp_hour.rename(columns={'merged':'Air temperature, C'}, inplace = True)
 temp_hour.index.name = 'Timestamp'
 
 #If we had any daily transmissions
 #temp_day_1 = pd.DataFrame(temp_hour['Air temperature, C'].resample('D').mean())
 #temperature_day_full = pd.concat((temp_day_1['Air temperature, C'],transmitted_day['temperature']), axis=1)
 #temperature_day_full['merged'] = temperature_day_full[['Air temperature, C','temperature']].sum(axis=1, min_count=1)
 #temp_day = pd.DataFrame(temperature_day_full['merged'].copy())
 #temp_day.rename(columns={'merged':'Air temperature, C'}, inplace = True)
 #temp_day.index.name = 'Timestamp'
 temp_day = temp_day.resample('D').mean()
 temp_day.plot()
 
 temp_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_temperature.csv', index = True, float_format = '%g')
 temp_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_temperature.csv', index = True, float_format = '%g')
    #+END_SRC



****** Radiation 
 The transmitted radiation data does not look right - and since we do not have the tilt, I think I will declare this data too uncertain to use. 
 So we only use the downloaded data:
#+NAME: zac_a_preQC_radiation
  #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>
    
fig, ax = plt.subplots(1,1,figsize = (10,5))
with xr.open_dataset(zac_a_path) as ds:
 #    ds = ds[['dsr','usr','dlr','ulr']].to_dataframe()
 #    ds['dsr'] = ds['dsr'].where(ds['dsr'] != -9999.)
 #    ds['usr'] = ds['usr'].where(ds['usr'] != -9999.)
    ds = ds[['dsr','usr','dsr_corr','usr_corr','dlr','ulr','albedo','cloud_cov', 't_surf', 'I']].to_dataframe()
    ds['dsr_corr'] = ds['dsr_corr'].where(ds['dsr_corr'] != -9999.)
    ds['usr_corr'] = ds['usr_corr'].where(ds['usr_corr'] != -9999.)
    ds['dlr'] = ds['dlr'].where(ds['dlr'] != -9999.)
    ds['ulr'] = ds['ulr'].where(ds['ulr'] != -9999.)
    

count10min = ds.resample('H').count()
rad_hour = ds.resample('H').mean()
rad_hour[count10min<6] = np.nan
#count_hours = rad_hour.resample('D').count()
# rad_day = rad_hour.resample('D').mean()
# rad_day[count_hours<24 ] = np.nan

#rad_day.plot(ax=ax)

rad_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_radiation.csv', index = True, float_format = '%g')
# rad_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_radiation.csv', index = True, float_format = '%g')


 #+END_SRC


******* Looking into the possibilities of merging with transmitted data
 First load in libraries and data
   #+BEGIN_SRC python
 <<load_libs>>
 <<data_file_paths>>
 <<load_transmitted_trusted>>
 # Converting transmitted radiation to physical units
 # From the nead header 
 dsr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
 usr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
 #dlr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
 #ulr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)

 dsr_trans = (transmitted['shortwaveradiationin']*10) / dsr_eng_coef #* 100
 usr_trans= (transmitted['shortwaveradiationout']*10) / usr_eng_coef #* 100
 #ds['dlr'] = ((ds['dlr']*1000) / dlr_eng_coef) + 5.67*10**(-8)*(ds['t_rad'] + T_0)**4 
 #    ds['ulr'] = ((ds['ulr']*1000) / ulr_eng_coef) + 5.67*10**(-8)*(ds['t_rad'] + T_0)**4


    
 fig, ax = plt.subplots(1,1,figsize = (10,5))
 with xr.open_dataset(zac_a_path) as ds:

     dsr = ds['dsr'].where(ds['dsr'] != -9999.).to_dataframe()
     usr = ds['usr'].where(ds['usr'] !=  -9999.).to_dataframe()
     dlr = ds['dlr'].where(ds['dlr'] != -9999.).to_dataframe()
     ulr = ds['ulr'].where(ds['ulr'] !=  -9999.).to_dataframe()
     #print(t_1)


    


 new_index = pd.date_range(dsr.index[0],t_1.index[-1], freq = '10min')

 #dsr = dsr.reindex(new_index)
 #usr = usr.reindex(new_index)
 #dlr = dlr.reindex(new_index)
 #ulr = ulr.reindex(new_index)



 #fig,ax = plt.subplots(1,1,figsize = (10,5))
 dsr.plot(ax= ax)
 usr.plot(ax=ax)

 #+END_SRC


 #+BEGIN_SRC python
 dsr_hour = dsr.resample('H').mean()
 dsr_hour_full = pd.concat((dsr_hour['dsr'], dsr_trans), axis = 1)
 dsr_hour_full['dsr'].plot(ax=ax)
 dsr_hour_full['shortwaveradiationin'].plot(ax=ax)
 #print(dsr_hour_full)
 dsr_hour_full.plot()
 #+END_SRC

****** Tilt 
 So we only use the downloaded data:
#+NAME: zac_a_preQC_tilt
  #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>
    
fig, ax = plt.subplots(1,1,figsize = (10,5))
with xr.open_dataset(zac_a_path) as ds:
 #    ds = ds[['dsr','usr','dlr','ulr']].to_dataframe()
 #    ds['dsr'] = ds['dsr'].where(ds['dsr'] != -9999.)
 #    ds['usr'] = ds['usr'].where(ds['usr'] != -9999.)
    ds = ds[['tilt_x','tilt_y']].to_dataframe()
    ds['tilt_x'] = ds['tilt_x'].where(ds['tilt_x'] != -9999.)
    ds['tilt_y'] = ds['tilt_y'].where(ds['tilt_y'] != -9999.)
    

count10min = ds.resample('H').count()
tilt_hour = ds.resample('H').mean()
tilt_hour[count10min<6] = np.nan
#count_hours = rad_hour.resample('D').count()
# rad_day = rad_hour.resample('D').mean()
# rad_day[count_hours<24 ] = np.nan

#rad_day.plot(ax=ax)

tilt_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_tilt.csv', index = True, float_format = '%g')
# rad_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_radiation.csv', index = True, float_format = '%g')


 #+END_SRC



******* Looking into the possibilities of merging with transmitted data
 First load in libraries and data
   #+BEGIN_SRC python
 <<load_libs>>
 <<data_file_paths>>
 <<load_transmitted_trusted>>
 # Converting transmitted radiation to physical units
 # From the nead header 
 dsr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
 usr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
 #dlr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
 #ulr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)

 dsr_trans = (transmitted['shortwaveradiationin']*10) / dsr_eng_coef #* 100
 usr_trans= (transmitted['shortwaveradiationout']*10) / usr_eng_coef #* 100
 #ds['dlr'] = ((ds['dlr']*1000) / dlr_eng_coef) + 5.67*10**(-8)*(ds['t_rad'] + T_0)**4 
 #    ds['ulr'] = ((ds['ulr']*1000) / ulr_eng_coef) + 5.67*10**(-8)*(ds['t_rad'] + T_0)**4


    
 fig, ax = plt.subplots(1,1,figsize = (10,5))
 with xr.open_dataset(zac_a_path) as ds:

     dsr = ds['dsr'].where(ds['dsr'] != -9999.).to_dataframe()
     usr = ds['usr'].where(ds['usr'] !=  -9999.).to_dataframe()
     dlr = ds['dlr'].where(ds['dlr'] != -9999.).to_dataframe()
     ulr = ds['ulr'].where(ds['ulr'] !=  -9999.).to_dataframe()
     #print(t_1)


    


 new_index = pd.date_range(dsr.index[0],t_1.index[-1], freq = '10min')

 #dsr = dsr.reindex(new_index)
 #usr = usr.reindex(new_index)
 #dlr = dlr.reindex(new_index)
 #ulr = ulr.reindex(new_index)



 #fig,ax = plt.subplots(1,1,figsize = (10,5))
 dsr.plot(ax= ax)
 usr.plot(ax=ax)

 #+END_SRC

 #+RESULTS:
 :results:
 # Out [48]: 
 # text/plain
 : <AxesSubplot:xlabel='time'>

 # text/plain
 : <Figure size 720x360 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/63eed35ae21f8ca75549ff6a8fac58cad7d70027.png]]
 :end:

 #+BEGIN_SRC python
 dsr_hour = dsr.resample('H').mean()
 dsr_hour_full = pd.concat((dsr_hour['dsr'], dsr_trans), axis = 1)
 dsr_hour_full['dsr'].plot(ax=ax)
 dsr_hour_full['shortwaveradiationin'].plot(ax=ax)
 #print(dsr_hour_full)
 dsr_hour_full.plot()
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [49]: 
 # text/plain
 : <AxesSubplot:>

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/d6a163229cb70f4dd527f578424b340331c0636d.png]]
 :end:







****** Relative humidity
#+NAME: zac_a_preQC_relative_humidity
#+BEGIN_SRC python :session
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_a_path) as ds:
    RH = ds['rh'].to_pandas()
    T = ds['t_1'].to_pandas()

count10min = RH.resample('H').count()

# To get the hourly mean we need to go to specific humidity and back
# defining constants based on Lowe, 1972
a0 = 6.107799961
a1 = 4.436518521*10**(-1)
a2 = 1.428945805*10**(-2)
a3 = 2.650648471*10**(-4)
a4 = 3.031240396*10**(-6)
a5 = 2.034080948*10**(-8)
a6 = 6.136820929*10**(-11)

es = a0 + a1*T+a2*T**2 +a3*T**3+a4*T**4+a5*T**5+a6*T**6
e = RH*es/100

es_hour = es.resample('H').mean()
e_hour = e.resample('H').mean()
rh_hour = 100*e_hour/es_hour

rh_hour_simple =RH.resample('H').mean()
#rh_hour_simple[count10min<6] = np.nan
#rh_hour[count10min<6] = np.nan
 
#rh_day = rh_hour.resample('D').mean()
#rh_day[count_hours<24 ] = np.nan
rh_hour = rh_hour.where(rh_hour.values > 0, np.nan)
rh_hour = rh_hour.where(rh_hour.values <100, np.nan)

#rh_hour.plot()
#rh_hour_simple.plot()

rh_hour = pd.DataFrame(rh_hour)
rh_hour.columns = ['rh']
rh_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_relative_humidity.csv', index = True, float_format = '%g')
#rh_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_relative_humidity.csv', index = True, float_format = '%g')


 #+END_SRC




****** Windspeed
#+NAME: zac_a_preQC_wind_speed
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_a_path) as ds:
    df = ds[['wspd']].to_dataframe()

count10min = df.resample('H').count()
wspd_hour = df.resample('H').mean()
wspd_hour[count10min<6] = np.nan
# count_hours = wspd_hour.resample('D').count()
# count_hours.plot()
# count10min.plot()
# wspd_day = wspd_hour.resample('D').mean()
# wspd_day[count_hours<24 ] = np.nan

# wspd_day.plot()
# wspd_hour.plot()

wspd_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_wind_speed.csv', index = True, float_format = '%g')
# wspd_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_wind_speed.csv', index = True, float_format = '%g')
    
 #+END_SRC


****** pressure
#+NAME: zac_a_preQC_pressure
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_a_path) as ds:
    df = ds[['p']].to_dataframe()

count10min = df.resample('H').count()
p_hour = df.resample('H').mean()
p_hour[count10min<6] = np.nan
#count_hours = p_hour.resample('D').count()
#count_hours.plot()
#count10min.plot()
#p_day = p_hour.resample('D').mean()
#p_day[count_hours<24 ] = np.nan

#p_day.plot()
#p_hour.plot()

p_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_pressure.csv', index = True, float_format = '%g')
#p_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_pressure.csv', index = True, float_format = '%g')
    
 #+END_SRC


****** boom height
#+NAME: zac_a_preQC_boom_height
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_a_path) as ds:
    df = ds[['z_boom']].to_dataframe()

count10min = df.resample('H').count()
z_hour = df.resample('H').median()
z_hour[count10min<6] = np.nan
#count_hours = z_hour.resample('D').count()
#count_hours.plot()
#count10min.plot()
#z_day = z_hour.resample('D').median()
#z_day[count_hours<24 ] = np.nan

#z_day.plot()
#z_hour.plot()

z_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_boom_height.csv', index = True, float_format = '%g')
#z_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_boom_height.csv', index = True, float_format = '%g')
    
 #+END_SRC



****** SR50 on stakes
#+NAME: zac_a_preQC_stake_height
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_a_path) as ds:
    df = ds[['z_stake']].to_dataframe()

count10min = df.resample('H').count()
z_hour = df.resample('H').median()
z_hour[count10min<6] = np.nan
#count_hours = z_hour.resample('D').count()
#count_hours.plot()
#count10min.plot()
#z_day = z_hour.resample('D').median()
#z_day[count_hours<24 ] = np.nan

#z_day.plot()
#z_hour.plot()

z_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_SR50_stake_height.csv', index = True, float_format = '%g')
#z_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_SR50_stake_height.csv', index = True, float_format = '%g')
    
 #+END_SRC



***** Post QC
#+NAME: run_all_postQC_for_zac_a
#+BEGIN_SRC python
<<zac_a_postQC_temperature>>
<<zac_a_postQC_radiation>>
<<zac_a_postQC_tilt>>
<<zac_a_postQC_relative_humidity>>
<<zac_a_postQC_wind_speed>>
<<zac_a_postQC_pressure>>
<<zac_a_postQC_boom_height>>
<<zac_a_postQC_stake_height>>
#+END_SRC
****** Temperature
#+NAME: zac_a_postQC_temperature
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

temp_hour = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)

# Bad data deleted
temp_hour['2015-01-05':'2015-05-01'] = np.nan
temp_hour[:'2009-08-08 21:00'] = np.nan

#count_hours = temp_hour.resample('D').count()
#temp_day = temp_hour.resample('D').mean()
#temp_day[count_hours<24 ] = np.nan

temp_hour.to_csv('data_v1.0/gem_database/2022/zac_a_hour_temperature.csv', index = True, float_format = '%g')
#temp_day.to_csv('data_v1.0/gem_database/2022/zac_a_day_temperature.csv', index = True, float_format = '%g')

 #+END_SRC



****** Radiation
#+NAME: zac_a_postQC_radiation
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

rad_hour = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

 # Deliting bad data: Out of bounds
maximum = 1000
variable = 'dsr_corr'
rad_hour[variable][rad_hour[variable]>maximum] = np.nan
rad_hour[variable][rad_hour[variable]<0] = np.nan

variable = 'usr_corr'
albedo = rad_hour['usr_corr']/rad_hour['dsr_corr']
rad_hour[variable][albedo>1] = np.nan
variable = 'albedo'
rad_hour[variable][albedo>1] = np.nan
 
variable = 'ulr'
rad_hour[variable][rad_hour['ulr']<150] = np.nan
variable = 'cloud_cov'
rad_hour[variable][rad_hour['ulr']<150] = np.nan
variable = 't_surf'
rad_hour[variable][rad_hour['ulr']<150] = np.nan

variable = 'dlr'
rad_hour[variable][rad_hour['dlr']<120] = np.nan
variable = 'cloud_cov'
rad_hour[variable][rad_hour['dlr']<120] = np.nan 
variable = 't_surf'
rad_hour[variable][rad_hour['dlr']<120] = np.nan 

 # Deleting bad data manually
cols = ['usr','dsr','dsr_corr','usr_corr','dlr','ulr','albedo','cloud_cov','t_surf']
rad_hour.loc['2015-01-01':'2015-05-01',cols] = np.nan

 # Then calculate daily averages again
#count_hours = rad_hour.resample('D').count()
#rad_day = rad_hour.resample('D').mean()
#rad_day[count_hours<24 ] = np.nan

rad_hour.to_csv('data_v1.0/gem_database/2022/zac_a_hour_radiation.csv', index = True, float_format = '%g')
# rad_day.to_csv('data_v1.0/gem_database/2022/zac_a_day_radiation.csv', index = True, float_format = '%g')

 #+END_SRC

****** Tilt

#+NAME: zac_a_postQC_tilt
  #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

tilt_hour = pd.read_csv(datapath+'preQC/zac_a_hour_tilt.csv', parse_dates = True, index_col=0)
tilt_hour.to_csv('data_v1.0/gem_database/2022/zac_a_hour_tilt.csv', index = True, float_format = '%g')
# rad_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_radiation.csv', index = True, float_format = '%g')


 #+END_SRC
 
****** Relative humidity
#+NAME: zac_a_postQC_relative_humidity
 #+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rh_hour = pd.read_csv(datapath+'preQC/zac_a_hour_relative_humidity.csv', parse_dates = True, index_col=0)


# Outliers
rh_hour = rh_hour.where(rh_hour['rh']< 100., np.nan)
rh_hour = rh_hour.where(rh_hour['rh']>= 0., np.nan)
# Deleting bad data manually
rh_hour['2014-12-01':'2015-05-01'] = np.nan
rh_hour['2011':'2014'] = np.nan
rh_hour['2016-04-01':] = np.nan

#count_hours = rh_hour.resample('D').count()
#rh_day = rh_hour.resample('D').mean()
#rh_day[count_hours<24 ] = np.nan

rh_hour.to_csv('data_v1.0/gem_database/2022/zac_a_hour_relative_humidity.csv', index = True, float_format = '%g')
#rh_day.to_csv('data_v1.0/gem_database/2022/zac_a_day_relative_humidity.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS: zac_a_postQC_relative_humidity
 : None

 #+RESULTS:

****** Wind speed
#+NAME: zac_a_postQC_wind_speed
 #+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
wspd_hour = pd.read_csv(datapath+'preQC/zac_a_hour_wind_speed.csv', parse_dates = True, index_col=0)


# Bad data
wspd_hour['2020-August-15':'2021-July-21'] = np.nan
wspd_hour['2014-12-01':'2015-05-01'] = np.nan

#count_hours = wspd_hour.resample('D').count()
#wspd_day = wspd_hour.resample('D').mean()
#wspd_day[count_hours<24 ] = np.nan

wspd_hour.to_csv('data_v1.0/gem_database/2022/zac_a_hour_wind_speed.csv', index = True, float_format = '%g')
# wspd_day.to_csv('data_v1.0/gem_database/2022/zac_a_day_wind_speed.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS: zac_a_postQC_wind_speed
 : None


****** Pressure
#+NAME: zac_a_postQC_pressure
 #+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
p_hour = pd.read_csv(datapath+'preQC/zac_a_hour_pressure.csv', parse_dates = True, index_col=0)

#Outliers

p_hour = p_hour.where(p_hour['p']> 800., np.nan)


# Bad Data
p_hour['2020-August-15':'2021-July-21'] = np.nan
p_hour['2014-12-01':'2015-05-01'] = np.nan

#count_hours = p_hour.resample('D').count()
#p_day = p_hour.resample('D').mean()
#p_day[count_hours<24 ] = np.nan

p_hour.to_csv('data_v1.0/gem_database/2022/zac_a_hour_pressure.csv', index = True, float_format = '%g')
#p_day.to_csv('data_v1.0/gem_database/2022/zac_a_day_pressure.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS: zac_a_postQC_pressure
 : None


****** Boom height
#+NAME: zac_a_postQC_boom_height
 #+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
boom_hour = pd.read_csv(datapath+'preQC/zac_a_hour_boom_height.csv', parse_dates = True, index_col=0)

#Outliers

boom_hour = boom_hour.where(boom_hour['z_boom']> 0.1, np.nan)
boom_hour = boom_hour.where(boom_hour['z_boom']< 4, np.nan)

# Bad data
boom_hour['2012-April-16':'2013-August-28'] = np.nan
boom_hour['2013-December-20':'2014-April-22'] = np.nan
boom_hour['2015-January-2':'2016-April-22'] = np.nan
boom_hour['2018-April-20':'2018-April-24'] = np.nan


#count_hours = boom_hour.resample('D').count()
#boom_day = boom_hour.resample('D').median()
#boom_day[count_hours<24 ] = np.nan

boom_hour.to_csv('data_v1.0/gem_database/2022/zac_a_hour_boom_height.csv', index = True, float_format = '%g')
#boom_day.to_csv('data_v1.0/gem_database/2022/zac_a_day_boom_height.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS: zac_a_postQC_boom_height
 : None

**** zac_l
***** Pre QC
#+NAME: run_all_preQC_for_zac_l
#+BEGIN_SRC python


<<zac_l_preQC_temperature>>
<<zac_l_preQC_radiation>>
<<zac_l_preQC_tilt>>
<<zac_l_preQC_relative_humidity>>
<<zac_l_preQC_wind_speed>>
<<zac_l_preQC_pressure>>
<<zac_l_preQC_boom_height>>
<<zac_l_preQC_stake_height>>
#+END_SRC

****** Temperature
#+NAME: zac_l_preQC_temperature
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_l_path) as ds:
    df = ds[['t_1']].to_dataframe()

count10min = df.resample('H').count()
temp_hour = df.resample('H').mean()
temp_hour[count10min<6] = np.nan
#count_hours = temp_hour.resample('D').count()
#count_hours.plot()
#count10min.plot()
#temp_day = temp_hour.resample('D').mean()
#temp_day[count_hours<24 ] = np.nan

#temp_day.plot()
#temp_hour.plot()

temp_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_temperature.csv', index = True, float_format = '%g')
#temp_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_temperature.csv', index = True, float_format = '%g')

 #+END_SRC

 #+RESULTS: zac_l_preQC_temperature
 : None



****** Radiation
#+NAME: zac_l_preQC_radiation
   #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>
    
fig, ax = plt.subplots(1,1,figsize = (10,5))
with xr.open_dataset(zac_l_path) as ds:
     #print(ds)
    ds = ds[['dsr','usr','dsr_corr','usr_corr','dlr','ulr','albedo','cloud_cov','t_surf', 'I']].to_dataframe()

    ds['dsr_corr'] = ds['dsr_corr'].where(ds['dsr_corr'] != -9999.)
    ds['usr_corr'] = ds['usr_corr'].where(ds['usr_corr'] != -9999.)
    ds['dlr'] = ds['dlr'].where(ds['dlr'] != -9999.)
    ds['ulr'] = ds['ulr'].where(ds['ulr'] != -9999.)#.to_dataframe()
    ds['ulr'] = ds['ulr'].where(ds['ulr'] < 10000.)#.to_dataframe()
    ds['dlr'] = ds['dlr'].where(ds['dlr'] < 10000.)#.to_dataframe()

count10min = ds.resample('H').count()
rad_hour = ds.resample('H').mean()
rad_hour[count10min<6] = np.nan
#count_hours = rad_hour.resample('D').count()
#count_hours.plot()
#rad_day = rad_hour.resample('D').mean()
#rad_day[count_hours<24 ] = np.nan

#rad_day.plot(ax=ax)

rad_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_radiation.csv', index = True, float_format = '%g')
#rad_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_radiation.csv', index = True, float_format = '%g')


 #+END_SRC

 #+RESULTS: zac_l_preQC_radiation
 : None


****** Tilt 
 So we only use the downloaded data:
#+NAME: zac_l_preQC_tilt
  #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>
    
fig, ax = plt.subplots(1,1,figsize = (10,5))
with xr.open_dataset(zac_l_path) as ds:
 #    ds = ds[['dsr','usr','dlr','ulr']].to_dataframe()
 #    ds['dsr'] = ds['dsr'].where(ds['dsr'] != -9999.)
 #    ds['usr'] = ds['usr'].where(ds['usr'] != -9999.)
    ds = ds[['tilt_x','tilt_y']].to_dataframe()
    ds['tilt_x'] = ds['tilt_x'].where(ds['tilt_x'] != -9999.)
    ds['tilt_y'] = ds['tilt_y'].where(ds['tilt_y'] != -9999.)
    

count10min = ds.resample('H').count()
tilt_hour = ds.resample('H').mean()
tilt_hour[count10min<6] = np.nan
#count_hours = rad_hour.resample('D').count()
# rad_day = rad_hour.resample('D').mean()
# rad_day[count_hours<24 ] = np.nan

#rad_day.plot(ax=ax)

tilt_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_tilt.csv', index = True, float_format = '%g')
# rad_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_radiation.csv', index = True, float_format = '%g')


 #+END_SRC


******* Looking into the possibilities of merging with transmitted data
 First load in libraries and data
   #+BEGIN_SRC python
 <<load_libs>>
 <<data_file_paths>>
 <<load_transmitted_trusted>>
 # Converting transmitted radiation to physical units
 # From the nead header 
 dsr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
 usr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
 #dlr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
 #ulr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)

 dsr_trans = (transmitted['shortwaveradiationin']*10) / dsr_eng_coef #* 100
 usr_trans= (transmitted['shortwaveradiationout']*10) / usr_eng_coef #* 100
 #ds['dlr'] = ((ds['dlr']*1000) / dlr_eng_coef) + 5.67*10**(-8)*(ds['t_rad'] + T_0)**4 
 #    ds['ulr'] = ((ds['ulr']*1000) / ulr_eng_coef) + 5.67*10**(-8)*(ds['t_rad'] + T_0)**4


    
 fig, ax = plt.subplots(1,1,figsize = (10,5))
 with xr.open_dataset(zac_a_path) as ds:

     dsr = ds['dsr'].where(ds['dsr'] != -9999.).to_dataframe()
     usr = ds['usr'].where(ds['usr'] !=  -9999.).to_dataframe()
     dlr = ds['dlr'].where(ds['dlr'] != -9999.).to_dataframe()
     ulr = ds['ulr'].where(ds['ulr'] !=  -9999.).to_dataframe()
     #print(t_1)


    


 new_index = pd.date_range(dsr.index[0],t_1.index[-1], freq = '10min')

 #dsr = dsr.reindex(new_index)
 #usr = usr.reindex(new_index)
 #dlr = dlr.reindex(new_index)
 #ulr = ulr.reindex(new_index)



 #fig,ax = plt.subplots(1,1,figsize = (10,5))
 dsr.plot(ax= ax)
 usr.plot(ax=ax)

 #+END_SRC

 #+RESULTS:
 :results:
 # Out [48]: 
 # text/plain
 : <AxesSubplot:xlabel='time'>

 # text/plain
 : <Figure size 720x360 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/63eed35ae21f8ca75549ff6a8fac58cad7d70027.png]]
 :end:

 #+BEGIN_SRC python
 dsr_hour = dsr.resample('H').mean()
 dsr_hour_full = pd.concat((dsr_hour['dsr'], dsr_trans), axis = 1)
 dsr_hour_full['dsr'].plot(ax=ax)
 dsr_hour_full['shortwaveradiationin'].plot(ax=ax)
 #print(dsr_hour_full)
 dsr_hour_full.plot()
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [49]: 
 # text/plain
 : <AxesSubplot:>

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/d6a163229cb70f4dd527f578424b340331c0636d.png]]
 :end:









****** Relative humidity
#+NAME: zac_l_preQC_relative_humidity
 #+BEGIN_SRC python :session
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_l_path) as ds:
    RH = ds['rh'].to_pandas()
    T = ds['t_1'].to_pandas()

count10min = RH.resample('H').count()

# To get the hourly mean we need to go to specific humidity and back
# defining constants based on Lowe, 1972
a0 = 6.107799961
a1 = 4.436518521*10**(-1)
a2 = 1.428945805*10**(-2)
a3 = 2.650648471*10**(-4)
a4 = 3.031240396*10**(-6)
a5 = 2.034080948*10**(-8)
a6 = 6.136820929*10**(-11)

es = a0 + a1*T+a2*T**2 +a3*T**3+a4*T**4+a5*T**5+a6*T**6
e = RH*es/100

es_hour = es.resample('H').mean()
e_hour = e.resample('H').mean()

rh_hour = 100*e_hour/es_hour
#rh_hour_simple =RH.resample('H').mean()
#rh_hour_simple[count10min<6] = np.nan
#rh_hour[count10min<6] = np.nan
 
#rh_day = rh_hour.resample('D').mean()
#rh_day[count_hours<24 ] = np.nan
#rh_hour = rh_hour.where(rh_hour.values > 0, np.nan)
#rh_hour = rh_hour.where(rh_hour.values <100, np.nan)

#rh_hour.plot()
#rh_hour_simple.plot()
rh_hour = pd.DataFrame(rh_hour)
rh_hour.columns = ['rh']
rh_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_relative_humidity.csv', index = True, float_format = '%g')
# rh_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_relative_humidity.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS: zac_l_preQC_relative_humidity
 : None


****** Windspeed
#+NAME: zac_l_preQC_wind_speed
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_l_path) as ds:
    df = ds[['wspd']].to_dataframe()

count10min = df.resample('H').count()
wspd_hour = df.resample('H').mean()
wspd_hour[count10min<6] = np.nan
#count_hours = wspd_hour.resample('D').count()
#count_hours.plot()
#count10min.plot()
#wspd_day = wspd_hour.resample('D').mean()
#wspd_day[count_hours<24 ] = np.nan

#wspd_day.plot()
#wspd_hour.plot()

wspd_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_wind_speed.csv', index = True, float_format = '%g')
#wspd_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_wind_speed.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS: zac_l_preQC_wind_speed
 : None


****** pressure
#+NAME: zac_l_preQC_pressure
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_l_path) as ds:
    df = ds[['p']].to_dataframe()

count10min = df.resample('H').count()
p_hour = df.resample('H').mean()
p_hour[count10min<6] = np.nan
#count_hours = p_hour.resample('D').count()
#count_hours.plot()
#count10min.plot()
#p_day = p_hour.resample('D').mean()
#p_day[count_hours<24 ] = np.nan

#p_day.plot()
#p_hour.plot()

p_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_pressure.csv', index = True, float_format = '%g')
#p_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_pressure.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS: zac_l_preQC_pressure
 : None


****** OLD? Surface lowering (z_pt)

#+BEGIN_SRC python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_l_path) as ds:
     lowering = ds['z_pt_corr'].to_dataframe()


 #lowering = df['z_pt_corr'].copy()
 #for year in np.arange(2008,2021+1):
 #    lowering[str(year)] = df['z_pt_corr'][str(year)]-df['z_pt_corr'][str(year)+'-5-1':str(year)+'-5-15'].mean()

 #lowering[lowering<-5] = np.nan
 #lowering[lowering>0.5] = np.nan
 #lowering.plot()
 lowering.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_lowering.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [102]: 
 :end:
 


****** boom height
#+NAME: zac_l_preQC_boom_height
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_l_path) as ds:
    df = ds[['z_boom']].to_dataframe()

count10min = df.resample('H').count()
z_hour = df.resample('H').median()
z_hour[count10min<6] = np.nan
count_hours = z_hour.resample('D').count()
#count_hours.plot()
#count10min.plot()
#z_day = z_hour.resample('D').median()
#z_day[count_hours<24 ] = np.nan

#z_day.plot()
#z_hour.plot()

z_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_boom_height.csv', index = True, float_format = '%g')
#z_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_boom_height.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS: zac_l_preQC_boom_height
 : None

****** SR50 on stakes
#+NAME: zac_l_preQC_stake_height
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_l_path) as ds:
    df = ds[['z_stake']].to_dataframe()

count10min = df.resample('H').count()
z_hour = df.resample('H').median()
z_hour[count10min<6] = np.nan
#count_hours = z_hour.resample('D').count()
#count_hours.plot()
#count10min.plot()
#z_day = z_hour.resample('D').median()
#z_day[count_hours<24 ] = np.nan

#z_day.plot()
#z_hour.plot()

z_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_SR50_stake_height.csv', index = True, float_format = '%g')
#z_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_SR50_stake_height.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS: zac_l_preQC_stake_height
 : None


***** Post QC
#+NAME: run_all_postQC_for_zac_l
#+BEGIN_SRC python
<<zac_l_postQC_temperature>>
<<zac_l_postQC_radiation>>
<<zac_l_postQC_tilt>>
<<zac_l_postQC_relative_humidity>>
<<zac_l_postQC_wind_speed>>
<<zac_l_postQC_pressure>>
<<zac_l_postQC_boom_height>>
<<zac_l_postQC_stake_height>>
#+END_SRC
****** Temperature
#+NAME: zac_l_postQC_temperature
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

temp_hour = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)

#count_hours = temp_hour.resample('D').count()
#temp_day = temp_hour.resample('D').mean()
#temp_day[count_hours<24 ] = np.nan


temp_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_temperature.csv', index = True, float_format = '%g')
#temp_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_temperature.csv', index = True, float_format = '%g')

 #+END_SRC

 #+RESULTS: zac_l_postQC_temperature
 : None


****** Radiation
#+NAME: zac_l_postQC_radiation
#+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

rad_hour = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)

 # Deliting bad data: Out of bounds
maximum = 1000
 
variable = 'dsr_corr'
rad_hour[variable][rad_hour[variable]>maximum] = np.nan
rad_hour[variable][rad_hour[variable]<0] = np.nan

variable = 'usr_corr'
albedo = rad_hour['usr_corr']/rad_hour['dsr_corr']
rad_hour[variable][albedo>1] = np.nan
variable = 'albedo'
rad_hour[variable][albedo>1] = np.nan
 
variable = 'ulr'
rad_hour[variable][rad_hour['ulr']<150] = np.nan
variable = 'cloud_cov'
rad_hour[variable][rad_hour['ulr']<150] = np.nan
variable = 't_surf'
rad_hour[variable][rad_hour['ulr']<150] = np.nan

variable = 'dlr'
rad_hour[variable][rad_hour['dlr']<120] = np.nan
variable = 'cloud_cov'
rad_hour[variable][rad_hour['dlr']<120] = np.nan 
variable = 't_surf'
rad_hour[variable][rad_hour['dlr']<120] = np.nan 

# Deleting bad data manually

#rad_hour['usr_corr']['2021-01-01':'2021-07-21'] = np.nan
#cols = ['usr','dsr','dsr_corr','usr_corr','dlr','ulr','albedo','cloud_cov','t_surf']
#rad_hour.loc['2020-07-01':'2021-07-22',cols] = np.nan

rad_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_radiation.csv', index = True, float_format = '%g')
# rad_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_radiation.csv', index = True, float_format = '%g')

 #+END_SRC

 #+RESULTS: zac_l_postQC_radiation
 : None


****** Tilt

#+NAME: zac_l_postQC_tilt
  #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

tilt_hour = pd.read_csv(datapath+'preQC/zac_l_hour_tilt.csv', parse_dates = True, index_col=0)
tilt_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_tilt.csv', index = True, float_format = '%g')
# rad_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_radiation.csv', index = True, float_format = '%g')


 #+END_SRC

****** Relative humidity
#+NAME: zac_l_postQC_relative_humidity
 #+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rad_hour = pd.read_csv(datapath+'preQC/zac_l_hour_relative_humidity.csv', parse_dates = True, index_col=0)


# Outliers
rad_hour = rad_hour.where(rad_hour<= 100., np.nan)
rad_hour = rad_hour.where(rad_hour>= 0., np.nan)


#count_hours = rad_hour.resample('D').count()
#rad_day = rad_hour.resample('D').mean()
#rad_day[count_hours<24 ] = np.nan

rad_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_relative_humidity.csv', index = True, float_format = '%g')
#rad_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_relative_humidity.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS: zac_l_postQC_relative_humidity
 : None





****** Wind speed
#+NAME: zac_l_postQC_wind_speed
 #+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
wspd_hour = pd.read_csv(datapath+'preQC/zac_l_hour_wind_speed.csv', parse_dates = True, index_col=0)



# count_hours = wspd_hour.resample('D').count()
# wspd_day = wspd_hour.resample('D').mean()
# wspd_day[count_hours<24 ] = np.nan

wspd_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_wind_speed.csv', index = True, float_format = '%g')
#wspd_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_wind_speed.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS: zac_l_postQC_wind_speed
 : None


****** Pressure
#+NAME: zac_l_postQC_pressure
 #+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
p_hour = pd.read_csv(datapath+'preQC/zac_l_hour_pressure.csv', parse_dates = True, index_col=0)

#Outliers

p_hour = p_hour.where(p_hour['p']> 870., np.nan)


p_hour['2016-February-26':'2016-March-1'] = np.nan
p_hour['2017-January-5':'2017-February-22'] = np.nan
p_hour['2018-February-21':'2018-February-28'] = np.nan


count_hours = p_hour.resample('D').count()
p_day = p_hour.resample('D').mean()
p_day[count_hours<24 ] = np.nan

p_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_pressure.csv', index = True, float_format = '%g')
#p_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_pressure.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS: zac_l_postQC_pressure
 : None


****** Boom height
#+NAME: zac_l_postQC_boom_height
 #+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
boom_hour = pd.read_csv(datapath+'preQC/zac_l_hour_boom_height.csv', parse_dates = True, index_col=0)

 #Outliers

boom_hour = boom_hour.where(boom_hour['z_boom']> 0.1, np.nan)
boom_hour = boom_hour.where(boom_hour['z_boom']< 2.75, np.nan)

 # Bad data
boom_hour['2011-January-25':'2013-May-3'] = np.nan
boom_hour['2019':'2020'] = np.nan

#count_hours = boom_hour.resample('D').count()
#boom_day = boom_hour.resample('D').median()
#boom_day[count_hours<24 ] = np.nan

boom_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_boom_height.csv', index = True, float_format = '%g')
#boom_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_boom_height.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS: zac_l_postQC_boom_height
 : None

**** zac_u

***** Pre QC
#+NAME: run_all_preQC_for_zac_u
#+BEGIN_SRC python
<<zac_u_preQC_temperature>>
<<zac_u_preQC_radiation>>
<<zac_u_preQC_tilt>>
<<zac_u_preQC_relative_humidity>>
<<zac_u_preQC_wind_speed>>
<<zac_u_preQC_pressure>>
<<zac_u_preQC_boom_height>>
<<zac_u_preQC_stake_height>>
#+END_SRC
****** Temperature
#+NAME: zac_u_preQC_temperature
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_u_path) as ds:
    df = ds[['t_1']].to_dataframe()

count10min = df.resample('H').count()
temp_hour = df.resample('H').mean()
temp_hour[count10min<6] = np.nan
#count_hours = temp_hour.resample('D').count()
# count_hours.plot()
 #count10min.plot()
# temp_day = temp_hour.resample('D').mean()
# temp_day[count_hours<24 ] = np.nan

# temp_day.plot()
# temp_hour.plot()

temp_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_temperature.csv', index = True, float_format = '%g')
# temp_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_temperature.csv', index = True, float_format = '%g')

 #+END_SRC

 #+RESULTS: zac_u_preQC_temperature
 : None

****** Radiation
#+NAME: zac_u_preQC_radiation
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>
    
#fig, ax = plt.subplots(1,1,figsize = (10,5))
with xr.open_dataset(zac_u_path) as ds:
     #ds = ds[['dsr','usr','dlr','ulr']].to_dataframe()
     #ds['dsr'] = ds['dsr'].where(ds['dsr'] != -9999.)
    ds = ds[['dsr','usr','dsr_corr','usr_corr','dlr','ulr','albedo','cloud_cov','t_surf', 'I']].to_dataframe()

    ds['dsr_corr'] = ds['dsr_corr'].where(ds['dsr_corr'] != -9999.)
    ds['usr_corr'] = ds['usr_corr'].where(ds['usr_corr'] != -9999.)
     #ds['usr'] = ds['usr'].where(ds['usr'] != -9999.)
    ds['dlr'] = ds['dlr'].where(ds['dlr'] != -9999.)
    ds['ulr'] = ds['ulr'].where(ds['ulr'] != -9999.)#.to_dataframe()
    ds['ulr'] = ds['ulr'].where(ds['ulr'] < 10000.)#.to_dataframe()
    ds['dlr'] = ds['dlr'].where(ds['dlr'] < 10000.)#.to_dataframe()


count10min = ds.resample('H').count()
rad_hour = ds.resample('H').mean()
rad_hour[count10min<6] = np.nan
# count_hours = rad_hour.resample('D').count()
# rad_day = rad_hour.resample('D').mean()
# rad_day[count_hours<24 ] = np.nan
 #count_hours['April-2014'].plot()
 #count10min['April-2014'].plot()
 #rad_hour['April-14-2014':'April-15-2014'].plot(ax=ax)

rad_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_radiation.csv', index = True, float_format = '%g')
# rad_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_radiation.csv', index = True, float_format = '%g')

 #+END_SRC

 #+RESULTS: zac_u_preQC_radiation
 : None


****** Tilt 
 So we only use the downloaded data:
#+NAME: zac_u_preQC_tilt
  #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>
    
fig, ax = plt.subplots(1,1,figsize = (10,5))
with xr.open_dataset(zac_u_path) as ds:
 #    ds = ds[['dsr','usr','dlr','ulr']].to_dataframe()
 #    ds['dsr'] = ds['dsr'].where(ds['dsr'] != -9999.)
 #    ds['usr'] = ds['usr'].where(ds['usr'] != -9999.)
    ds = ds[['tilt_x','tilt_y']].to_dataframe()
    ds['tilt_x'] = ds['tilt_x'].where(ds['tilt_x'] != -9999.)
    ds['tilt_y'] = ds['tilt_y'].where(ds['tilt_y'] != -9999.)
    

count10min = ds.resample('H').count()
tilt_hour = ds.resample('H').mean()
tilt_hour[count10min<6] = np.nan
#count_hours = rad_hour.resample('D').count()
# rad_day = rad_hour.resample('D').mean()
# rad_day[count_hours<24 ] = np.nan

#rad_day.plot(ax=ax)

tilt_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_tilt.csv', index = True, float_format = '%g')
# rad_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_radiation.csv', index = True, float_format = '%g')


 #+END_SRC


******* Looking into the possibilities of merging with transmitted data
 First load in libraries and data
   #+BEGIN_SRC python
 <<load_libs>>
 <<data_file_paths>>
 <<load_transmitted_trusted>>
 # Converting transmitted radiation to physical units
 # From the nead header 
 dsr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
 usr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
 #dlr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
 #ulr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)

 dsr_trans = (transmitted['shortwaveradiationin']*10) / dsr_eng_coef #* 100
 usr_trans= (transmitted['shortwaveradiationout']*10) / usr_eng_coef #* 100
 #ds['dlr'] = ((ds['dlr']*1000) / dlr_eng_coef) + 5.67*10**(-8)*(ds['t_rad'] + T_0)**4 
 #    ds['ulr'] = ((ds['ulr']*1000) / ulr_eng_coef) + 5.67*10**(-8)*(ds['t_rad'] + T_0)**4


    
 fig, ax = plt.subplots(1,1,figsize = (10,5))
 with xr.open_dataset(zac_a_path) as ds:

     dsr = ds['dsr'].where(ds['dsr'] != -9999.).to_dataframe()
     usr = ds['usr'].where(ds['usr'] !=  -9999.).to_dataframe()
     dlr = ds['dlr'].where(ds['dlr'] != -9999.).to_dataframe()
     ulr = ds['ulr'].where(ds['ulr'] !=  -9999.).to_dataframe()
     #print(t_1)


    


 new_index = pd.date_range(dsr.index[0],t_1.index[-1], freq = '10min')

 #dsr = dsr.reindex(new_index)
 #usr = usr.reindex(new_index)
 #dlr = dlr.reindex(new_index)
 #ulr = ulr.reindex(new_index)



 #fig,ax = plt.subplots(1,1,figsize = (10,5))
 dsr.plot(ax= ax)
 usr.plot(ax=ax)

 #+END_SRC

 #+RESULTS:
 :results:
 # Out [48]: 
 # text/plain
 : <AxesSubplot:xlabel='time'>

 # text/plain
 : <Figure size 720x360 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/63eed35ae21f8ca75549ff6a8fac58cad7d70027.png]]
 :end:

 #+BEGIN_SRC python
 dsr_hour = dsr.resample('H').mean()
 dsr_hour_full = pd.concat((dsr_hour['dsr'], dsr_trans), axis = 1)
 dsr_hour_full['dsr'].plot(ax=ax)
 dsr_hour_full['shortwaveradiationin'].plot(ax=ax)
 #print(dsr_hour_full)
 dsr_hour_full.plot()
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [49]: 
 # text/plain
 : <AxesSubplot:>

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/d6a163229cb70f4dd527f578424b340331c0636d.png]]
 :end:








****** Relative humidity
#+NAME: zac_u_preQC_relative_humidity
 #+BEGIN_SRC python 
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_u_path) as ds:
    RH = ds['rh'].to_pandas()
    T = ds['t_1'].to_pandas()

count10min = RH.resample('H').count()

# To get the hourly mean we need to go to specific humidity and back
# defining constants based on Lowe, 1972
a0 = 6.107799961
a1 = 4.436518521*10**(-1)
a2 = 1.428945805*10**(-2)
a3 = 2.650648471*10**(-4)
a4 = 3.031240396*10**(-6)
a5 = 2.034080948*10**(-8)
a6 = 6.136820929*10**(-11)

es = a0 + a1*T+a2*T**2 +a3*T**3+a4*T**4+a5*T**5+a6*T**6
e = RH*es/100

es_hour = es.resample('H').mean()
e_hour = e.resample('H').mean()

rh_hour = 100*e_hour/es_hour
#rh_hour_simple =RH.resample('H').mean()
#rh_hour_simple[count10min<6] = np.nan
#rh_hour[count10min<6] = np.nan
 
#rh_day = rh_hour.resample('D').mean()
#rh_day[count_hours<24 ] = np.nan
#rh_hour = rh_hour.where(rh_hour.values > 0, np.nan)
#rh_hour = rh_hour.where(rh_hour.values <100, np.nan)

#rh_hour.plot()
#rh_hour_simple.plot()

rh_hour = pd.DataFrame(rh_hour)
rh_hour.columns = ['rh']
rh_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_relative_humidity.csv', index = True, float_format = '%g')
#rh_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_relative_humidity.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS: zac_u_preQC_relative_humidity
 : None

****** Windspeed
#+NAME: zac_u_preQC_wind_speed
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_u_path) as ds:
    df = ds[['wspd']].to_dataframe()

count10min = df.resample('H').count()
wspd_hour = df.resample('H').mean()
wspd_hour[count10min<6] = np.nan
# count_hours = wspd_hour.resample('D').count()
# count_hours.plot()
# count10min.plot()
# wspd_day = wspd_hour.resample('D').mean()
# wspd_day[count_hours<24 ] = np.nan

# wspd_day.plot()
# wspd_hour.plot()

wspd_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_wind_speed.csv', index = True, float_format = '%g')
# wspd_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_wind_speed.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS: zac_u_preQC_wind_speed
 : None

****** pressure
#+NAME: zac_u_preQC_pressure
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_u_path) as ds:
    df = ds[['p']].to_dataframe()

count10min = df.resample('H').count()
p_hour = df.resample('H').mean()
p_hour[count10min<6] = np.nan
# count_hours = p_hour.resample('D').count()
# count_hours.plot()
# count10min.plot()
# p_day = p_hour.resample('D').mean()
# p_day[count_hours<24 ] = np.nan

# p_day.plot()
# p_hour.plot()

p_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_pressure.csv', index = True, float_format = '%g')
#p_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_pressure.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS: zac_u_preQC_pressure
 : None

****** boom height
#+NAME: zac_u_preQC_boom_height
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>
with xr.open_dataset(zac_u_path) as ds:
    df = ds[['z_boom']].to_dataframe()

count10min = df.resample('H').count()
z_hour = df.resample('H').median()
z_hour[count10min<6] = np.nan
# count_hours = z_hour.resample('D').count()
# count_hours.plot()
# count10min.plot()
# z_day = z_hour.resample('D').median()
# z_day[count_hours<24 ] = np.nan

# z_day.plot()
# z_hour.plot()

z_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_boom_height.csv', index = True, float_format = '%g')
# z_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_boom_height.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS: zac_u_preQC_boom_height
 : None


****** SR50 on stakes
#+NAME: zac_u_preQC_stake_height
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_u_path) as ds:
    df = ds[['z_stake']].to_dataframe()

count10min = df.resample('H').count()
z_hour = df.resample('H').median()
z_hour[count10min<6] = np.nan
# count_hours = z_hour.resample('D').count()
# count_hours.plot()
# count10min.plot()
# z_day = z_hour.resample('D').median()
# z_day[count_hours<24 ] = np.nan

# z_day.plot()
# z_hour.plot()

z_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_SR50_stake_height.csv', index = True, float_format = '%g')
# z_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_SR50_stake_height.csv', index = True, float_format = '%g')
    
 #+END_SRC 

 #+RESULTS: zac_u_preQC_stake_height
 : None
 

***** Post QC

#+NAME: run_all_postQC_for_zac_u
#+BEGIN_SRC python
<<zac_u_postQC_temperature>>
<<zac_u_postQC_radiation>>
<<zac_u_postQC_tilt>>
<<zac_u_postQC_relative_humidity>>
<<zac_u_postQC_wind_speed>>
<<zac_u_postQC_pressure>>
<<zac_u_postQC_boom_height>>
<<zac_u_postQC_stake_height>>
#+END_SRC

****** Temperature
#+NAME: zac_u_postQC_temperature
#+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
temp_hour = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)

 # Bad data deleted
temp_hour['2020-09-15':'2021-08-01'] = np.nan
temp_hour['2014-10-30':'2015-12-31'] = np.nan


#count_hours = temp_hour.resample('D').count()
#temp_day = temp_hour.resample('D').mean()
#temp_day[count_hours<24 ] = np.nan

temp_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_temperature.csv', index = True, float_format = '%g')
#temp_day.to_csv('data_v1.0/gem_database/2022/zac_u_day_temperature.csv', index = True, float_format = '%g')

 #+END_SRC

 #+RESULTS: zac_u_postQC_temperature
 : None

****** Radiation
#+NAME: zac_u_postQC_radiation
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

rad_hour = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)

rad_hour.plot()

# Deliting bad data: Out of bounds
maximum = 1000
variable = 'dsr_corr'
rad_hour[variable][rad_hour[variable]>maximum] = np.nan
rad_hour[variable][rad_hour[variable]<0] = np.nan

variable = 'usr_corr'
albedo = rad_hour['usr_corr']/rad_hour['dsr_corr']
rad_hour[variable][albedo>1] = np.nan
variable = 'albedo'
rad_hour[variable][albedo>1] = np.nan
 
variable = 'ulr'
rad_hour[variable][rad_hour['ulr']<150] = np.nan
variable = 'cloud_cov'
rad_hour[variable][rad_hour['ulr']<150] = np.nan
variable = 't_surf'
rad_hour[variable][rad_hour['ulr']<150] = np.nan

variable = 'dlr'
rad_hour[variable][rad_hour['dlr']<120] = np.nan
variable = 'cloud_cov'
rad_hour[variable][rad_hour['dlr']<120] = np.nan 
variable = 't_surf'
rad_hour[variable][rad_hour['dlr']<120] = np.nan 

cols = ['usr','dsr','dsr_corr','usr_corr','dlr','ulr','albedo','cloud_cov','t_surf']
 # Deleting bad data manually
rad_hour.loc['2020-08-15':'2021-08-01',cols] = np.nan # Station was tilted
rad_hour.loc['2015-01-01':'2015-12-31',cols] = np.nan

rad_hour.loc[:'2012-05-05',cols] = np.nan
#rad_hour.plot()
 # Then calculate daily averages again
#count_hours = rad_hour.resample('D').count()

#rad_day = rad_hour.resample('D').mean()
#rad_day[count_hours<24 ] = np.nan
rad_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_radiation.csv', index = True, float_format = '%g')
# rad_day.to_csv('data_v1.0/gem_database/2022/zac_u_day_radiation.csv', index = True, float_format = '%g')

 #+END_SRC

 #+RESULTS: zac_u_postQC_radiation
 : None

****** Tilt

#+NAME: zac_u_postQC_tilt
  #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

tilt_hour = pd.read_csv(datapath+'preQC/zac_u_hour_tilt.csv', parse_dates = True, index_col=0)
tilt_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_tilt.csv', index = True, float_format = '%g')
# rad_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_radiation.csv', index = True, float_format = '%g')


 #+END_SRC

****** Relative humidity
#+NAME: zac_u_postQC_relative_humidity
 #+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rh_hour = pd.read_csv(datapath+'preQC/zac_u_hour_relative_humidity.csv', parse_dates = True, index_col=0)


# Outliers
rh_hour = rh_hour.where(rh_hour<= 100., np.nan)
rh_hour = rh_hour.where(rh_hour>= 0., np.nan)


 # Bad data
rh_hour['2020-August-15':'2021-July-21'] = np.nan
rh_hour['2014-10-30':'2015-12-31'] = np.nan


 #count_hours = rh_hour.resample('D').count()
 #rh_day = rh_hour.resample('D').mean()
 #rh_day[count_hours<24 ] = np.nan

rh_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_relative_humidity.csv', index = True, float_format = '%g')
 #rh_day.to_csv('data_v1.0/gem_database/2022/zac_u_day_relative_humidity.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS: zac_u_postQC_relative_humidity
 : None



****** Wind speed
#+NAME: zac_u_postQC_wind_speed
#+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
wspd_hour = pd.read_csv(datapath+'preQC/zac_u_hour_wind_speed.csv', parse_dates = True, index_col=0)


# Bad data
wspd_hour['2020-August-15':'2022-July-21'] = np.nan
wspd_hour['2014-10-30':'2015-12-31'] = np.nan

#count_hours = wspd_hour.resample('D').count()
#wspd_day = wspd_hour.resample('D').mean()
#wspd_day[count_hours<24 ] = np.nan

wspd_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_wind_speed.csv', index = True, float_format = '%g')
#wspd_day.to_csv('data_v1.0/gem_database/2022/zac_u_day_wind_speed.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS: zac_u_postQC_wind_speed
 : None

****** Pressure
#+NAME: zac_u_postQC_pressure
 #+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
p_hour = pd.read_csv(datapath+'preQC/zac_u_hour_pressure.csv', parse_dates = True, index_col=0)

 #Outliers

p_hour = p_hour.where(p_hour['p']> 850., np.nan)

# Bad data
p_hour['2014-10-30':'2015-12-31'] = np.nan
p_hour['2016-April-4':'2016-April-18'] = np.nan
p_hour['2017-January-23':'2017-March-2'] = np.nan


#count_hours = p_hour.resample('D').count()
#p_day = p_hour.resample('D').mean()
#p_day[count_hours<24 ] = np.nan

p_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_pressure.csv', index = True, float_format = '%g')
#p_day.to_csv('data_v1.0/gem_database/2022/zac_u_day_pressure.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS: zac_u_postQC_pressure
 : None




 
****** Boom height
#+NAME: zac_u_postQC_boom_height
 #+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
boom_hour = pd.read_csv(datapath+'preQC/zac_u_hour_boom_height.csv', parse_dates = True, index_col=0)

#Outliers

boom_hour = boom_hour.where(boom_hour['z_boom']> 0.1, np.nan)
boom_hour = boom_hour.where(boom_hour['z_boom']< 2.75, np.nan)

# Bad data
boom_hour['2012-January-1':'2016-April-20'] = np.nan
boom_hour['2019-June-20':'2021-July-26'] = np.nan
#boom_hour['2014-10-30':'2015-12-31'] = np.nan



#count_hours = boom_hour.resample('D').count()
#boom_day = boom_hour.resample('D').median()
#boom_day[count_hours<24 ] = np.nan

boom_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_boom_height.csv', index = True, float_format = '%g')
#boom_day.to_csv('data_v1.0/gem_database/2022/zac_u_day_boom_height.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS: zac_u_postQC_boom_height
 : None


*** Glaciological observations


**** PTA

***** raw to check when the pta was changed/redrilled
****** Zac_l
#+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_l_path) as ds:
    df = ds[['z_pt_corr','z_boom', 'albedo']].to_dataframe()

df['z_pt_corr'].plot()

 #+END_SRC

****** Zac_u
#+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_u_path) as ds:
    df = ds[['z_pt_corr','z_boom', 'albedo']].to_dataframe()

df['z_pt_corr'].plot()

 #+END_SRC

 #+RESULTS:
 :RESULTS:
 : <AxesSubplot: xlabel='time'>
 [[file:./.ob-jupyter/fc8876ca90d6de9df0b9e520bd217a2f6e0d1138.png]]
 :END:


***** raw-(ish)
****** Zac_l
#+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_l_path) as ds:
    df = ds[['z_pt_corr','z_boom', 'albedo']].to_dataframe()


lowering = df['z_pt_corr'].copy()
for year in np.arange(2008,2021+1):
    lowering[str(year)] = df['z_pt_corr'][str(year)]-df['z_pt_corr'][str(year)+'-5-15':str(year)+'-5-30'].mean()
#lowering['2010'] = np.nan
#lowering['2019'] = np.nan
lowering[lowering<-5] = np.nan
lowering[lowering>0.5] = np.nan
lowering.plot()
lowering.to_csv('data_v1.0/gem_database/2022/raw/zac_l_pta.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS:
 : None

****** Zac_u
#+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_u_path) as ds:
    df = ds[['z_pt_corr','z_boom', 'albedo']].to_dataframe()

df['z_pt_corr'][:'2017-04-30'] = np.nan
lowering = df['z_pt_corr'].copy()
for year in np.arange(2017,2021+1):
    lowering[str(year)] = df['z_pt_corr'][str(year)]-df['z_pt_corr'][str(year)+'-5-1':str(year)+'-5-15'].mean()
lowering[lowering<-5] = np.nan
lowering[lowering>0.5] = np.nan
lowering.plot()
lowering.to_csv('data_v1.0/gem_database/2022/raw/zac_u_pta.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS:
 : None


 
****** Old/random?
 #+BEGIN_SRC python



 z_pt = df['z_pt_corr']
 z_boom = df['z_boom']
 
 z_pt[:'2017-04-30'] = np.nan
 year = '2019'
 z_pt = z_pt[year+'-May-1':year+'-September-1']
 z_boom  = z_boom [year+'-May-1':year+'-September-1']
 albedo = albedo[year+'-May-1':year+'-September-1']
 albedo_day = albedo.resample('D').mean()
 ax = z_pt[year].plot()
 ax1 = ax.twinx()
 albedo_day[year].plot(ax=ax1, color = 'tab:orange')



#+END_SRC

#+RESULTS:
:RESULTS:
# [goto error]
: [0;31m---------------------------------------------------------------------------[0m
: [0;31mNameError[0m                                 Traceback (most recent call last)
: Cell [0;32mIn[5], line 8[0m
: [1;32m      6[0m z_pt [38;5;241m=[39m z_pt[year[38;5;241m+[39m[38;5;124m'[39m[38;5;124m-May-1[39m[38;5;124m'[39m:year[38;5;241m+[39m[38;5;124m'[39m[38;5;124m-September-1[39m[38;5;124m'[39m]
: [1;32m      7[0m z_boom  [38;5;241m=[39m z_boom [year[38;5;241m+[39m[38;5;124m'[39m[38;5;124m-May-1[39m[38;5;124m'[39m:year[38;5;241m+[39m[38;5;124m'[39m[38;5;124m-September-1[39m[38;5;124m'[39m]
: [0;32m----> 8[0m albedo [38;5;241m=[39m [43malbedo[49m[year[38;5;241m+[39m[38;5;124m'[39m[38;5;124m-May-1[39m[38;5;124m'[39m:year[38;5;241m+[39m[38;5;124m'[39m[38;5;124m-September-1[39m[38;5;124m'[39m]
: [1;32m      9[0m albedo_day [38;5;241m=[39m albedo[38;5;241m.[39mresample([38;5;124m'[39m[38;5;124mD[39m[38;5;124m'[39m)[38;5;241m.[39mmean()
: [1;32m     10[0m ax [38;5;241m=[39m z_pt[year][38;5;241m.[39mplot()
: 
: [0;31mNameError[0m: name 'albedo' is not defined
:END:



#+BEGIN_SRC python
<<get_meltrates_from_z_pt>>
meltrates['rate'].plot()

#+END_SRC

#+RESULTS:
:results:
# Out [3]: 
# text/plain
: <AxesSubplot:xlabel='time'>

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/55813a3b8e1f170eaeb85912e452140c3313291d.png]]
:end:

#+BEGIN_SRC python

meltrates = meltrates.where(meltrates.year != 2010)
meltrates = meltrates.where(meltrates.year != 2019)
#print(meltrates)
meltrates = meltrates.dropna()
meltrates_annual = pd.pivot(meltrates, index = 'doy', columns = 'year', values = 'rate')
meltrates
#print(meltrates_annual)
#+END_SRC


  #+NAME: get_meltrates_from_z_pt
  #+BEGIN_SRC python
  z_pt = -df['z_pt_corr'].resample('d').mean()
  z_pt_diff = z_pt.diff()
  z_pt_diff[np.abs(z_pt_diff) > 0.1] = np.nan
  z_pt_diff[z_pt_diff < 0] = 0
  meltrates = pd.DataFrame(z_pt_diff)
  meltrates.columns = ['rate']
  meltrates['year'] = meltrates.index.year
  meltrates['doy'] = meltrates.index.dayofyear
  meltrates_daymean = meltrates.groupby('doy').rate.mean()
  meltrates_daystd = meltrates.groupby('doy').rate.std()
  meltrates_max = meltrates_daymean+meltrates_daystd
  meltrates_min = meltrates_daymean-meltrates_daystd
  meltrates_min[meltrates_min<0] = 0
  meltrates = meltrates.where(meltrates.doy>150)
  meltrates = meltrates.where(meltrates.doy<250)


  #+END_SRC


***** Removing bad data and prepare preQC data 
****** zac_l
#+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
z_l = pd.read_csv(datapath+'raw/zac_l_pta.csv', parse_dates = True, index_col=0)


z_l.loc[:'2008-7-1'] = np.nan
z_l.loc['2008-8-23':'2008-12-31'] = np.nan
z_l.loc['2008'] = z_l.loc['2008']-z_l.loc['2008-7-2'].mean()

z_l.loc['2009-1-1':'2009-7-4'] = np.nan
z_l.loc['2009-8-16':'2009-12-31'] = np.nan
z_l.loc['2009'] = z_l.loc['2009']-z_l.loc['2009-7-5'].mean()

z_l.loc['2010-1-1':'2010-6-22'] = np.nan
z_l.loc['2010-8-2':'2010-12-31'] = np.nan
z_l.loc['2010'] = z_l.loc['2010']-z_l.loc['2010-6-23'].mean()

z_l.loc['2011-1-1':'2011-6-19'] = np.nan
z_l.loc['2011-8-24':'2011-12-31'] = np.nan
z_l.loc['2011'] = z_l.loc['2011']-z_l.loc['2011-6-20'].mean()

z_l.loc['2012-1-1':'2012-6-29'] = np.nan
z_l.loc['2012-8-24':'2012-12-31'] = np.nan
z_l.loc['2012'] = z_l.loc['2012']-z_l.loc['2012-6-30'].mean()

z_l.loc['2013-1-1':'2013-6-3'] = np.nan
z_l.loc['2013-8-29':'2013-12-31'] = np.nan
z_l.loc['2013'] = z_l.loc['2013']-z_l.loc['2013-6-4'].mean()

z_l.loc['2014-1-1':'2014-7-8'] = np.nan
z_l.loc['2014-8-23':'2014-12-31'] = np.nan
z_l.loc['2014'] = z_l.loc['2014']-z_l.loc['2014-7-8'].mean()

z_l.loc['2015-1-1':'2015-7-3'] = np.nan
z_l.loc['2015-8-21':'2015-12-31'] = np.nan
z_l.loc['2015'] = z_l.loc['2015']-z_l.loc['2015-7-4'].mean()
  
z_l.loc['2016-1-1':'2016-6-25'] = np.nan
z_l.loc['2016-8-29':'2016-12-31'] = np.nan
z_l.loc['2016'] = z_l.loc['2016']-z_l.loc['2016-6-26'].mean()

z_l.loc['2017-1-1':'2017-6-27'] = np.nan
z_l.loc['2017-8-31':'2017-12-31'] = np.nan
z_l.loc['2017'] = z_l.loc['2017']-z_l.loc['2017-6-28'].mean()

z_l.loc['2018-1-1':'2018-7-30'] = np.nan
z_l.loc['2018-8-24':'2018-12-31'] = np.nan
z_l.loc['2018'] = z_l.loc['2018']-z_l.loc['2018-7-31'].mean()

z_l.loc['2019'] = np.nan

z_l.loc['2020-1-1':'2020-6-19'] = np.nan
z_l.loc['2020-8-28':'2020-12-31'] = np.nan
z_l.loc['2020'] = z_l.loc['2020']-z_l.loc['2020-6-20'].mean()

z_l.loc['2021-1-1':'2021-6-16'] = np.nan
z_l.loc['2021-8-17':'2021-12-31'] = np.nan
z_l.loc['2021'] = z_l.loc['2021']-z_l.loc['2021-6-17'].mean()




month = z_l.index.month
z_l[month<4] = np.nan
z_l[month>9] = np.nan

z_l_hour = z_l.resample('H').mean()
z_l_day = z_l.resample('D').mean() 
z_l_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_pta.csv', index = True, float_format = '%g')
z_l_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_pta.csv', index = True, float_format = '%g')
  #+END_SRC

  #+RESULTS:
  : None


******* 2008
   #+BEGIN_SRC python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'preQC/zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2008'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2008'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2008,7,2),ymin,ymax)

   lowering[:'2008-July-1'] = np.nan
   lowering['2008'] = lowering-lowering['2008-July-2'].mean()
   #+END_SRC

   #+RESULTS:
   :results:
   # Out [104]: 
   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/e83e63d284a0f4967bf4b8a95f2183cfbfe77467.png]]
   :end:


******* 2009
   #+BEGIN_SRC python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2009'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2009'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2009,6,25),ymin,ymax)
   ax.vlines(datetime.datetime(2009,5,14), ymin,ymax)
   #z_l['2009'] = z_l['2009']+z_l['2009-June-26'].mean()
   z_l['2009-5-14':'2009-6-24']= np.nan
   #+END_SRC

   #+RESULTS:
   :results:
   # Out [12]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/b25e32aaa99c20fa3b7362736f94187525d551e7.png]]
   :end:


******* 2010
   #+BEGIN_SRC python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2010'].plot(ax = ax)
   ax1 = ax.twinx()
   #albedo['2010'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2010,5,15),ymin,ymax)
   ax.vlines(datetime.datetime(2010,8,2),ymin,ymax)

   #z_l['2010-1-1':'2010-5-15'] = np.nan
   #z_l['2010'] = z_l['2010']-z_l['2010-5-16'].mean() 
   #z_l['2010-8-2':'2010-12-31'] = np.nan
   #+END_SRC

   #+RESULTS:
   :results:
   # Out [98]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/36e271e6526b10bd2e5c45b2cef6cc4d47ec006f.png]]
   :end:


******* 2011
   #+BEGIN_SRC python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2011'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2011'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2011,5,7),ymin,ymax, color = 'red')


   z_l['2011-1-1':'2010-5-7'] = np.nan
   z_l['2011'] = z_l['2011']-z_l['2011-5-8'].mean() 

   #+END_SRC

   #+RESULTS:
   :results:
   # Out [114]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/c55b60b88003c43f616769be0e8d2b4fdc147b88.png]]
   :end:




******* 2012
   #+BEGIN_SRC python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2012'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2012'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2012,4,18),ymin,ymax, color = 'red')


   #z_l['2012-1-1':'2012-4-18'] = np.nan
   #z_l['2012'] = z_l['2012']-z_l['2012-4-18'].mean() 

   #+END_SRC

   #+RESULTS:
   :results:
   # Out [124]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/5ed75104e9c20bdb4648e6be427a358b9cab2eca.png]]
   :end:

******* 2013

   #+BEGIN_SRC python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2013'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2013'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2013,4,18),ymin,ymax, color = 'red')


   #z_l['2012-1-1':'2012-4-18'] = np.nan
   z_l['2013'] = z_l['2013']-z_l['2013-4-1':'2013-5-1'].mean() 

   #+END_SRC

   #+RESULTS:
   :results:
   # Out [131]: 
   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/0e90ee31623366285b06b2bf1bbcfef35ebca3b6.png]]
   :end:


******* 2014

   #+BEGIN_SRC python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2014'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2014'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2014,4,18),ymin,ymax, color = 'red')


   #z_l['2012-1-1':'2012-4-18'] = np.nan
   z_l['2014'] = z_l['2014']-z_l['2014-5-1':'2014-6-1'].mean() 

   #+END_SRC

   #+RESULTS:
   :results:
   # Out [134]: 
   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/fea1b2eef56c24331f0e6d21f3df32679664ab6b.png]]
   :end:


******* 2015

   #+BEGIN_SRC python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2015'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2015'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2015,6,2),ymin,ymax, color = 'red')


   #z_l['2015-1-1':'2015-6-2'] = np.nan
   z_l['2015'] = z_l['2015']-z_l['2015-6-3':'2015-6-15'].mean() 
   #z_l['2015-12-1':'2015-12-31'] = np.nan
   #+END_SRC

   #+RESULTS:
   :results:
   # Out [175]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/1a4082d7464247d2731202eaf80dd6b9b6e445c8.png]]
   :end:

******* 2016

   #+BEGIN_SRC python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2016'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2016'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2016,5,1),ymin,ymax, color = 'red')


   z_l['2016-1-1':'2016-5-1'] = np.nan
   z_l['2016'] = z_l['2016']-z_l['2016-5-2':'2016-5-15'].mean() 

   #+END_SRC

   #+RESULTS:
   :results:
   # Out [146]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/292ab78ee12489e9c74c76bb840b02365fae52c5.png]]
   :end:


******* 2017

   #+BEGIN_SRC python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2017'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2017'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2017,5,1),ymin,ymax, color = 'red')


   z_l['2017-1-1':'2016-5-1'] = np.nan
   z_l['2017'] = z_l['2017']-z_l['2017-5-2':'2017-5-15'].mean() 

   #+END_SRC

   #+RESULTS:
   :results:
   # Out [152]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/0261497536b15dfac40fa9634d771882212b968e.png]]
   :end:


******* 2018

   #+BEGIN_SRC python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2018'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2018'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2018,5,1),ymin,ymax, color = 'red')


   z_l['2018-1-1':'2018-5-1'] = np.nan
   z_l['2018'] = z_l['2018']-z_l['2018-5-2':'2018-5-15'].mean() 

   #+END_SRC

   #+RESULTS:
   :results:
   # Out [159]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/7e2fbe2bd65d8ac3a4ee42033aa12c46f5c7cda1.png]]
   :end:


******* 2019

   #+BEGIN_SRC python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2019'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2019'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2019,5,1),ymin,ymax, color = 'red')

   z_l['2019'] = np.nan
   #z_l['2018-1-1':'2018-5-1'] = np.nan
   #z_l['2018'] = z_l['2018']-z_l['2018-5-2':'2018-5-15'].mean() 

   #+END_SRC

   #+RESULTS:
   :results:
   # Out [162]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/1d8a64d218214436b6b758b2dde74118f37cabde.png]]
   :end:


******* 2020

   #+BEGIN_SRC python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2020'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2020'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2019,5,1),ymin,ymax, color = 'red')

   #z_l['2020'] = np.nan
   #z_l['2018-1-1':'2018-5-1'] = np.nan
   z_l['2020'] = z_l['2020']-z_l['2020-5-2':'2020-5-15'].mean() 

   #+END_SRC

   #+RESULTS:
   :results:
   # Out [165]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/dde540a0359743eba4799efd5e0564b0071cfd33.png]]
   :end:


******* 2021

   #+BEGIN_SRC python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2021'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2021'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2019,5,1),ymin,ymax, color = 'red')

   #z_l['2020'] = np.nan
   z_l['2021-9-1':'2021-12-31'] = np.nan
   z_l['2021'] = z_l['2021']-z_l['2021-5-2':'2021-5-15'].mean()


   #+END_SRC

   #+RESULTS:
   :results:
   # Out [168]: 
   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/e81e14238f8c7a1d69a8468ea480be04d909aaa1.png]]
   :end:

****** zac_u
#+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
z_u = pd.read_csv(datapath+'raw/zac_u_pta.csv', parse_dates = True, index_col=0)



z_u.loc['2017'] = z_u.loc['2017']-z_u.loc['2017-July-17':'2017-July-17'].mean()

z_u.loc[:'2017-July-18'] = np.nan
z_u.loc['2018'] = np.nan
z_u.loc['2019'] = z_u.loc['2019']-z_u.loc['2019-June-28'].mean()
z_u.loc['2020'] = z_u.loc['2020']-z_u.loc['2020-June-15':'2020-June-22'].mean()
z_u.loc['2020-January-1':'2020-June-15'] = np.nan
z_u.loc['2020-August-12':'2020-December-31'] = np.nan

z_u.loc['2021'] = z_u.loc['2021']-z_u.loc['2021-April-15':'2021-April-22'].mean()
z_u.loc['2021-January-1':'2021-July-27'] = np.nan
z_u.loc['2021-September-1':'2021-December-31'] = np.nan

month = z_u.index.month
z_u[month<4] = np.nan
z_u[month>9] = np.nan

z_u_hour = z_u.resample('H').mean()
z_u_day = z_u.resample('D').mean() 
z_u_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_pta.csv', index = True, float_format = '%g')
z_u_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_pta.csv', index = True, float_format = '%g')

z_u_hour.plot()
#+END_SRC

#+RESULTS:
: Axes(0.125,0.11;0.775x0.77)

******* 2017
   #+BEGIN_SRC python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_u = pd.read_csv(datapath+'zac_u_hour_lowering.csv', parse_dates = True, index_col=0)
   rad_u = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)


   albedo = (rad_u['usr_corr'].resample('D').sum()/rad_u['usr_corr'].resample('D').count())/(rad_u['dsr_corr'].resample('D').sum()/rad_u['dsr_corr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_u['2017'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2017'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2017,7,18),ymin,ymax)
   ax.vlines(datetime.datetime(2017,8,31),ymin,ymax)

   #+END_SRC

   #+RESULTS:
   :RESULTS:
   : /tmp/ipykernel_9944/3634662671.py:14: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.
   :   z_u['2017'].plot(ax = ax)
   : <matplotlib.collections.LineCollection at 0x7f626065a550>
   [[file:./.ob-jupyter/8b12c71b89d6d0226bdde3144730105187b51b48.png]]
   :END:

******* 2020
   #+BEGIN_SRC python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_u = pd.read_csv(datapath+'zac_u_hour_lowering.csv', parse_dates = True, index_col=0)
   rad_u = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)


   albedo = (rad_u['usr_corr'].resample('D').sum()/rad_u['usr_corr'].resample('D').count())/(rad_u['dsr_corr'].resample('D').sum()/rad_u['dsr_corr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_u['2020'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2020'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2020,6,22),ymin,ymax)
   ax.vlines(datetime.datetime(2020,8,12),ymin,ymax)

   #+END_SRC

   #+RESULTS:
   :RESULTS:
   : /tmp/ipykernel_9944/62226053.py:14: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.
   :   z_u['2020'].plot(ax = ax)
   : <matplotlib.collections.LineCollection at 0x7f625f672f70>
   [[file:./.ob-jupyter/c1e68d2286a0d89dc40d9827b4927e64d223f2bc.png]]
   :END:


******* 2021
   #+BEGIN_SRC python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_u = pd.read_csv(datapath+'zac_u_hour_lowering.csv', parse_dates = True, index_col=0)
   rad_u = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)


   albedo = (rad_u['usr_corr'].resample('D').sum()/rad_u['usr_corr'].resample('D').count())/(rad_u['dsr_corr'].resample('D').sum()/rad_u['dsr_corr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_u['2021'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2021'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2021,7,25),ymin,ymax)
   ax.vlines(datetime.datetime(2021,8,29),ymin,ymax)

   #+END_SRC

   #+RESULTS:
   :RESULTS:
   : /tmp/ipykernel_9944/257407625.py:14: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.
   :   z_u['2021'].plot(ax = ax)
   : <matplotlib.collections.LineCollection at 0x7f625db73070>
   [[file:./.ob-jupyter/c8cc83ed66fbd0b1ffab29fbd96b8885d342f8bd.png]]
   :END:













**** SR50 on stakes
***** preQC
****** zac_l
 #+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_l_path) as ds:
    df = ds[['z_stake']].to_dataframe()

count10min = df.resample('H').count()
z_hour = df.resample('H').median()
z_hour[count10min<6] = np.nan
#count_hours = z_hour.resample('D').count()
# count_hours.plot()
# count10min.plot()
# z_day = z_hour.resample('D').median()
# z_day[count_hours<24 ] = np.nan

# z_day.plot()
# z_hour.plot()

z_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_SR50_stake_height.csv', index = True, float_format = '%g')
# z_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_SR50_stake_height.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS:
 : None


****** zac_u
#+BEGIN_SRC python
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_u_path) as ds:
    df = ds[['z_stake']].to_dataframe()

count10min = df.resample('H').count()
z_hour = df.resample('H').median()
z_hour[count10min<6] = np.nan
# count_hours = z_hour.resample('D').count()
# count_hours.plot()
# count10min.plot()
# z_day = z_hour.resample('D').median()
# z_day[count_hours<24 ] = np.nan

# z_day.plot()
# z_hour.plot()

z_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_SR50_stake_height.csv', index = True, float_format = '%g')
# z_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_SR50_stake_height.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS:
 : None
 

***** Prepare preQC data for comparison with ice ablation, setting to zero at ice melt start)

****** zac_l
#+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
z_l = pd.read_csv(datapath+'preQC/zac_l_hour_SR50_stake_height.csv', parse_dates = True, index_col=0)

#z_l.loc['2008'] = z_l.loc['2008']-z_l.loc['2008-July-2'].mean()
#z_l.loc['2009'] = z_l.loc['2009']-z_l.loc['2009-July-5'].mean()
#z_l.loc['2010'] = z_l.loc['2010']-z_l.loc['2010-6-23'].mean()
#z_l.loc['2011'] = z_l.loc['2011']-z_l.loc['2011-6-20'].mean()
#z_l.loc['2012'] = z_l.loc['2012']-z_l.loc['2012-6-30'].mean()
#z_l.loc['2013'] = z_l.loc['2013']-z_l.loc['2013-6-4'].mean()
#z_l.loc['2014'] = z_l.loc['2014']-z_l.loc['2014-7-9'].mean()
#z_l.loc['2015'] = z_l.loc['2015']-z_l.loc['2015-7-1'].mean() 
#z_l.loc['2016'] = z_l.loc['2016']-z_l.loc['2016-6-26'].mean()
#z_l.loc['2017'] = z_l.loc['2017']-z_l.loc['2017-6-28'].mean()
#z_l.loc['2018'] = z_l.loc['2018']-z_l.loc['2018-7-31'].mean() 
#z_l.loc['2019'] = z_l.loc['2019']-z_l.loc['2019-6-13'].mean()
#z_l.loc['2020'] = z_l.loc['2020']-z_l.loc['2020-6-20'].mean()
#z_l.loc['2021'] = z_l.loc['2021']-z_l.loc['2021-6-17'].mean()


z_l.loc[:'2008-7-1'] = np.nan
z_l.loc['2008-8-23':'2008-12-31'] = np.nan
z_l.loc['2008'] = z_l.loc['2008']-z_l.loc['2008-7-2'].mean()

z_l.loc['2009-1-1':'2009-7-4'] = np.nan
z_l.loc['2009-7-18':'2009-12-31'] = np.nan
z_l.loc['2009'] = z_l.loc['2009']-z_l.loc['2009-7-5'].mean()

z_l.loc['2010-1-1':'2010-6-22'] = np.nan
z_l.loc['2010-8-2':'2010-12-31'] = np.nan
z_l.loc['2010'] = z_l.loc['2010']-z_l.loc['2010-6-23'].mean()

z_l.loc['2011-1-1':'2011-6-19'] = np.nan
z_l.loc['2011-8-24':'2011-12-31'] = np.nan
z_l.loc['2011'] = z_l.loc['2011']-z_l.loc['2011-6-20'].mean()

z_l.loc['2012-1-1':'2012-6-29'] = np.nan
z_l.loc['2012-8-24':'2012-12-31'] = np.nan
z_l.loc['2012'] = z_l.loc['2012']-z_l.loc['2012-6-30'].mean()

z_l.loc['2013-1-1':'2013-6-3'] = np.nan
z_l.loc['2013-8-29':'2013-12-31'] = np.nan
z_l.loc['2013'] = z_l.loc['2013']-z_l.loc['2013-6-4'].mean()

z_l.loc['2014-1-1':'2014-7-8'] = np.nan
z_l.loc['2014-8-23':'2014-12-31'] = np.nan
z_l.loc['2014'] = z_l.loc['2014']-z_l.loc['2014-7-8'].mean()

z_l.loc['2015-1-1':'2015-7-3'] = np.nan
z_l.loc['2015-8-21':'2015-12-31'] = np.nan
z_l.loc['2015'] = z_l.loc['2015']-z_l.loc['2015-7-4'].mean()
  
z_l.loc['2016-1-1':'2016-6-25'] = np.nan
z_l.loc['2016-8-29':'2016-12-31'] = np.nan
z_l.loc['2016'] = z_l.loc['2016']-z_l.loc['2016-6-26'].mean()

z_l.loc['2017-1-1':'2017-6-27'] = np.nan
z_l.loc['2017-8-31':'2017-12-31'] = np.nan
z_l.loc['2017'] = z_l.loc['2017']-z_l.loc['2017-6-28'].mean()

z_l.loc['2018-1-1':'2018-7-30'] = np.nan
z_l.loc['2018-8-24':'2018-12-31'] = np.nan
z_l.loc['2018'] = z_l.loc['2018']-z_l.loc['2018-7-31'].mean()

z_l.loc['2019'] = np.nan

z_l.loc['2020-1-1':'2020-6-19'] = np.nan
z_l.loc['2020-8-28':'2020-12-31'] = np.nan
z_l.loc['2020'] = z_l.loc['2020']-z_l.loc['2020-6-20'].mean()

z_l.loc['2021-1-1':'2021-6-16'] = np.nan
z_l.loc['2021-8-17':'2021-12-31'] = np.nan
z_l.loc['2021'] = z_l.loc['2021']-z_l.loc['2021-6-17'].mean()



month = z_l.index.month
z_l[month<4] = np.nan
z_l[month>9] = np.nan

z_l_hour = z_l.resample('H').mean()
z_l_day = z_l.resample('D').mean() 
z_l_hour.loc['July-2009'].plot()
z_l_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_SR50_stake_height_ice.csv', index = True, float_format = '%g')
z_l_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_SR50_stake_height_ice.csv', index = True, float_format = '%g')
#+END_SRC

#+RESULTS:
: None

****** zac_u



#+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
z_u = pd.read_csv(datapath+'preQC/zac_u_hour_SR50_stake_height.csv', parse_dates = True, index_col=0)

z_u.loc['2008'] = z_u.loc['2008']-z_u.loc['2008-July-2'].mean()
z_u.loc['2009'] = z_u.loc['2009']-z_u.loc['2009-July-5'].mean()
z_u.loc['2010'] = z_u.loc['2010']-z_u.loc['2010-July-5'].mean()
z_u.loc['2011'] = z_u.loc['2011']-z_u.loc['2011-June-20'].mean()
z_u.loc['2012'] = z_u.loc['2012']-z_u.loc['2012-July-11'].mean()

z_u.loc['2013'] = z_u.loc['2013']-z_u.loc['2013-June-4'].mean()
z_u.loc['2014'] = z_u.loc['2014']-z_u.loc['2014-July-20'].mean()
z_u.loc['2015'] = z_u.loc['2015']-z_u.loc['2015-July-4'].mean()
z_u.loc['2016'] = z_u.loc['2016']-z_u.loc['2016-July-1'].mean()
z_u.loc['2017'] = z_u.loc['2017']-z_u.loc['2017-July-18'].mean()

z_u.loc['2018'] = z_u.loc['2018']-z_u.loc['2018-July-31'].mean()
z_u.loc['2019'] = z_u.loc['2019']-z_u.loc['2019-June-27'].mean()
z_u.loc['2020'] = z_u.loc['2020']-z_u.loc['2020-June-22'].mean()
z_u.loc['2021'] = z_u.loc['2021']-z_u.loc['2021-June-17'].mean()
  


month = z_u.index.month
z_u[month<4] = np.nan
z_u[month>9] = np.nan

z_u_hour = z_u.resample('H').mean()
z_u_day = z_u.resample('D').mean() 
z_u_hour.plot()
z_u_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_SR50_stake_height_ice.csv', index = True, float_format = '%g')
z_u_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_SR50_stake_height_ice', index = True, float_format = '%g')
#+END_SRC

#+RESULTS:
: None



**** Compare PTA and SR50 and create an ice ablation dataseries
The QC process is iterative - I check PTA against SR50, but I also see if I am able to model the melt within realistic bounds. If not, there might be something wrong with the instrument - for example a wrong calibration coefficient. 


***** zac_l
#+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
pta = pd.read_csv(datapath+'preQC/zac_l_hour_pta.csv', parse_dates = True, index_col=0)
sr50 = pd.read_csv(datapath+'preQC/zac_l_hour_SR50_stake_height_ice.csv', parse_dates = True, index_col=0)
#z_u = pd.read_csv(datapath+'zac_u_hour_lowering.csv', parse_dates = True, index_col=0)
pta = -pta




fig, ax = plt.subplots(6,1,figsize = (10,15))
#pta.loc['2008'].plot(ax = ax[0])
#sr50.loc['2008'].plot(ax = ax[0])
#pta.loc['2009'].plot(ax = ax[1])
#sr50.loc['2009'].plot(ax = ax[1])
#pta.loc['2010'].plot(ax = ax[2])
#sr50.loc['2010'].plot(ax = ax[2])
#pta.loc['2011'].plot(ax = ax[3])
#sr50.loc['2011'].plot(ax = ax[3])
#pta.loc['2012'].plot(ax = ax[4])
#sr50.loc['2012'].plot(ax = ax[4])
#pta.loc['2013'].plot(ax = ax[5])
#sr50.loc['2013'].plot(ax = ax[5])
pta.loc['2014'].plot(ax = ax[0])
sr50.loc['2014'].plot(ax = ax[0])
pta.loc['2015'].plot(ax = ax[1])
sr50.loc['2015'].plot(ax = ax[1])
pta.loc['2016'].plot(ax = ax[2])
sr50.loc['2016'].plot(ax = ax[2])
pta.loc['2017'].plot(ax = ax[3])
sr50.loc['2017'].plot(ax = ax[3])
pta.loc['2018'].plot(ax = ax[4])
sr50.loc['2018'].plot(ax = ax[4])
pta.loc['2019'].plot(ax = ax[5])
sr50.loc['2019'].plot(ax = ax[5])

#pta.loc['2020'].plot(ax = ax[0])
#sr50.loc['2020'].plot(ax = ax[0])
#pta.loc['2021'].plot(ax = ax[1])
#sr50.loc['2021'].plot(ax = ax[1])



ablation = pta.copy()
ablation.rename(columns = {'z_pt_corr':'ice_ablation'}, inplace = True)
#ablation.loc['2010'] = sr50.loc['2010']
#ablation.loc['2015'] = sr50.loc['2015']
#ablation.loc['2016'] = sr50.loc['2016']

ablation_hour = ablation.resample('H').mean()
ablation_day = ablation.resample('D').mean() 
ablation_hour.plot()
ablation_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_ice_ablation.csv', index = True, float_format = '%g')
ablation_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_ice_ablation.csv', index = True, float_format = '%g')

#+END_SRC

#+RESULTS:
: None




***** zac_u
#+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
pta = pd.read_csv(datapath+'preQC/zac_u_hour_pta.csv', parse_dates = True, index_col=0)
sr50 = pd.read_csv(datapath+'preQC/zac_u_hour_SR50_stake_height_ice.csv', parse_dates = True, index_col=0)
#z_u = pd.read_csv(datapath+'zac_u_hour_lowering.csv', parse_dates = True, index_col=0)
pta = -pta

fig, ax = plt.subplots(6,1,figsize = (10,15))
#pta.loc['2008'].plot(ax = ax[0])
#sr50.loc['2008'].plot(ax = ax[0])
#pta.loc['2009'].plot(ax = ax[1])
#sr50.loc['2009'].plot(ax = ax[1])
#pta.loc['2010'].plot(ax = ax[2])
#sr50.loc['2010'].plot(ax = ax[2])
#pta.loc['2011'].plot(ax = ax[3])
#sr50.loc['2011'].plot(ax = ax[3])
#pta.loc['2012'].plot(ax = ax[4])
#sr50.loc['2012'].plot(ax = ax[4])
#pta.loc['2013'].plot(ax = ax[5])
#sr50.loc['2013'].plot(ax = ax[5])

#pta.loc['2014'].plot(ax = ax[0])
#sr50.loc['2014'].plot(ax = ax[0])
#pta.loc['2015'].plot(ax = ax[1])
#sr50.loc['2015'].plot(ax = ax[1])
#pta.loc['2016'].plot(ax = ax[2])
#sr50.loc['2016'].plot(ax = ax[2])
#pta.loc['2017'].plot(ax = ax[3])
#sr50.loc['2017'].plot(ax = ax[3])
#pta.loc['2018'].plot(ax = ax[4])
#sr50.loc['2018'].plot(ax = ax[4])
#pta.loc['2019'].plot(ax = ax[5])
#sr50.loc['2019'].plot(ax = ax[5])
pta.loc['2020'].plot(ax = ax[0])
sr50.loc['2020'].plot(ax = ax[0])
pta.loc['2021'].plot(ax = ax[1])
sr50.loc['2021'].plot(ax = ax[1])
pta.loc['2022'].plot(ax = ax[2])
sr50.loc['2022'].plot(ax = ax[2])

ablation = pta.copy()
ablation.rename(columns = {'z_pt_corr':'ice_ablation'}, inplace = True)

ablation.loc['2011'] = sr50.loc['2011']
ablation.loc['2014'] = sr50.loc['2014']
ablation.loc['2016'] = sr50.loc['2016']

ablation_hour = ablation.resample('H').mean()
ablation_day = ablation.resample('D').mean() 
ablation_hour.plot()
ablation_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_ice_ablation.csv', index = True, float_format = '%g')
ablation_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_ice_ablation.csv', index = True, float_format = '%g')

#+END_SRC

#+RESULTS:
: None

**** Final filtering of ice ablation
****** zac_l
This is just a plot
#+BEGIN_SRC python :results output
<<load_libs>>
import matplotlib.dates as mdates
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
pta = pd.read_csv(datapath+'preQC/zac_l_hour_pta.csv', parse_dates = True, index_col=0)
sr50 = pd.read_csv(datapath+'zac_l_hour_SR50_stake_height_ice.csv', parse_dates = True, index_col=0)
z_l = pd.read_csv(datapath+'preQC/zac_l_hour_ice_ablation.csv', parse_dates = True, index_col=0)
pta = -pta

pta = pta.resample('D').mean()
sr50 = sr50.resample('D').mean()

pta_filtered = pta[pta.index.month.isin([6, 7, 8])]
sr50_filtered = sr50[sr50.index.month.isin([6,7, 8])]
# Convert dates to strings
pta_filtered['date_str'] = pta_filtered.index.strftime('%Y-%m-%d')
sr50_filtered['date_str'] = sr50_filtered.index.strftime('%Y-%m-%d')
# Plot

# generate annual ticks and labels
july_ticks = pd.date_range(start=pta_filtered.index.min(), end=pta_filtered.index.max(), freq='AS-JUL')
july_xticks = july_ticks.strftime('%Y-%m-%d')
july_labels = july_ticks.strftime('%Y')

plt.figure(figsize=(8, 3))
plt.plot(pta_filtered['date_str'], pta_filtered['z_pt_corr'], label = '$Z_{pta}$')
plt.plot(sr50_filtered['date_str'], sr50_filtered['z_stake'], label = '$Z_{stake}$')
plt.xticks(july_xticks,july_labels, rotation=90)
plt.ylabel('m ice')
plt.ylim(0,3.8)
plt.legend()

plt.tight_layout()


#fig.savefig('QCfigs/PTA_vs_SR50.png', dpi = 300)
plt.savefig('../glaciobasis/essd/manuscript/figures/fig11.png', dpi = 300)
#+END_SRC

#+RESULTS:
#+begin_example
/tmp/babel-UnOTLq/python-gjz9l1:19: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  pta_filtered['date_str'] = pta_filtered.index.strftime('%Y-%m-%d')
/tmp/babel-UnOTLq/python-gjz9l1:20: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  sr50_filtered['date_str'] = sr50_filtered.index.strftime('%Y-%m-%d')
#+end_example


This is just a plot

#+BEGIN_SRC python
<<load_libs>>
import matplotlib.dates as mdates
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
pta = pd.read_csv(datapath+'preQC/zac_l_hour_pta.csv', parse_dates = True, index_col=0)
sr50 = pd.read_csv(datapath+'zac_l_hour_SR50_stake_height_ice.csv', parse_dates = True, index_col=0)
z_l = pd.read_csv(datapath+'preQC/zac_l_hour_ice_ablation.csv', parse_dates = True, index_col=0)
pta = -pta

pta = pta.resample('D').mean()
sr50 = sr50.resample('D').mean()

fig, ax = plt.subplots(2,3,figsize = (8,6), sharey = True)
pta.loc['20-June-2008':'1-Sep-2008'].plot(ax = ax[0,0])
sr50.loc['20-June-2008':'1-Sep-2008'].plot(ax = ax[0,0])
ax[0,0].set_ylim(0,2.7)

#ax[0,0].set_xticks([])

pta.loc['20-June-2009':'1-Sep-2009'].plot(ax = ax[0,1])
sr50.loc['20-June-2009':'1-Sep-2009'].plot(ax = ax[0,1])
ax[0,1].set_ylim(0,2.7)

pta.loc['20-June-2010':'1-Sep-2010'].plot(ax = ax[0,2])
sr50.loc['20-June-2010':'1-Sep-2010'].plot(ax = ax[0,2])
ax[0,2].set_ylim(0,2.7)

pta.loc['20-June-2012':'1-Sep-2012'].plot(ax = ax[1,0])
sr50.loc['20-June-2012':'1-Sep-2012'].plot(ax = ax[1,0])
ax[1,0].set_ylim(0,2.7)

pta.loc['20-June-2015':'1-Sep-2015'].plot(ax = ax[1,1])
sr50.loc['20-June-2015':'1-Sep-2015'].plot(ax = ax[1,1])
#ax[0].set_xlim(datetime.datetime(2015,6,1),datetime.datetime(2015,9,1))
ax[1,1].set_ylim(0,2.7)

pta.loc['20-June-2016':'1-Sep-2016'].plot(ax = ax[1,2])
sr50.loc['20-June-2016':'1-Sep-2016'].plot(ax = ax[1,2])
#ax[1].set_xlim(datetime.datetime(2016,6,1),datetime.datetime(2016,9,1))
ax[1,2].set_ylim(0,2.7)

ax[0,0].set_ylabel('m ice')
ax[1,0].set_ylabel('m ice')
fig.tight_layout()

#fig.savefig('QCfigs/PTA_vs_SR50.png', dpi = 300)
#fig.savefig('../glaciobasis/essd/manuscript/figures/fig11.png', dpi = 300)
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python

<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
pta = pd.read_csv(datapath+'preQC/zac_l_hour_pta.csv', parse_dates = True, index_col=0)
sr50 = pd.read_csv(datapath+'preQC/zac_l_hour_SR50_stake_height_ice.csv', parse_dates = True, index_col=0)
z_l = pd.read_csv(datapath+'preQC/zac_l_hour_ice_ablation.csv', parse_dates = True, index_col=0)
pta = -pta
year = 2009
fig, ax = plt.subplots(1,1,figsize = (10,5))
pta.loc[str(year)].plot(ax = ax)
sr50.loc[str(year)].plot(ax = ax)
z_l.loc[str(year)].plot(ax=ax)
#+END_SRC

#+RESULTS:
: Axes(0.125,0.11;0.775x0.77)

#+BEGIN_SRC python

<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
z_l = pd.read_csv(datapath+'preQC/zac_l_hour_ice_ablation.csv', parse_dates = True, index_col=0)

month = z_l.index.month
z_l[month<4] = np.nan
z_l[month>9] = np.nan

z_l_hour = z_l.resample('H').mean()
z_l_day = z_l.resample('D').mean() 
z_l_hour.plot()
z_l_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_ice_ablation.csv', index = True, float_format = '%g')
z_l_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_ice_ablation.csv', index = True, float_format = '%g')
#+END_SRC

#+RESULTS:
: None



****** zac_u



#+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
z_u = pd.read_csv(datapath+'preQC/zac_u_hour_ice_ablation.csv', parse_dates = True, index_col=0)

year = 2022
fig, ax = plt.subplots(1,1,figsize = (10,5))
pta.loc[str(year)].plot(ax = ax)
sr50.loc[str(year)].plot(ax = ax)
z_u.loc[str(year)].plot(ax=ax)
#+END_SRC

#+RESULTS:
: Axes(0.125,0.11;0.775x0.77)

#+BEGIN_SRC python

<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
z_u = pd.read_csv(datapath+'preQC/zac_u_hour_ice_ablation.csv', parse_dates = True, index_col=0)

z_u.loc['2011-Jan-1':'2011-June-19'] = np.nan
z_u.loc['2011-Aug-20':'2011-Dec-31'] = np.nan
z_u.loc['2014'] = np.nan
z_u.loc['2015'] = np.nan
z_u.loc['2016-Jan-1':'2016-June-29'] = np.nan
z_u.loc['2016-Aug-25':'2016-Dec-31'] = np.nan
z_u.loc['2017'] = np.nan
z_u.loc['2019-Jan-1':'2019-June-27'] = np.nan
z_u.loc['2016-Aug-26':'2016-Dec-31'] = np.nan
z_u.loc['2020'] = np.nan
z_u.loc['2021'] = np.nan



month = z_u.index.month
z_u[month<4] = np.nan
z_u[month>9] = np.nan

z_u_hour = z_u.resample('H').mean()
z_u_day = z_u.resample('D').mean() 
z_u_hour.plot()
z_u_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_ice_ablation.csv', index = True, float_format = '%g')
z_u_day.to_csv('data_v1.0/gem_database/2022/zac_u_day_ice_ablation.csv', index = True, float_format = '%g')
#+END_SRC

#+RESULTS:
: None


** QC


*** Overview

Plotting all preQC values against all postQC values

#+BEGIN_SRC python
import matplotlib.pyplot as plt
import numpy as np

# Sample data
time = np.linspace(0, 10, 100)
temperature = np.sin(time) * 20
solar_radiation = np.abs(np.sin(time)) * 800
# ... continue for your other variables ...

# List of data and corresponding labels
data_list = [temperature, solar_radiation]  # Continue for other variables...
labels = ["Temperature", "Solar Radiation"]  # Continue for other variable names...

fig, axes = plt.subplots(len(data_list), 1, sharex=True, figsize=(10, 8))

for ax, data, label in zip(axes, data_list, labels):
    ax.plot(time, data, label=label)
    ax.set_ylabel(label)
    ax.legend()

axes[-1].set_xlabel("Time")

plt.tight_layout()
plt.show()


#+END_SRC


#+BEGIN_SRC python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

workingfolder = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

#zac_l
site = 'zac_l'
temp = pd.read_csv(workingfolder + site + '_hour_temperature.csv', index_col = 0)
rh = pd.read_csv(workingfolder + site + '_hour_relative_humidity.csv', index_col = 0)
p = pd.read_csv(workingfolder + site + '_hour_pressure.csv', index_col = 0)
rad = pd.read_csv(workingfolder + site + '_hour_radiation.csv', index_col = 0)
ws = pd.read_csv(workingfolder + site + '_hour_wind_speed.csv', index_col = 0)
height = pd.read_csv(workingfolder + site + '_hour_boom_height.csv', index_col = 0)
#surf = pd.read_csv(workingfolder + site + '_hour_lowering.csv', index_col = 0)
ablation = pd.read_csv(workingfolder + site + '_hour_ice_ablation.csv', index_col = 0)

merged = temp.join(rh, how='outer').join(p, how='outer').join(rad, how='outer').join(ws, how='outer').join(height, how='outer').join(ablation, how='outer')

merged.rename(columns = {'t_1':'T_air', 'rh': 'RH', 'p': 'P_air','dsr_corr':'SR_in', 'usr_corr':'SR_out', 'dlr':'LR_in', 'ulr':'LR_out', 'wspd':'WS', 'z_boom':'H', 'ice_ablation':'Ablation_ice' }, inplace = True)
#merged.plot(subplots = True, figsize = (12,12))

cols = ['T_air', 'RH', 'P_air', 'SR_in', 'SR_out', 'LR_in','LR_out', 'WS', 'H','Ablation_ice']
<<drop_head_and_tail_nans_fast>>
data = drop_nan_rows(merged)
dates = data.index
#fig, axes = plt.subplots(len(data[cols]), 1, sharex=True, figsize=(10, 8))

#for ax, data, label in zip(axes, data[cols], cols):
#    ax.plot(dates, data, label=label)
#    ax.set_ylabel(label)
#    ax.legend()

fig = data[cols].plot(subplots = True, figsize = (10,8))


# Set x-ticks to the beginning of each year
#years = [datetime.date(year, 1, 1) for year in range(2008, 2023)]
#axes[-1].set_xticks(years)
#axes[-1].set_xlabel("Year")
#fig.autofmt_xdate()  # Better formatting for dates

plt.tight_layout()


#+END_SRC


#+BEGIN_SRC python




labels = ["T_air", "RH", "P_air", "SR_in", "SR_out", "LR_in", "LR_out", "WS", "H", "Ablation_ice"]
offsets = range(0, 8*10, 10)  # These values determine the distance between each plot on the y-axis

# Plotting
for var_data, label, offset in zip(data, labels, offsets):
    plt.plot(time, var_data + offset, label=label)

plt.yticks(offsets, labels)  # Re-label y-axis ticks with variable names
plt.xlabel("Time")
plt.ylabel("Variables")
plt.title("Multiple Climate Variables Over Time")
plt.legend(loc="upper left", bbox_to_anchor=(1, 1))  # Legend outside of the main plot area
plt.tight_layout()
plt.show()

#+END_SRC



*** temperature
**** Utilities
#+NAME: plot_gradients_full_period
#+BEGIN_SRC python
fig,ax = plt.subplots(5,1,figsize = (8,6), sharex=True)
panel1 = ax[0]
panel2 = ax[1]
panel3 = ax[2]
panel4 = ax[3]
panel5 = ax[4]


zac_l_pre[variable].plot(ax = panel1, label = 'zac_l', color = 'tab:blue')
zac_u_pre[variable].plot(ax = panel1, label = 'zac_u', color = 'tab:orange')
zac_a_pre[variable].plot(ax = panel1, label = 'zac_a', color = 'tab:green')
panel1.legend(loc= 3,ncol = 3)
#panel1.set_title('Pre QC')
panel1.set_ylabel(ylabel)
#panel1.text(datetime.datetime(2008,7,1),15,'A')

zac_l[variable].plot(ax = panel2, label = 'zac_l', color = 'tab:blue')
zac_u[variable].plot(ax = panel2, label = 'zac_u', color = 'tab:orange')
zac_a[variable].plot(ax = panel2, label = 'zac_a', color = 'tab:green')

#panel2.legend(ncol = 3)
#panel2.set_title('Post QC')
panel2.set_ylabel(ylabel)
panel2.legend(loc= 3,ncol = 3)

d_l_u = (zac_l-zac_u)/(zac_l_elev-zac_u_elev)*100
d_u_a = (zac_u-zac_a)/(zac_u_elev-zac_a_elev)*100
d_l_a = (zac_l-zac_a)/(zac_l_elev-zac_a_elev)*100

d_l_u_pre = (zac_l_pre-zac_u_pre)/(zac_l_elev-zac_u_elev)*100
d_u_a_pre = (zac_u_pre-zac_a_pre)/(zac_u_elev-zac_a_elev)*100
d_l_a_pre = (zac_l_pre-zac_a_pre)/(zac_l_elev-zac_a_elev)*100

d_l_u_pre[variable].plot(ax = panel3, label = 'Flagged in QC', color = 'tab:red', linewidth = 0.8)
d_u_a_pre[variable].plot(ax=panel4, label = 'Flagged in QC', color = 'tab:red', linewidth = 0.8)
d_l_a_pre[variable].plot(ax=panel5, label = 'Flagged in QC', color = 'tab:red', linewidth = 0.8)

d_l_u[variable].plot(ax=panel3, label = 'zac_l minus zac_u', color = 'gray')
d_u_a[variable].plot(ax=panel4, label = 'zac_u minus zac_a', color = 'gray')
d_l_a[variable].plot(ax=panel5, label = 'zac_l minus zac_a', color = 'gray')
#panel3.set_title('Gradients')
panel3.legend(ncol = 2)
panel3.set_ylabel(ylabel + '/100m')
panel4.legend(ncol = 2)
panel4.set_ylabel(ylabel + '/100m')
panel5.legend(ncol = 2)
panel5.set_ylabel(ylabel + '/100m')
# Label each subplot
for i, ax in enumerate(ax, start=1):
    ax.text(0.01, 0.95, '('+chr(96 + i)+')', transform=ax.transAxes, 
            fontsize=12, fontweight='normal', va='top')

fig.tight_layout()
plt.show()
#+END_SRC

#+RESULTS: plot_gradients_full_period
: None


#+NAME: plot_the_timeperiod
#+BEGIN_SRC python
# The year with the problem
fig,ax = plt.subplots(4,2,figsize = (12,10))
temp_l[variable].plot(ax = ax[0,1], label = 'zac_l')
temp_u[variable].plot(ax = ax[0,1], label = 'zac_u')
temp_a[variable].plot(ax = ax[0,1], label = 'zac_a')
ax[0,1].legend()
ax[0,1].set_title(timeperiod1_title)
ax[0,1].set_xlim(timeperiod1)

d_l_u[variable].plot(ax=ax[1,1], label = 'zac_l minus zac_u')
d_u_a[variable].plot(ax=ax[2,1], label = 'zac_u minus zac_a')
d_l_a[variable].plot(ax=ax[3,1], label = 'zac_l minus zac_a')
ymin,ymax = ax[0,1].get_ylim()
ax[0,1].vlines(date_of_interest1,ymin,ymax, color = 'gray', linestyle = '--')
ax[0,1].vlines(date_of_interest2,ymin,ymax, color = 'gray', linestyle = '--')
ymin,ymax = ax[1,1].get_ylim()
ax[1,1].vlines(date_of_interest1,ymin,ymax, color = 'gray', linestyle = '--')
ax[1,1].vlines(date_of_interest2,ymin,ymax, color = 'gray', linestyle = '--')
ymin,ymax = ax[2,1].get_ylim()
ax[2,1].vlines(date_of_interest1,ymin,ymax, color = 'gray', linestyle = '--')
ax[2,1].vlines(date_of_interest2,ymin,ymax, color = 'gray', linestyle = '--')

ymin,ymax = ax[3,1].get_ylim()
ax[3,1].vlines(date_of_interest1,ymin,ymax, color = 'gray', linestyle = '--')
ax[3,1].vlines(date_of_interest2,ymin,ymax, color = 'gray', linestyle = '--')
ax[1,1].legend()
ax[2,1].legend()
ax[3,1].legend()
ax[1,1].set_xlim(timeperiod1)
ax[2,1].set_xlim(timeperiod1)
ax[3,1].set_xlim(timeperiod1)
ax[0,1].set_ylim(-40,20)
#ax[1,1].set_ylim(-0.05,0.05)
#ax[2,1].set_ylim(-0.05,0.05)
#ax[3,1].set_ylim(-0.05,0.05)


# The previous year for reference
temp_l[variable].plot(ax = ax[0,0], label = 'zac_l')
temp_u[variable].plot(ax = ax[0,0], label = 'zac_u')
temp_a[variable].plot(ax = ax[0,0], label = 'zac_a')
ax[0,0].legend()
ax[0,0].set_title(timeperiod2_title)

ax[0,0].set_xlim(timeperiod2)



d_l_u[variable].plot(ax=ax[1,0], label = 'zac_l minus zac_u')
d_u_a[variable].plot(ax=ax[2,0], label = 'zac_u minus zac_a')
d_l_a[variable].plot(ax=ax[3,0], label = 'zac_l minus zac_a')
ax[1,0].legend()
ax[2,0].legend()
ax[3,0].legend()
ax[1,0].set_xlim(timeperiod2)
ax[2,0].set_xlim(timeperiod2)
ax[3,0].set_xlim(timeperiod2)

ax[0,0].set_ylim(-40,20)
#ax[1,0].set_ylim(-0.05,0.05)
#ax[2,0].set_ylim(-0.05,0.05)
#ax[3,0].set_ylim(-0.05,0.05)




#+END_SRC



**** post QC check
***** data only
#+BEGIN_SRC python
<<load_libs>>
# meta data
#rcParams.update({'font.size': 8})
zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_temperature.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)

variable = 't_1'
ylabel = '$^\circ$C'


zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan


fig,ax = plt.subplots(3,1,figsize = (8,6), sharex=True)
zac_l[variable].plot(ax = ax[0], label = 'ZAC_L', color = 'tab:blue')
zac_u[variable].plot(ax = ax[1], label = 'ZAC_U', color = 'tab:orange')
zac_a[variable].plot(ax = ax[2], label = 'ZAC_A', color = 'tab:green')

zac_l_pre[variable].plot(ax = ax[0], label = 'Discarded', color = 'tab:blue', alpha = 0.3)
zac_u_pre[variable].plot(ax = ax[1], label = 'Discarded', color = 'tab:orange', alpha = 0.3)
zac_a_pre[variable].plot(ax = ax[2], label = 'Discarded', color = 'tab:green', alpha = 0.3)

ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel(ylabel)
ax[1].set_ylabel(ylabel)
ax[2].set_ylabel(ylabel)
#ax[0].set_ylim(0,100)
#ax[1].set_ylim(0,100)
#ax[2].set_ylim(0,100)

ax[0].legend()
ax[1].legend()
ax[2].legend()


#+END_SRC



***** Drift 
#+BEGIN_SRC python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_temperature.csv', parse_dates = True, index_col=0)

variable = 't_1'
ylabel = '%'



fig,ax = plt.subplots(3,1,figsize = (10,10), sharex=True)
zac_l['t_1'].plot(ax = ax[0], label = 'Air temperature at ZAC_L', color = 'tab:blue')
zac_u['t_1'].plot(ax = ax[1], label = 'Air temperature at ZAC_U', color = 'tab:orange')
zac_a['t_1'].plot(ax = ax[2], label = 'Air temperature at ZAC_A', color = 'tab:pink')
ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel('$^\circ$C')
ax[1].set_ylabel('$^\circ$C')
ax[2].set_ylabel('$^\circ$C')
ax[0].legend()
ax[1].legend()
ax[2].legend()
fig.savefig('QCfigs/temperature_drift.png')

#+END_SRC




***** Gradients

#+BEGIN_SRC python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_temperature.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)

variable = 't_1'
ylabel = '$^\circ$C'
<<plot_gradients_full_period>>

disc_data_l = (zac_l_pre[variable].count()-zac_l[variable].count())/zac_l_pre[variable].count()*100
disc_data_u = (zac_u_pre[variable].count()-zac_u[variable].count())/zac_u_pre[variable].count()*100
disc_data_a = (zac_a_pre[variable].count()-zac_a[variable].count())/zac_a_pre[variable].count()*100
print('The total percentage of discarded data at: ')
print('zac_l is '+ str(disc_data_l))
print('zac_u is '+ str(disc_data_u))
print('zac_a is '+ str(disc_data_a))
#fig.savefig('QCfigs/Temperature_final.png')
fig.savefig('../glaciobasis/essd/manuscript/figures/fig04.png', dpi = 300)
#+END_SRC

#+RESULTS:
: None


**** QC
Gradients

#+BEGIN_SRC python
<<import_libraries>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
variable = 't_1'
temp_l = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)

<<plot_gradients_full_period>>

temp_l = pd.read_csv(datapath+'preQC/zac_l_day_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_day_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_day_temperature.csv', parse_dates = True, index_col=0)

<<plot_gradients_full_period>>

#+END_SRC



There are issues with zac_u in winter 2014/2015 and likely with zac_l in winter 2020/2021








**** The issue at zac_u in 2020/2021

I will remove both data from zac_u between 2010-10-01 and 2021-07-01

#+BEGIN_SRC python
<<import_libraries>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

temp_l = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)


timeperiod1_title = '2020-01-01 to 2021-12-31'
timeperiod1 = (datetime.datetime(2020,1,1), datetime.datetime(2021,12,31))
timeperiod2_title = '2019-01-01 to 2020-12-31'
timeperiod2 = (datetime.datetime(2019,1,1), datetime.datetime(2020,12,31))
date_of_interest1=datetime.datetime(2020,9,15)
date_of_interest2=datetime.datetime(2021,8,1)

<<plot_the_timeperiod>>

temp_l = pd.read_csv(datapath+'preQC/zac_l_day_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_day_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_day_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)


<<plot_the_timeperiod>>


#+END_SRC



**** The issue at zac_a in 2015

The data from zac_a between 2015-01-05 to 2015-05-01 looks strange and will be removed

#+BEGIN_SRC python
<<import_libraries>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

# meta data
variable = 't_1'
zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

temp_l = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)

timeperiod1_title = '2014-11-1 to 2015-10-15'
timeperiod1 = (datetime.datetime(2014,11,1),datetime.datetime(2015,10,15))
timeperiod2_title = '2013-11-1 to 2014-10-15'
timeperiod2 = (datetime.datetime(2013,11,1),datetime.datetime(2014,10,15))
date_of_interest1=datetime.datetime(2015,1,5)
date_of_interest2=datetime.datetime(2015,5,1)

<<plot_the_timeperiod>>

temp_l = pd.read_csv(datapath+'preQC/zac_l_day_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_day_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_day_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)


<<plot_the_timeperiod>>

#+END_SRC


#+NAME: plot_the_2015_issue_at_zac_l
#+BEGIN_SRC python
# The year with the problem
fig,ax = plt.subplots(4,2,figsize = (12,10))
temp_l['Air temperature, C'].plot(ax = ax[0,1], label = 'zac_l')
temp_u['Air temperature, C'].plot(ax = ax[0,1], label = 'zac_u')
temp_a['Air temperature, C'].plot(ax = ax[0,1], label = 'zac_a')
ax[0,1].legend()
ax[0,1].set_title('2015-1-1 to 2015-5-15')
ax[0,1].set_xlim(datetime.datetime(2015,1,1),datetime.datetime(2015,5,15))

d_l_u['Air temperature, C'].plot(ax=ax[1,1], label = 'zac_l minus zac_u')
d_u_a['Air temperature, C'].plot(ax=ax[2,1], label = 'zac_u minus zac_a')
d_l_a['Air temperature, C'].plot(ax=ax[3,1], label = 'zac_l minus zac_a')
ymin,ymax = ax[3,1].get_ylim()
ax[3,1].vlines(datetime.datetime(2015,1,12),ymin,ymax)
ax[1,1].legend()
ax[2,1].legend()
ax[3,1].legend()
ax[1,1].set_xlim(datetime.datetime(2015,1,1),datetime.datetime(2015,5,15))
ax[2,1].set_xlim(datetime.datetime(2015,1,1),datetime.datetime(2015,5,15))
ax[3,1].set_xlim(datetime.datetime(2015,1,1),datetime.datetime(2015,5,15))
ax[0,1].set_ylim(-40,20)
#ax[1,1].set_ylim(-0.05,0.05)
#ax[2,1].set_ylim(-0.05,0.05)
#ax[3,1].set_ylim(-0.05,0.05)


# The previous year for reference
temp_l['Air temperature, C'].plot(ax = ax[0,0], label = 'zac_l')
temp_u['Air temperature, C'].plot(ax = ax[0,0], label = 'zac_u')
temp_a['Air temperature, C'].plot(ax = ax[0,0], label = 'zac_a')
ax[0,0].legend()
ax[0,0].set_title('2013-1-1 to 2013-5-12')
timeperiod = (datetime.datetime(2013,1,1),datetime.datetime(2013,5,12))
ax[0,0].set_xlim(timeperiod)



d_l_u['Air temperature, C'].plot(ax=ax[1,0], label = 'zac_l minus zac_u')
d_u_a['Air temperature, C'].plot(ax=ax[2,0], label = 'zac_u minus zac_a')
d_l_a['Air temperature, C'].plot(ax=ax[3,0], label = 'zac_l minus zac_a')
ax[1,0].legend()
ax[2,0].legend()
ax[3,0].legend()
ax[1,0].set_xlim(timeperiod)
ax[2,0].set_xlim(timeperiod)
ax[3,0].set_xlim(timeperiod)

ax[0,0].set_ylim(-40,20)
#ax[1,0].set_ylim(-0.05,0.05)
#ax[2,0].set_ylim(-0.05,0.05)
#ax[3,0].set_ylim(-0.05,0.05)




#+END_SRC
**** The issue at zac_a beginning of record
We remove data from before 2009-08-08 21:00
#+BEGIN_SRC python
<<import_libraries>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

# meta data
variable = 't_1'
zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

temp_l = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)

timeperiod1_title = '2009-8-1 to 2009-9-1'
timeperiod1 = (datetime.datetime(2009,8,5),datetime.datetime(2009,8,6))
timeperiod2_title = '2010-8-1 to 2010-9-1'
timeperiod2 = (datetime.datetime(2010,8,5),datetime.datetime(2010,8,6))
date_of_interest1=datetime.datetime(2015,1,5)
date_of_interest2=datetime.datetime(2015,5,1)

<<plot_the_timeperiod>>

#+END_SRC


**** The issue at zac_u in 2015

The conclusion from the below investigation is that we will discard the zac_u data from 2015, the fan must have stopped running
The excat period that can be discarded at zac_u: 2014-10-30 to 2015-12-31
Hourly
#+BEGIN_SRC python
<<import_libraries>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
# meta data
variable = 't_1'
zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

temp_l = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)

<<plot_the_2015_issue_at_zac_u>>

temp_l = pd.read_csv(datapath+'preQC/zac_l_day_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_day_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_day_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)

<<plot_the_2015_issue_at_zac_u>>


#+END_SRC


#+NAME: plot_the_2015_issue_at_zac_u
#+BEGIN_SRC python
# The year with the problem
fig,ax = plt.subplots(4,2,figsize = (12,10))
temp_l[variable].plot(ax = ax[0,1], label = 'zac_l')
temp_u[variable].plot(ax = ax[0,1], label = 'zac_u')
temp_a[variable].plot(ax = ax[0,1], label = 'zac_a')
ax[0,1].legend()
ax[0,1].set_title('2014-10-30 to 2015-12-31')
ax[0,1].set_xlim(datetime.datetime(2014,10,30),datetime.datetime(2015,12,31))



d_l_u[variable].plot(ax=ax[1,1], label = 'zac_l minus zac_u')
d_u_a[variable].plot(ax=ax[2,1], label = 'zac_u minus zac_a')
d_l_a[variable].plot(ax=ax[3,1], label = 'zac_l minus zac_a')
ax[1,1].legend()
ax[2,1].legend()
ax[3,1].legend()
ax[1,1].set_xlim(datetime.datetime(2014,10,30),datetime.datetime(2015,12,31))
ax[2,1].set_xlim(datetime.datetime(2014,10,30),datetime.datetime(2015,12,31))
ax[3,1].set_xlim(datetime.datetime(2014,10,30),datetime.datetime(2015,12,31))
ax[0,1].set_ylim(-40,20)
#ax[1,1].set_ylim(-0.05,0.05)
#ax[2,1].set_ylim(-0.05,0.05)
#ax[3,1].set_ylim(-0.05,0.05)


# The previous year for reference
temp_l[variable].plot(ax = ax[0,0], label = 'zac_l')
temp_u[variable].plot(ax = ax[0,0], label = 'zac_u')
temp_a[variable].plot(ax = ax[0,0], label = 'zac_a')
ax[0,0].legend()
ax[0,0].set_title('2013-10-30 to 2014-12-31')
ax[0,0].set_xlim(datetime.datetime(2013,10,30),datetime.datetime(2014,12,31))



d_l_u[variable].plot(ax=ax[1,0], label = 'zac_l minus zac_u')
d_u_a[variable].plot(ax=ax[2,0], label = 'zac_u minus zac_a')
d_l_a[variable].plot(ax=ax[3,0], label = 'zac_l minus zac_a')
ax[1,0].legend()
ax[2,0].legend()
ax[3,0].legend()
ax[1,0].set_xlim(datetime.datetime(2013,10,30),datetime.datetime(2014,12,31))
ax[2,0].set_xlim(datetime.datetime(2013,10,30),datetime.datetime(2014,12,31))
ax[3,0].set_xlim(datetime.datetime(2013,10,30),datetime.datetime(2014,12,31))

ax[0,0].set_ylim(-40,20)
#ax[1,0].set_ylim(-0.05,0.05)
#ax[2,0].set_ylim(-0.05,0.05)
#ax[3,0].set_ylim(-0.05,0.05)




#+END_SRC






#+BEGIN_SRC python
<<import_libraries>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
temp_l = pd.read_csv(datapath+'preQC/zac_l_day_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_day_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_day_temperature.csv', parse_dates = True, index_col=0)


fig,ax = plt.subplots(4,1,figsize = (10,10), sharex=True)
temp_l['Air temperature, C'].plot(ax = ax[0], label = 'zac_l')
temp_u['Air temperature, C'].plot(ax = ax[0], label = 'zac_u')
temp_a['Air temperature, C'].plot(ax = ax[0], label = 'zac_a')
ax[0].legend()

d_l_u = temp_l-temp_u
d_u_a = temp_u-temp_a
d_l_a = temp_l-temp_a

d_l_u['Air temperature, C'].plot(ax=ax[1], label = 'zac_l minus zac_u')
d_u_a['Air temperature, C'].plot(ax=ax[2], label = 'zac_u minus zac_a')
d_l_a['Air temperature, C'].plot(ax=ax[3], label = 'zac_l minus zac_a')
ax[1].legend()
ax[2].legend()
ax[3].legend()


#+END_SRC





*** Tilt

#+BEGIN_SRC python :session
<<load_libs>>
<<data_file_paths>>

fig, ax = plt.subplots(3,1,figsize = (7.5,5))
with xr.open_dataset(zac_l_path) as ds:
    df = ds[['tilt_x','tilt_y']].to_dataframe()
    df = df.where(df<22, np.nan)
    df = df.where(df>-20, np.nan)
    df = df.where(df!=0, np.nan)


count10min = df.resample('H').count()
tilt_hour = df.resample('H').mean()
#tilt_hour[count10min<6] = np.nan

tilt_hour['tilt_x'].plot(ax = ax[0], color = 'tab:blue', label = '$ZAC\_L$: $Tilt_x$')
tilt_hour['tilt_y'].plot(ax = ax[0], color = 'deepskyblue', label = '$ZAC\_L$: $Tilt_y$')
tilt_hour_ref = tilt_hour.copy()
ax[0].set_xlim(tilt_hour_ref.index[0],tilt_hour_ref.index[-1] )



with xr.open_dataset(zac_u_path) as ds:
    df = ds[['tilt_x','tilt_y']].to_dataframe()
    df = df.where(df<22, np.nan)
    df = df.where(df>-20, np.nan)
    df = df.where(df!=0, np.nan)

count10min = df.resample('H').count()
tilt_hour = df.resample('H').mean()
tilt_hour[count10min<6] = np.nan

tilt_hour['tilt_y'].plot(ax= ax[1], color = 'tab:orange', label = '$ZAC\_U$: $Tilt_x$')
tilt_hour['tilt_x'].plot(ax= ax[1], color = 'gold', label = '$ZAC\_U$: $Tilt_y$')
#df['tilt_y'].plot(ax= ax[1], color = 'tab:orange')
#df['tilt_x'].plot(ax= ax[1], color = 'tab:blue')

ax[1].set_xlim(tilt_hour_ref.index[0],tilt_hour_ref.index[-1] )

with xr.open_dataset(zac_a_path) as ds:
    df = ds[['tilt_x','tilt_y']].to_dataframe()
    df = df.where(df<22, np.nan)
    df = df.where(df>-20, np.nan)
    df = df.where(df!=0, np.nan)
count10min = df.resample('H').count()
tilt_hour = df.resample('H').mean()
tilt_hour[count10min<6] = np.nan
tilt_hour['tilt_y'].plot(ax= ax[2], color = 'tab:green', label = '$ZAC\_A$: $Tilt_x$')
tilt_hour['tilt_x'].plot(ax= ax[2], color = 'limegreen', label = '$ZAC\_A$: $Tilt_y$')

ax[2].set_xlim(tilt_hour_ref.index[0],tilt_hour_ref.index[-1] )

ax[0].set_ylabel('Degree')
ax[1].set_ylabel('Degree')
ax[2].set_ylabel('Degree')
ax[0].set_ylim(-22,20)
ax[1].set_ylim(-22,20)
ax[2].set_ylim(-22,20)
ax[0].legend(loc=9, ncols =2)
ax[1].legend(loc=3, ncols =2)
ax[2].legend(loc=1, ncols =2)


for i, ax in enumerate(ax, start=1):
    ax.text(0.01, 0.95, '('+chr(96 + i)+')', transform=ax.transAxes, 
            fontsize=12, fontweight='normal', va='top')

fig.tight_layout()

#fig.savefig('QCfigs/tilt.png')
fig.savefig('../glaciobasis/essd/manuscript/figures/fig06.png', dpi = 300)
#+END_SRC

#+RESULTS:
: None



*** Radiation
**** Utilities
#+NAME: read_in_hourly_preQC_radiation
#+BEGIN_SRC python
rad_l = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

hourmax = 1000
maximum = hourmax

rad_l[variable][rad_l[variable]>maximum] = np.nan
rad_l[variable][rad_l[variable]<0] = np.nan

rad_u[variable][rad_u[variable]>maximum] = np.nan
rad_u[variable][rad_u[variable]<0] = np.nan

rad_a[variable][rad_a[variable]>maximum] = np.nan
rad_a[variable][rad_a[variable]<0] = np.nan
#+END_SRC

#+NAME: read_in_hourly_radiation_postQC
#+BEGIN_SRC python
rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

#+END_SRC

#+NAME: read_in_daily_preQC_radiation
#+BEGIN_SRC python
rad_l = pd.read_csv(datapath+'preQC/zac_l_day_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'preQC/zac_u_day_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'preQC/zac_a_day_radiation.csv', parse_dates = True, index_col=0)
daymax = 500
maximum = daymax


rad_l[variable][rad_l[variable]>maximum] = np.nan
rad_l[variable][rad_l[variable]<0] = np.nan

rad_u[variable][rad_u[variable]>maximum] = np.nan
rad_u[variable][rad_u[variable]<0] = np.nan

rad_a[variable][rad_a[variable]>maximum] = np.nan
rad_a[variable][rad_a[variable]<0] = np.nan
#+END_SRC


#+NAME: plot_rad_gradients_full_period
#+BEGIN_SRC python
fig,ax = plt.subplots(4,1,figsize = (10,10), sharex=True)
rad_l[variable].plot(ax = ax[0], label = 'zac_l')
rad_u[variable].plot(ax = ax[0], label = 'zac_u')
rad_a[variable].plot(ax = ax[0], label = 'zac_a')
ax[0].legend()

d_l_u = (rad_l-rad_u)
d_u_a = (rad_u-rad_a)
d_l_a = (rad_l-rad_a)

d_l_u[variable].plot(ax=ax[1], label = 'zac_l minus zac_u')
d_u_a[variable].plot(ax=ax[2], label = 'zac_u minus zac_a')
d_l_a[variable].plot(ax=ax[3], label = 'zac_l minus zac_a')
ax[1].legend()
ax[2].legend()
ax[3].legend()

#+END_SRC


#+NAME: plot_rad_gradients_selected_period
#+BEGIN_SRC python
<<plot_rad_gradients_full_period>>
ax[3].set_xlim(startdate,enddate)
#+END_SRC

**** Post QC check



#+BEGIN_SRC python
  <<load_libs>>
  datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
  variable = 'dsr_corr'
  rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
  rad_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
  rad_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)



<<plot_rad_gradients_full_period>>
#+END_SRC



***** SRin
****** Final

#+BEGIN_SRC python
<<load_libs>>

# meta data
#rcParams.update({'font.size': 8})
zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

variable = 'dsr_corr'
ylabel = 'W/m2'


zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan


fig,ax = plt.subplots(3,1,figsize = (7.5,5))
zac_l[variable].plot(ax = ax[0], label = 'ZAC_L', color = 'tab:blue')
zac_u[variable].plot(ax = ax[1], label = 'ZAC_U', color = 'tab:orange')
zac_a[variable].plot(ax = ax[2], label = 'ZAC_A', color = 'tab:green')

zac_l_pre[variable].plot(ax = ax[0], label = 'Discarded', color = 'tab:blue', alpha = 0.3)
zac_u_pre[variable].plot(ax = ax[1], label = 'Discarded', color = 'tab:orange', alpha = 0.3)
zac_a_pre[variable].plot(ax = ax[2], label = 'Discarded', color = 'tab:green', alpha = 0.3)

ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel(ylabel)
ax[1].set_ylabel(ylabel)
ax[2].set_ylabel(ylabel)
#ax[0].set_ylim(0,100)
#ax[1].set_ylim(0,100)
#ax[2].set_ylim(0,100)
ax[0].set_xlim(datetime.datetime(2008,4,1),datetime.datetime(2022,5,1))
ax[1].set_xlim(datetime.datetime(2008,4,1),datetime.datetime(2022,5,1))
ax[2].set_xlim(datetime.datetime(2008,4,1),datetime.datetime(2022,5,1))

ax[0].legend(loc=3)
ax[1].legend(loc=3)
ax[2].legend(loc=1)
#fig.savefig('QCfigs/SRin_compare.png')
for i, ax in enumerate(ax, start=1):
    ax.text(0.01, 0.95, '('+chr(96 + i)+')', transform=ax.transAxes, 
            fontsize=12, fontweight='normal', va='top')
fig.savefig('../glaciobasis/essd/manuscript/figures/fig07.png', dpi = 300)
#+END_SRC

#+RESULTS:
: None



****** Gradients

#+BEGIN_SRC python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)

zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

variable = 'dsr_corr'
ylabel = 'W m$^{-2}$'
zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan
<<plot_gradients_full_period>>


disc_data_l = (zac_l_pre[variable].count()-zac_l[variable].count())/zac_l_pre[variable].count()*100
disc_data_u = (zac_u_pre[variable].count()-zac_u[variable].count())/zac_u_pre[variable].count()*100
disc_data_a = (zac_a_pre[variable].count()-zac_a[variable].count())/zac_a_pre[variable].count()*100
print('The total percentage of discarded data at: ')
print('zac_l is '+ str(disc_data_l))
print('zac_u is '+ str(disc_data_u))
print('zac_a is '+ str(disc_data_a))
fig.savefig('QCfigs/SRin_final.png')

#+END_SRC


****** Checking for shifts in maximum 
#+BEGIN_SRC python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

variable = 'dsr_corr'
ylabel = 'W m$^{-2}$'

fig, ax = plt.subplots(2,1)
zac_l[variable].resample('D').median().resample('Y').mean().plot(ax = ax[0])
zac_l['cloud_cov'].resample('Y').median().plot()
#fig.savefig('QCfigs/SRin_final.png')

#+END_SRC


****** Calculating potential incoming solar radiation for zac_l


******* Create a G folder with the location of the GIMP dem (epsg 3413):
#+BEGIN_SRC sh :results none 
grass -e -c EPSG:3413 G
#+END_SRC

******* Load in the gimp tiles and patch together + calculate aspect and slope
    #+BEGIN_SRC sh :results none 
    grass --text ./G/PERMANENT 
    # Wollaston region
    # g.region s=74.0300000 n=74.8700000 e=-18.7100000 w=-22.2500000 res=0.01 -aps
    #g.mapset PERMANENT
    #v.in.ogr input=shp/Zackenberg_river_catchment.gpkg output=zackenberg
    #g.region vector=zackenberg res=0.0001 -aps


    r.in.gdal input=~/data/GIMP/tile_5_3_fit_30m_dem.tif output=dem53 -o --o
    r.in.gdal input=~/data/GIMP/tile_5_4_fit_30m_dem.tif output=dem54 -o --o
    g.region raster=dem53,dem54
    r.patch input=dem53,dem54 output=demNE --o
    g.remove -f type=raster pattern=dem* exclude=demNE
    #g.region raster=demNE -p
    #v.in.ogr input=shp/zackenberg_river_catchment.shp output=zackenberg --o
    #r.in.gdal input=/home/shl@geus.dk/data/gem/geobasis/Zackenberg_LastDoyOfObservedSnow_2021_epsg3413.tif output=SED_2021 --o

    v.in.ogr input=GRASS_files/aoi.shp output=aoi --o
    #v.in.ogr input=GRASS_files/mast_aoi.shp output=aoi --o
    #g.region raster=SED_2021
    g.region vector=aoi -aps

    #g.region raster=demNE -p

    r.slope.aspect elevation=demNE aspect=aspect.dem slope=slope.dem --overwrite

    #+END_SRC



******* Calculating the sun parameters
    #+BEGIN_SRC sh :results none
    g.mapset -c SUN
    g.region -dp

    r.horizon elevation=demNE step=1 output=horangle

    #g.remove -f type=raster pattern=horangle*
    #g.remove -f type=raster pattern=glob*
    #g.remove -f type=raster pattern=it*
    #g.remove -f type=raster pattern=b05*
    #g.remove -f type=raster pattern=glob05*


    #+END_SRC



    #+BEGIN_SRC sh
    grass --text ./G/SUN
    g.region -dp
    for i in {1..365..1}
    do
    r.sun elevation=demNE linke_value=1.0 distance_step=1.5 horizon_basename=horangle horizon_step=1  aspect=aspect.dem slope=slope.dem albedo_value=0.2 day=$i glob_rad=glob$i insol_time=it$i nproc=4 --overwrite
    done
    #+END_SRC

    #+RESULTS:

******* Creating the strds and write out as netcdf

******** Writing lists for the strds to order the days from 1 to 365
    #+BEGIN_SRC sh
    #grass --text ./G/SUN 
    variable=glob
    g.list raster pattern="${variable}?" sep=newline > ${variable}_list.txt

    for i in {1..9..1}
    do
    g.list raster pattern="${variable}${i}?" sep=newline >> ${variable}_list.txt
    done

    for i in {10..36..1}
    do
    g.list raster pattern="${variable}${i}?" sep=newline >> ${variable}_list.txt
    done
    #+END_SRC

    #+RESULTS:


******** Creating the temporal datasets and write out netcdfs
    #+BEGIN_SRC sh :results none
    grass --text ./G/SUN 

    #g.region -dp
    # Setting the three dimensional region
    #g.region s=74.0300000 n=74.8700000 e=-18.7100000 w=-22.2500000 t=366.0 b=1.0 res=0.01 tbres=1 res3=0.01 -p3

    #g.region raster=demNE -ap
    #g.region vector=aoi -ap

    r.out.gdal in=demNE output=GRASS_files/mast_aoi.tif --overwrite

    t.create type=strds temporaltype=relative output=glob_radiation_daily title="Daily incoming radiation" description="Dataset with calculated daily incoming solar radiation" --overwrite
 
    t.register -i type=raster input=glob_radiation_daily file=glob_list.txt start=1 increment=1 unit='days' --overwrite 

    t.info input=glob_radiation_daily

    g.region raster=demNE vector=aoi tbres=1 b=100 t=200 -ap3


    t.rast.to.rast3 input=glob_radiation_daily output=glob_radiation_daily_r3 --overwrite


    r3.info glob_radiation_daily_r3
    #g.region raster_3d=glob_radiation_daily_r3 -p

    r3.out.netcdf input=glob_radiation_daily_r3 output=GRASS_files/glob_radiation_daily.nc --overwrite


    #t.create type=strds temporaltype=relative output=insolation_time_daily title="Daily direct beam radiation" description="Dataset with calculated insolation time" --overwrite 

    #t.register -i type=raster input=insolation_time_daily file=it_list.txt start=1 increment=1 unit='days' --overwrite 
    #t.rast.to.rast3 input=insolation_time_daily output=insolation_time_daily_r3
    #g.region raster_3d=insolation_time_daily_r3
    #r3.out.netcdf input=insolation_time_daily_r3 output=netcdfs/data/wollaston/insolation_time_daily.nc --overwrite



    #+END_SRC

    Sampling the strds
    #+BEGIN_SRC sh 

    v.in.ogr input=GRASS_files/zac_l_pos.shp output=zac_l_pos
    v.in.ogr input=GRASS_files/mast_pos.shp output=mast_pos

    t.rast.what points=mast_pos strds=glob_radiation_daily output=GRASS_files/glob_mast.csv null_value=NA separator=comma --overwrite
    t.rast.what points=zac_l_pos strds=glob_radiation_daily output=GRASS_files/glob_zac_l.csv null_value=NA separator=comma --overwrite

    #+END_SRC








****** Comparing potential global radiation to observations at zac_l

#+BEGIN_SRC python
<<load_libs>>
from matplotlib import rcParams
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)


zac_l['I'] = zac_l['I'].where(zac_l['I']>=0,0)

rcParams.update({'font.size': 8})

for year in range(2008,2022+1):
    fig, ax = plt.subplots(1,1, figsize = (3,2))
    zac_l[['I','dsr_corr']].loc[str(year)].plot(ax = ax)
    ax.set_ylim(0,900)
    fig.tight_layout()
    fig.savefig('QCfigs/zac_l_SRin_vs_I_'+str(year)+'.png')

zac_u['I'] = zac_u['I'].where(zac_u['I']>=0,0)

for year in range(2012,2022+1):
    fig, ax = plt.subplots(1,1, figsize = (3,2))
    zac_u[['I','dsr_corr']].loc[str(year)].plot(ax = ax)
    ax.set_ylim(0,900)
    fig.tight_layout()
    fig.savefig('QCfigs/zac_u_SRin_vs_I_'+str(year)+'.png')

zac_a['I'] = zac_a['I'].where(zac_a['I']>=0,0)

for year in range(2009,2019+1):
    fig, ax = plt.subplots(1,1, figsize = (3,2))
    zac_a[['I','dsr_corr']].loc[str(year)].plot(ax = ax)
    ax.set_ylim(0,900)
    fig.tight_layout()
    fig.savefig('QCfigs/zac_a_SRin_vs_I_'+str(year)+'.png')


#+END_SRC



#+BEGIN_SRC python
#mast_swin = pd.read_csv('/home/shl@geus.dk/data/gem/climatebasis/climate_mast/View_ClimateBasis_Zackenberg_Data_Radiation_Short_wave_incoming_radiation_@_200_cm__5min_average_W_m2270920221347180965.csv', sep = '\t', index_col = 0, parse_dates = {'date':['Date','Time']}, na_values = -9999)


#glob_rad = pd.read_csv('GRASS_files/glob_zac_l.csv', header = None)
#glob_rad.columns = ['x', 'y', 'doy','doy+1','glob_rad']
#glob_rad.set_index('doy', inplace = True)
#glob_rad = pd.DataFrame(glob_rad['glob_rad'])


variable = 'dsr_corr'
ylabel = 'W m$^{-2}$'

dsr_corr_sum = pd.DataFrame(zac_l['dsr_corr'].interpolate().resample('D').sum())
#usr_corr_sum = pd.DataFrame(zac_l['usr_corr'].interpolate().resample('D').sum())

# calculate the difference in days between each date
df = pd.DataFrame(zac_l['dsr_corr']).dropna()
df['diff'] = df.index.to_series().diff()

# only interpolate where the difference to the previous date is <= 1 day
df.loc[df['diff'] <= pd.Timedelta(days=1), 'dsr_corr'] = df.loc[df['diff'] <= pd.Timedelta(days=1), 'dsr_corr'].interpolate()
usr_corr_sum = df['dsr_corr'].resample('D').sum()

dsr_sum = pd.DataFrame(zac_l['dsr'].resample('D').sum())
usr_sum = pd.DataFrame(zac_l['usr'].resample('D').sum())

alb_raw = zac_l['usr'].resample('D').sum()/zac_l['dsr'].resample('D').sum()
mast_dsr_sum = pd.DataFrame(mast_swin['SRI (W/m2)'].resample('H').mean().resample('D').sum())

day_max = pd.DataFrame(zac_l[variable].resample('D').max())

years = []
#day_max.groupby([day_max.index.dayofyear]).max().plot(ax = ax)

from scipy.signal import savgol_filter
from scipy.ndimage.filters import maximum_filter1d

y_max = maximum_filter1d(dsr_corr_sum['dsr_corr'], size=7)
top = savgol_filter(y_max, 101, 3)  # window size 51, polynomial order 3
dsr_corr_sum['max'] = top
#+END_SRC

#+BEGIN_SRC python
SRin = pd.DataFrame(dsr_corr_sum['max'])
mask = (SRin.index.year != 2015) & (SRin.index.year != 2020) & (SRin.index.year != 2021) & (SRin.index.year != 2022)
SRin = SRin[mask]


SRin_stats = pd.DataFrame(SRin.groupby(SRin.index.dayofyear).max())
SRin_stats['min'] = SRin.groupby(SRin.index.dayofyear).min()
SRin_stats['mean'] = SRin.groupby(SRin.index.dayofyear).mean()
SRin_stats['std'] = SRin.groupby(SRin.index.dayofyear).std()

fig, ax = plt.subplots(1,1, figsize = (12,5))

years = [2008,2009,2010,2011,2012,2013,2014,2016,2017,2018,2019]
legend = []



ax.fill_between(SRin_stats.index, SRin_stats['mean']-2*SRin_stats['std'], SRin_stats['mean']+2*SRin_stats['std'], alpha = 0.5, color = 'tab:blue')
legend.append('2 x std')

glob_rad.plot(ax=ax, linewidth = 3, color = 'black', linestyle = '--')
legend.append('glob')

for year in years:
    
    
    ax.plot(dsr_corr_sum.loc[str(year)].index.dayofyear,dsr_corr_sum.loc[str(year)]['max'], label = 'dsr_corr max- '+str(year))
    legend.append(str(year))
SRin_stats['mean'].plot(ax = ax, linewidth = 3, color = 'black')
legend.append('mean')
ax.legend(legend)
ax.set_ylabel('Wh/day')
fig.savefig('QCfigs/SRin_stats.png', dpi = 300)
  #+END_SRC



  #+BEGIN_SRC python
#years = []
fig, ax = plt.subplots(1,1, figsize = (12,3))

years = [2008,2009,2010,2011,2012,2013,2014,2016,2017,2018,2019]
legend = []
legend.append('glob')
glob_rad.plot(ax=ax, linewidth = 3, label = 'Potential incoming radiation')
for year in years:
    
    ax.plot(dsr_corr_sum.loc[str(year)].index.dayofyear,dsr_corr_sum.loc[str(year)]['max'], label = 'dsr_corr max- '+str(year))
    legend.append(str(year))

ax.legend(legend)

#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/5662a75ac6701360dc0a926160379af0d3006b1d.png]]

#+BEGIN_SRC python

for year in np.arange(2008,2021):
    #ax.plot(day_max.loc[str(year)].index.dayofyear,day_max.loc[str(year)])
    fig, ax = plt.subplots(1,1, figsize = (12,3))
    #ax1 = ax.twinx()
    glob_rad.plot(ax=ax, linewidth = 3, label = 'Potential incoming radiation')
    ax.plot(dsr_corr_sum.loc[str(year)].index.dayofyear,dsr_corr_sum.loc[str(year)]['dsr_corr'], label = 'dsr_corr - '+str(year))
    ax.plot(dsr_corr_sum.loc[str(year)].index.dayofyear,dsr_corr_sum.loc[str(year)]['max'], label = 'dsr_corr max- '+str(year))
    ax.plot(dsr_sum.loc[str(year)].index.dayofyear,dsr_sum.loc[str(year)], label = 'dsr - '+str(year))
    #ax1.plot(alb_raw.loc[str(year)].index.dayofyear,alb_raw.loc[str(year)], label = 'albedo')
    #ax.plot(mast_dsr_sum.loc[str(year)].index.dayofyear,mast_dsr_sum.loc[str(year)], label = 'mast dsr - '+str(year))
    ax.plot(usr_sum.loc[str(year)].index.dayofyear,usr_sum.loc[str(year)], label = 'usr - '+str(year))
    ax.legend()
#    ax1.set_ylim(-0.5,1.5)
    fig.savefig('QCfigs/SRin/zac_l_SRin_SRout_glob_'+str(year)+'.png', dpi = 300)
    #years.append(str(year))
#ax.legend(years)


#+END_SRC



#+BEGIN_SRC python
import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import savgol_filter

# This is your time-series data, replace it with your actual data
y = np.random.normal(size=500)
y = np.cumsum(y)
x = np.linspace(0, 4*np.pi, 500)
y = np.sin(x) + y / 10

# This applies the Savitzky-Golay filter
# The first argument is your data
# The second argument, window_length, is the number of points to use for the polynomial fit (must be odd)
# The third argument is the polynomial order, in this case 3.
yhat = savgol_filter(y, 51, 3)  # window size 51, polynomial order 3

plt.plot(y)
plt.plot(yhat, color='red')
plt.show()
#+END_SRC






****** Comparing potential global radiation to observations at climate mast to compare


#+BEGIN_SRC python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

mast = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
mast_swin = pd.read_csv('/home/shl@geus.dk/data/gem/climatebasis/climate_mast/View_ClimateBasis_Zackenberg_Data_Radiation_Short_wave_incoming_radiation_@_200_cm__5min_average_W_m2270920221347180965.csv', sep = '\t', index_col = 0, parse_dates = {'date':['Date','Time']}, na_values = -9999)


glob_rad = pd.read_csv('GRASS_files/glob_mast.csv', header = None)
glob_rad.columns = ['x', 'y', 'doy','doy+1','glob_rad']
glob_rad.set_index('doy', inplace = True)
glob_rad = pd.DataFrame(glob_rad['glob_rad'])


variable = 'dsr_corr'
ylabel = 'W m$^{-2}$'



dsr_sum = pd.DataFrame(mast_swin['SRI (W/m2)'].resample('H').mean().resample('D').sum())


day_max = pd.DataFrame(zac_l[variable].resample('D').max())

years = []
#day_max.groupby([day_max.index.dayofyear]).max().plot(ax = ax)

for year in np.arange(2012,2021):
    #ax.plot(day_max.loc[str(year)].index.dayofyear,day_max.loc[str(year)])
    fig, ax = plt.subplots(1,1, figsize = (12,3))
    glob_rad.plot(ax=ax, linewidth = 3, label = 'Potential incoming radiation')
    ax.plot(dsr_sum.loc[str(year)].index.dayofyear,dsr_sum.loc[str(year)], label = 'dsr - '+str(year))
    #ax.plot(usr_sum.loc[str(year)].index.dayofyear,usr_sum.loc[str(year)], label = 'usr - '+str(year))
    ax.legend()
    fig.savefig('QCfigs/SRin/mast_SRin_glob_'+str(year)+'.png', dpi = 300)
    #years.append(str(year))
#ax.legend(years)


#+END_SRC






***** SRout

#+BEGIN_SRC python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)

zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

variable = 'usr_corr'
ylabel = 'W m$^{-2}$'
zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan
<<plot_gradients_full_period>>


disc_data_l = (zac_l_pre[variable].count()-zac_l[variable].count())/zac_l_pre[variable].count()*100
disc_data_u = (zac_u_pre[variable].count()-zac_u[variable].count())/zac_u_pre[variable].count()*100
disc_data_a = (zac_a_pre[variable].count()-zac_a[variable].count())/zac_a_pre[variable].count()*100
print('The total percentage of discarded data at: ')
print('zac_l is '+ str(disc_data_l))
print('zac_u is '+ str(disc_data_u))
print('zac_a is '+ str(disc_data_a))
fig.savefig('QCfigs/SRout_final.png')

#+END_SRC

***** LWin

****** Final

#+BEGIN_SRC python
<<load_libs>>
# meta data
#rcParams.update({'font.size': 8})
zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

variable1 = 'dlr'
variable2 = 'ulr'
ylabel = 'W m$^{-2}$'


#zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
#zac_l[variable][zac_l[variable]==0] = np.nan
#zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
#zac_u[variable][zac_u[variable]==0] = np.nan
#zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
#zac_a[variable][zac_a[variable]==0] = np.nan


fig,ax = plt.subplots(3,1,figsize = (7.5,5))
zac_l[variable1].plot(ax = ax[0], label = '$ZAC\_L$: $LR_{in}$', color = 'tab:blue')
zac_u[variable1].plot(ax = ax[1], label = '$ZAC\_U$: $LR_{in}$', color = 'tab:orange')
zac_a[variable1].plot(ax = ax[2], label = '$ZAC\_A$: $LR_{in}$', color = 'tab:green')

zac_l[variable2].plot(ax = ax[0], label = '$ZAC\_L$: $LR_{out}$', color = 'deepskyblue')
zac_u[variable2].plot(ax = ax[1], label = '$ZAC\_U$: $LR_{out}$', color = 'gold')
zac_a[variable2].plot(ax = ax[2], label = '$ZAC\_A$: $LR_{out}$', color = 'limegreen')


zac_l_pre[variable1].plot(ax = ax[0], label = '', color = 'tab:blue', alpha = 0.3)
zac_u_pre[variable1].plot(ax = ax[1], label = '', color = 'tab:orange', alpha = 0.3)
zac_a_pre[variable1].plot(ax = ax[2], label = '', color = 'tab:green', alpha = 0.3)

zac_l_pre[variable2].plot(ax = ax[0], label = '', color = 'deepskyblue', alpha = 0.3)
zac_u_pre[variable2].plot(ax = ax[1], label = '', color = 'darkgoldenrod', alpha = 0.3)
zac_a_pre[variable2].plot(ax = ax[2], label = '', color = 'limegreen', alpha = 0.3)

ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel(ylabel)
ax[1].set_ylabel(ylabel)
ax[2].set_ylabel(ylabel)
#ax[0].set_title('ZAC_L')
#ax[1].set_title('ZAC_U')
#ax[2].set_title('ZAC_A')
#ax[0].set_ylim(0,100)
#ax[1].set_ylim(0,100)
#ax[2].set_ylim(0,100)
ax[0].set_xlim(datetime.datetime(2008,4,1),datetime.datetime(2022,5,1))
ax[1].set_xlim(datetime.datetime(2008,4,1),datetime.datetime(2022,5,1))
ax[2].set_xlim(datetime.datetime(2008,4,1),datetime.datetime(2022,5,1))
ax[0].set_xlabel('')
ax[1].set_xlabel('')
ax[2].set_xlabel('')
ax[0].legend(loc = 3)
ax[1].legend(loc = 3)
ax[2].legend()

for i, ax in enumerate(ax, start=1):
    ax.text(0.01, 0.95, '('+chr(96 + i)+')', transform=ax.transAxes, 
            fontsize=12, fontweight='normal', va='top')

fig.tight_layout()
#fig.savefig('QCfigs/LRin_compare.png')
fig.savefig('../glaciobasis/essd/manuscript/figures/fig10.png', dpi = 300)
#+END_SRC

#+RESULTS:
: None



****** gradients
#+BEGIN_SRC python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)

zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

variable = 'dlr'
ylabel = 'W m$^{-2}$'
zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan
<<plot_gradients_full_period>>


disc_data_l = (zac_l_pre[variable].count()-zac_l[variable].count())/zac_l_pre[variable].count()*100
disc_data_u = (zac_u_pre[variable].count()-zac_u[variable].count())/zac_u_pre[variable].count()*100
disc_data_a = (zac_a_pre[variable].count()-zac_a[variable].count())/zac_a_pre[variable].count()*100
print('The total percentage of discarded data at: ')
print('zac_l is '+ str(disc_data_l))
print('zac_u is '+ str(disc_data_u))
print('zac_a is '+ str(disc_data_a))
fig.savefig('QCfigs/LRin_final.png')

#+END_SRC


***** LWout

#+BEGIN_SRC python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)

zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

variable = 'ulr'
ylabel = 'W m$^{-2}$'
zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan
<<plot_gradients_full_period>>


disc_data_l = (zac_l_pre[variable].count()-zac_l[variable].count())/zac_l_pre[variable].count()*100
disc_data_u = (zac_u_pre[variable].count()-zac_u[variable].count())/zac_u_pre[variable].count()*100
disc_data_a = (zac_a_pre[variable].count()-zac_a[variable].count())/zac_a_pre[variable].count()*100
print('The total percentage of discarded data at: ')
print('zac_l is '+ str(disc_data_l))
print('zac_u is '+ str(disc_data_u))
print('zac_a is '+ str(disc_data_a))
fig.savefig('QCfigs/LRout_final.png')

#+END_SRC




***** Net LW
Comparing with KPC data

#+BEGIN_SRC python
<<load_libs>>
kpc_l = pd.read_csv('~/Downloads/KPC_L_hour.csv', index_col = 0, parse_dates = True)

lr_net = kpc_l['dlr']-kpc_l['ulr']
fig,ax = plt.subplots(2,1)
kpc_l.loc['2014']['dlr'].plot(ax=ax[0])
kpc_l.loc['2014']['ulr'].plot(ax=ax[0])
lr_net.loc['2014'].plot(ax=ax[1])
ax[0].legend()


rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)
lr_net = rad_l['dlr']-rad_l['ulr']
fig,ax = plt.subplots(2,1)
rad_l.loc['2012']['dlr'].plot(ax=ax[0])
rad_l.loc['2012']['ulr'].plot(ax=ax[0])
lr_net.loc['2012'].plot(ax=ax[1])
ax[0].legend()

#+END_SRC



***** Albedo
#+BEGIN_SRC python
<<load_libs>>
from sklearn.metrics import mean_squared_error,r2_score
import matplotlib.colors as mcolors
# meta data
#rcParams.update({'font.size': 8})
zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)

zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)
deep_purple = pd.read_csv('shunan_feng_albedo/zac_l_alb.csv')
deep_purple.index = pd.to_datetime(deep_purple['system:time_start'], format='%b %d, %Y')
deep_purple.drop(['system:time_start'], axis = 1, inplace = True)
deep_purple.index.name = 'Date'
deep_purple.rename(columns={'visnirAlbedo':'Albedo'}, inplace = True)

albedo = pd.DataFrame(zac_l['albedo'].dropna().resample('D').mean())
albedo.index.name = 'Date'

albedo_m2 = pd.DataFrame(zac_l['usr_corr'].resample('D').sum()/zac_l['dsr_corr'].resample('D').sum())
albedo_m2 = albedo_m2.where(albedo_m2> 0, np.nan)
albedo_m2 = albedo_m2.where(albedo_m2< 1, np.nan)
albedo_m2.index.name = 'Date'
albedo_m2.rename(columns = {0:'albedo'}, inplace = True)
print(albedo_m2)
#fig,ax = plt.subplots(1,1, figsize = (10,5))
#albedo.loc['2008'].plot(ax = ax)
#albedo_m2.loc['2008'].plot(ax = ax)

df = pd.merge(albedo_m2, deep_purple['Albedo'], on='Date', how='outer')

#print(df)
df = df.dropna()

years = df.index.year
#cmap = plt.cm.tab20
#norm = mcolors.Normalize(vmin=df['Year'].min(), vmax=df['Year'].max())
unique_years = np.unique(years)
color_map = plt.get_cmap('nipy_spectral', len(unique_years))
norm = plt.Normalize(min(unique_years), max(unique_years)+1)

fig, ax = plt.subplots(1,1, figsize = (3.5,3))
ax.plot([0, 1], [0, 1], color = 'gray', alpha = 0.5)
scatter = ax.scatter(df['albedo'], df['Albedo'], s = 5, alpha = 0.8)
#scatter = ax.scatter(df['albedo'], df['Albedo'], c=years, cmap=color_map, norm=norm, s = 5, alpha = 0.8)
#cbar = plt.colorbar(scatter, ticks=unique_years)
#cbar.set_label('Year')   
 

r2 = 1-(((df['albedo']-df['Albedo'])**2).sum())/(((df['albedo']-df['albedo'].mean())**2).sum())
r2 = np.round(r2*100)/100
#r2 = r2_score(observed_ice_melt_clean,modelled_ice_melt_clean) 
rmse = np.sqrt(mean_squared_error(df['albedo'], df['Albedo']))
rmse = np.round(rmse*100)/100

ax.text(0.01, 0.95, 'R = '+str(r2), transform=ax.transAxes, fontsize=10, va='top')
ax.text(0.01, 0.85, 'rmse = '+str(rmse), transform=ax.transAxes, fontsize=10, va='top')

# Scatterplot of 'Value1' vs 'Value2' with colors by 'Year'
#ax.scatter(df['albedo'], df['Albedo'], color=cmap(norm(df['Year'].values)))

# Create a colorbar
#sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
#fig.colorbar(sm)

ax.set_ylabel('Satellite derived albedo')
ax.set_xlabel('In-situ albedo')
ax.set_ylim(0,1)
ax.set_xlim(0,1)
fig.tight_layout()
#fig.savefig('QCfigs/albedo_vs_deep_purple_albedo.png', dpi = 300)
fig.savefig('../glaciobasis/essd/manuscript/figures/fig09.png', dpi = 300)
#+END_SRC



#+BEGIN_SRC python

for year in range(2008,2022):
    fig,ax = plt.subplots(1,1, figsize = (10,5))
    ax.plot(albedo.loc[str(year)].index.dayofyear,albedo.loc[str(year)])
    ax.scatter(deep_purple.loc[str(year)].index.dayofyear,deep_purple.loc[str(year)], color = 'purple')
    ax.set_xlim(120,255) 
    fig.savefig('QCfigs/zac_l_albedo_deep_purple_'+str(year)+'.png', dpi = 300)
#+END_SRC



#+BEGIN_SRC python




legend = []
fig,ax = plt.subplots(1,1, figsize = (15,15))
for year in range(2008,2022):
    ax.plot(albedo.loc[str(year)].index.dayofyear,albedo.loc[str(year)])
    legend.append(str(year))
ax.set_xlim(200,220)    
ax.legend(legend)
#+END_SRC


**** QC
***** paper plot
#+BEGIN_SRC python
<<load_libs>>
from matplotlib import rcParams
# meta data


datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

rcParams.update({'font.size': 8})
startyear = 2008
endyear = 2022
station = zac_l
station_name = 'zac_l'

station['I'] = station['I'].where(station['I']>=0,0)
#fig1, ax1 = plt.subplots(1,1, figsize = (10,3))
#fig2, ax2 = plt.subplots(1,1,figsize =(10,3))
#station['dsr_corr'].resample('D').max().plot(ax=ax2)

fig, axs = plt.subplots(2,1,figsize = (3.5, 5))
ax1 = axs[0]
ax2 = axs[1]

year = 2009
ax = ax1
station['I'].loc[str(year)].plot(ax = ax, color = 'gray', alpha = 0.5, label = '$I_{toa}$' )
station['dsr'].loc[str(year)].resample('D').max().plot(ax = ax,color = 'gray', label =  'SRin max')
station['dsr'].loc[str(year)].resample('D').min().plot(ax = ax, color = 'gray', linestyle = '--', label = 'SRin min')
station['dsr_corr'].loc[str(year)].resample('D').max().plot(ax = ax, color = 'gold', label =  'SRin_corr max', alpha = 0.8)
station['dsr_corr'].loc[str(year)].resample('D').min().plot(ax = ax, color = 'gold', linestyle = '--', label = 'SRin_corr min')

    
ax.set_ylim(0,1200)
ax.legend(loc = 1, ncols = 2)
ax.set_ylabel('$Wm^{2}$')

year = 2016
ax = ax2
station['I'].loc[str(year)].plot(ax = ax, color = 'gray', alpha = 0.5, label = '$I_{toa}$')
station['dsr'].loc[str(year)].resample('D').max().plot(ax = ax,color = 'gray', label =  'SRin max')
station['dsr'].loc[str(year)].resample('D').min().plot(ax = ax, color = 'gray', linestyle = '--', label = 'SRin min')
station['dsr_corr'].loc[str(year)].resample('D').max().plot(ax = ax, color = 'gold', label =  'SRin_corr max', alpha = 0.8)
station['dsr_corr'].loc[str(year)].resample('D').min().plot(ax = ax, color = 'gold', linestyle = '--', label = 'SRin_corr min')

    
ax.set_ylim(0,1200)
#ax.legend()g
ax.set_ylabel('$Wm^{2}$')

fig.tight_layout()
for i, ax in enumerate(axs, start=1):
    ax.text(0.01, 0.95, '('+chr(96 + i)+')', transform=ax.transAxes, 
            fontsize=12, fontweight='normal', va='top')
    
fig.savefig('../glaciobasis/essd/manuscript/figures/fig08.png',dpi = 300)
#fig2.savefig('QCfigs/'+station_name+'_SRin_corr_max_all_years.png')

#+END_SRC

#+RESULTS:
: None


***** Comparing potential global radiation to observations at zac_l

#+BEGIN_SRC python
<<load_libs>>
from matplotlib import rcParams
# meta data


datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

rcParams.update({'font.size': 6})
startyear = 2008
endyear = 2022
station = zac_l
station_name = 'zac_l'

<<Plot_all_SR_in_max_min_along_with_I>>

startyear = 2012
endyear = 2022
station = zac_u
station_name = 'zac_u'

<<Plot_all_SR_in_max_min_along_with_I>>

startyear = 2009
endyear = 2019
station = zac_a
station_name = 'zac_a'

<<Plot_all_SR_in_max_min_along_with_I>>
#+END_SRC


#+NAME: Plot_all_SR_in_max_min_along_with_I
#+BEGIN_SRC python
station['I'] = station['I'].where(station['I']>=0,0)
fig1, ax1 = plt.subplots(1,1, figsize = (10,3))
fig2, ax2 = plt.subplots(1,1,figsize =(10,3))
station['dsr_corr'].resample('D').max().plot(ax=ax2)
for year in range(startyear,endyear+1):
    y = station['dsr_corr'].loc[str(year)].resample('D').max()
    x = station['dsr_corr'].loc[str(year)].resample('D').max().index.dayofyear

    ax1.plot(x,y.rolling(14, center = True).max(), label = str(year))

    fig, ax = plt.subplots(1,1, figsize = (3,2))
    station['I'].loc[str(year)].plot(ax = ax)
    station['dsr'].loc[str(year)].resample('D').max().plot(ax = ax,color = 'tab:green', label =  'SRin max')
    station['dsr'].loc[str(year)].resample('D').min().plot(ax = ax, color = 'tab:green', linestyle = '--', label = 'SRin min')
    #station['dsr_corr'].loc[str(year)].plot(ax = ax, color = 'tab:orange')
    station['dsr_corr'].loc[str(year)].resample('D').max().plot(ax = ax, color = 'tab:orange', label =  'SRin_corr max')
    station['dsr_corr'].loc[str(year)].resample('D').min().plot(ax = ax, color = 'tab:orange', linestyle = '--', label = 'SRin_corr min')
    #station['dsr'].loc[str(year)].plot(ax = ax, linestyle = '--', linewidth = 0.5, alpha = 0.5, color = 'tab:green')
    
    ax.set_ylim(0,900)
    ax.legend()
    fig.tight_layout()
    fig.savefig('QCfigs/'+station_name+'_SRin_vs_I_'+str(year)+'.png')
fig1.savefig('QCfigs/'+station_name+'_SRin_corr_max_all_years_rolling.png')
fig2.savefig('QCfigs/'+station_name+'_SRin_corr_max_all_years.png')

#+END_SRC

***** Outliers

#+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rad_l = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)


fig,ax = plt.subplots(2,1,figsize = (10,5))
rad_l[['dsr_corr','usr_corr']].plot(ax = ax[0])
rad_l[['dlr','ulr']].plot(ax = ax[1])

fig,ax = plt.subplots(2,1,figsize = (10,5))
rad_u[['dsr_corr','usr_corr']].plot(ax = ax[0])
rad_u[['dlr','ulr']].plot(ax = ax[1])

fig,ax = plt.subplots(2,1,figsize = (10,5))
rad_a[['dsr_corr','usr_corr']].plot(ax = ax[0])
rad_a[['dlr','ulr']].plot(ax = ax[1])

#+END_SRC



***** Annual mean
#+BEGIN_SRC python
<<import_libraries>>

#+END_SRC


I have a suspicion that the radiometers calibration is not really good enough
But it looks like that dsr was just higher during 2016-2019 or something like that?

#+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)
mast_dsr = pd.read_csv('/home/shl@geus.dk/data/gem/climatebasis/climate_mast/View_ClimateBasis_Zackenberg_Data_Radiation_Short_wave_incoming_radiation_@_200_cm__5min_average_W_m2270920221347180965.csv', sep = '\t', index_col = 0, parse_dates = {'date':['Date','Time']}, na_values = -9999)

variable = 'dsr'
fig, ax = plt.subplots(1,1, figsize = (7,7))
rad_l[variable].resample('Y').mean().plot(ax = ax)
rad_a[variable].resample('Y').mean().plot(ax = ax)
rad_u[variable].resample('Y').mean().plot(ax = ax)
mast_dsr.resample('Y').mean().plot(ax=ax)
#ax1 = ax[0].twinx()
#rad_l[variable].resample('Y').count().plot(ax=ax1, linestyle = '--')

#+END_SRC


***** Drift
#+BEGIN_SRC python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

variable = 'dsr_corr'
ylabel = 'W$m^{-2}$'



fig,ax = plt.subplots(3,1,figsize = (10,10), sharex=True)
zac_l[variable].plot(ax = ax[0], label = 'SRin at ZAC_L', color = 'tab:blue')
zac_u[variable].plot(ax = ax[1], label = 'SRin at ZAC_U', color = 'tab:orange')
zac_a[variable].plot(ax = ax[2], label = 'SRin at ZAC_A', color = 'tab:pink')
ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel(ylabel)
ax[1].set_ylabel(ylabel)
ax[2].set_ylabel(ylabel)
ax[0].legend()
ax[1].legend()
ax[2].legend()
fig.savefig('QCfigs/SRin_drift.png')

#+END_SRC


***** Gradients
     #+BEGIN_SRC python
<<import_libraries>>
# meta data

#zac_l_elev = 644
#zac_u_elev = 877
#zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
variable = 'dsr_corr'

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rad_l = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

#<<read_in_hourly_preQC_radiation>>
#<<plot_rad_gradients_full_period>>
startdate=datetime.datetime(2008,1,1)
enddate= datetime.datetime(2008,12,31)
# We will remove 2011-03-15 13:00 to 2011-03-15 23:00

d_l_u = (rad_l-rad_u)
d_u_a = (rad_u-rad_a)
d_l_a = (rad_l-rad_a)


for year in range(2008, 2022+1):
    fig,ax = plt.subplots(4,1,figsize = (10,10), sharex=True)
    if year >= 2009 and year <2020:
        rad_a[variable][str(year)].plot(ax = ax[0], label = 'zac_a')
    ax[0].legend()
    ax[0].set_ylim(0,600)
    rad_u[variable][str(year)].plot(ax = ax[0], label = 'zac_u')
    rad_l[variable][str(year)].plot(ax = ax[0], label = 'zac_l')
    
    


    d_l_u[variable][str(year)].plot(ax=ax[1], label = 'zac_l minus zac_u')
    d_u_a[variable][str(year)].plot(ax=ax[2], label = 'zac_u minus zac_a')
    d_l_a[variable][str(year)].plot(ax=ax[3], label = 'zac_l minus zac_a')
    ax[1].legend()
    ax[2].legend()
    ax[3].legend()
    fig.savefig('QCfigs/'+variable+'_'+str(year)+'.png')


#+END_SRC


**** The issue from setting up zac_a

#+BEGIN_SRC python
<<read_in_hourly_radiation>>
startdate=datetime.datetime(2009,8,5)
enddate= datetime.datetime(2009,8,6)
# Startdate of record: 2009-08-06 21:00
<<plot_rad_gradients_selected_period>>
#+END_SRC



**** The visit in 2010
#+BEGIN_SRC python
<<read_in_hourly_radiation>>
startdate=datetime.datetime(2010,5,10)
enddate= datetime.datetime(2010,5,17)
<<plot_rad_gradients_selected_period>>
#+END_SRC


**** The several issues at zac_a in early 2011
#+BEGIN_SRC python
<<read_in_hourly_radiation>>
startdate=datetime.datetime(2011,3,14)
enddate= datetime.datetime(2011,3,17)
# We will remove 2011-03-15 13:00 to 2011-03-15 23:00
<<plot_rad_gradients_selected_period>>
#+END_SRC


#+BEGIN_SRC python
<<read_in_hourly_radiation>>
#<<read_in_hourly_radiation_postQC>>
startdate=datetime.datetime(2011,5,4)
enddate= datetime.datetime(2011,5,6)
# We will remove 2011-05-04 12:00 to 2011-05-05 17:00
<<plot_rad_gradients_selected_period>>
#+END_SRC

#+BEGIN_SRC python
<<read_in_hourly_radiation>>
#<<read_in_hourly_radiation_postQC>>
startdate=datetime.datetime(2011,1,1)
enddate= datetime.datetime(2011,11,1)
# We will remove 2011-05-04 12:00 to 2011-05-05 17:00
<<plot_rad_gradients_selected_period>>
#+END_SRC



**** The missing values in 2015


#+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
T_air = pd.read_csv(datapath+'zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
#print(T_air)
#<<read_in_hourly_radiation_postQC>>
<<read_in_hourly_preQC_radiation>>
startdate=datetime.datetime(2015,3,1)
#visit1 = datetime.datetime(2020,7,1)
#visit2 = datetime.datetime(2021,7,22)
enddate= datetime.datetime(2015,6,1)
fig, ax = plt.subplots(2,1)
ax1 = ax[0].twinx()
rad_l.loc[startdate:enddate].dsr_corr.plot(ax = ax[0])
rad_l.loc[startdate:enddate].usr_corr.plot(ax = ax[0])
rad_l.loc[startdate:enddate].dlr.plot(ax = ax[1])
rad_l.loc[startdate:enddate].ulr.plot(ax = ax[1])

T_air.loc[startdate:enddate].t_1.plot(ax = ax1, color = 'black', linewidth = 3)
#ymin,ymax = ax[1].get_ylim()
#print(ymax)
#ax[1].vlines(visit1, ymin, ymax, colors = 'black', linestyle = '--')
#ax[1].vlines(visit2, ymin, ymax, colors = 'black', linestyle = '--')
ax[0].legend()
ax[1].legend()

#ax[0].set_xlim(startdate,enddate)
#ax[1].set_xlim(startdate,enddate)

#fig.savefig('QCfigs/LR.png', dpi = 100)
#+END_SRC


**** Zac l in 2021 before the visit

the dsr values look suspecious, maybe the station has been too tilted or the instrument has problems. I will in any case remove the data up until the visit i July
Have a look at the annual figures in QCfigs

#+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
#<<read_in_hourly_radiation_postQC>>
<<read_in_hourly_preQC_radiation>>
startdate=datetime.datetime(2019,4,1)
visit1 = datetime.datetime(2020,7,1)
visit2 = datetime.datetime(2021,7,22)
enddate= datetime.datetime(2022,2,1)
fig, ax = plt.subplots(2,1)

rad_l.dsr_corr.plot(ax = ax[0])
rad_l.usr_corr.plot(ax = ax[0])
rad_l.dlr.plot(ax = ax[1])
rad_l.ulr.plot(ax = ax[1])
ymin,ymax = ax[1].get_ylim()
print(ymax)
ax[1].vlines(visit1, ymin, ymax, colors = 'black', linestyle = '--')
ax[1].vlines(visit2, ymin, ymax, colors = 'black', linestyle = '--')
ax[0].legend()
ax[1].legend()

ax[0].set_xlim(startdate,enddate)
ax[1].set_xlim(startdate,enddate)

fig.savefig('QCfigs/LR.png', dpi = 100)
#+END_SRC



*** Relative humidity



Load in the data
#+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rh_l = pd.read_csv(datapath+'zac_l_hour_relative_humidity.csv', parse_dates = True, index_col=0)
rh_u = pd.read_csv(datapath+'zac_u_hour_relative_humidity.csv', parse_dates = True, index_col=0)
rh_a = pd.read_csv(datapath+'zac_a_hour_relative_humidity.csv', parse_dates = True, index_col=0)

#+END_SRC


**** Post QC check

#+BEGIN_SRC python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_relative_humidity.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_relative_humidity.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_relative_humidity.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_relative_humidity.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_relative_humidity.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_relative_humidity.csv', parse_dates = True, index_col=0)

boom_l = pd.read_csv(datapath+'zac_l_hour_boom_height.csv', parse_dates = True, index_col=0)
boom_u = pd.read_csv(datapath+'zac_u_hour_boom_height.csv', parse_dates = True, index_col=0)
boom_a = pd.read_csv(datapath+'zac_a_hour_boom_height.csv', parse_dates = True, index_col=0)

variable = 'rh'
ylabel = '%'


zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan


fig,ax = plt.subplots(3,1,figsize = (7.5,5), sharex=True)
zac_l['rh'].plot(ax = ax[0], label = 'ZAC_L', color = 'tab:blue')
zac_u['rh'].plot(ax = ax[1], label = 'ZAC_U', color = 'tab:orange')
zac_a['rh'].plot(ax = ax[2], label = 'ZAC_A', color = 'tab:green')

zac_l_pre['rh'].plot(ax = ax[0], label = 'Discarded', color = 'tab:blue', alpha = 0.3)
zac_u_pre['rh'].plot(ax = ax[1], label = 'Discarded', color = 'tab:orange', alpha = 0.3)
zac_a_pre['rh'].plot(ax = ax[2], label = 'Discarded', color = 'tab:green', alpha = 0.3)


#ax0 = ax[0].twinx()
#ax1 = ax[1].twinx()
#ax2 = ax[2].twinx()
#boom_l['z_boom'].plot(ax = ax0, color = 'gray', alpha = 0.5)
#boom_u['z_boom'].plot(ax = ax1, color = 'gray', alpha = 0.5)
#boom_a['z_boom'].plot(ax = ax2, color = 'gray', alpha = 0.5)


ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel('%')
ax[1].set_ylabel('%')
ax[2].set_ylabel('%')
ax[0].set_ylim(0,100)
ax[1].set_ylim(0,100)
ax[2].set_ylim(0,100)

ax[0].legend(loc = 3)
ax[1].legend(loc = 3)
ax[2].legend(loc = 3)

# Label each subplot
for i, ax in enumerate(ax, start=1):
    ax.text(0.01, 0.95, '('+chr(96 + i)+')', transform=ax.transAxes, 
            fontsize=12, fontweight='normal', va='top')
fig.tight_layout()
#fig.savefig('QCfigs/RH_compare.png')
fig.savefig('../glaciobasis/essd/manuscript/figures/fig05.png', dpi = 300)
#+END_SRC

#+RESULTS:
: None


#+BEGIN_SRC python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_relative_humidity.csv', parse_dates = True, index_col=0)

zac_u = pd.read_csv(datapath+'zac_u_hour_relative_humidity.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_relative_humidity.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_relative_humidity.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_relative_humidity.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_relative_humidity.csv', parse_dates = True, index_col=0)

variable = 'rh'
ylabel = '%'

zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan
<<plot_gradients_full_period>>


disc_data_l = (zac_l_pre[variable].count()-zac_l[variable].count())/zac_l_pre[variable].count()*100
disc_data_u = (zac_u_pre[variable].count()-zac_u[variable].count())/zac_u_pre[variable].count()*100
disc_data_a = (zac_a_pre[variable].count()-zac_a[variable].count())/zac_a_pre[variable].count()*100
print('The total percentage of discarded data at: ')
print('zac_l is '+ str(disc_data_l))
print('zac_u is '+ str(disc_data_u))
print('zac_a is '+ str(disc_data_a))
fig.savefig('QCfigs/RH_final.png')

#+END_SRC


Just checking that the RH errors at ZAC_A is not related to the correction - it is not.

#+BEGIN_SRC python
# RH raw
<<load_libs>>
import glob
import nead
workingdir ='/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/'  
station = 'zac_a'
filelist = glob.glob(workingdir+'data_v1.0/L0M/'+station+'/'+station+'**.csv')
for infile in filelist:
    print(str(infile))
    <<read_infile_from_filelist>>
    <<add_variable_metadata>>
    ds = add_variable_metadata(ds)
    ds.rh.plot()

#+END_SRC



**** Checking for outliers
#+BEGIN_SRC python
fig, ax = plt.subplots(3,1,figsize=(10,10))
rh_l.plot(ax= ax[0])
rh_u.plot(ax= ax[1])
rh_a.plot(ax= ax[2])


#+END_SRC


**** Gradients
#+BEGIN_SRC python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (rh_l-rh_u)
d_u_a = (rh_u-rh_a)
d_l_a = (rh_l-rh_a)

rh_l['rh'].plot(ax= ax[0], label = 'zac_l')
rh_u['rh'].plot(ax= ax[0], label = 'zac_u')
rh_a['rh'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])


#+END_SRC



***** The issue at zac_u in 2020-2021

#+BEGIN_SRC python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (rh_l-rh_u)
d_u_a = (rh_u-rh_a)
d_l_a = (rh_l-rh_a)

rh_l['rh'].plot(ax= ax[0], label = 'zac_l')
rh_u['rh'].plot(ax= ax[0], label = 'zac_u')
rh_a['rh'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2020,1,1),datetime.datetime(2021,9,1))
startday = datetime.datetime(2020,8,15)
endday = datetime.datetime(2021,7,21)
ymin,ymax = 0, 100
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC


***** zac_a data appears to be a bit more unstable - but maybe its real

We will take a look at 2013-2014
#+BEGIN_SRC python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (rh_l-rh_u)
d_u_a = (rh_u-rh_a)
d_l_a = (rh_l-rh_a)

rh_l['rh'].plot(ax= ax[0], label = 'zac_l')
rh_u['rh'].plot(ax= ax[0], label = 'zac_u')
rh_a['rh'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2013,1,1),datetime.datetime(2014,12,1))
startday = datetime.datetime(2013,1,1)
endday = datetime.datetime(2014,12,1)
ymin,ymax = 0, 100
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC


*** Windspeed

**** post QC check

#+BEGIN_SRC python
<<load_libs>>
# meta data
rcParams.update({'font.size': 8})
zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_wind_speed.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_wind_speed.csv', parse_dates = True, index_col=0)

variable = 'wspd'
ylabel = 'm/s'


zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan


fig,ax = plt.subplots(3,1,figsize = (8,6), sharex=True)
zac_l[variable].plot(ax = ax[0], label = 'ZAC_L', color = 'tab:blue')
zac_u[variable].plot(ax = ax[1], label = 'ZAC_U', color = 'tab:orange')
zac_a[variable].plot(ax = ax[2], label = 'ZAC_A', color = 'tab:pink')

zac_l_pre[variable].plot(ax = ax[0], label = 'Discarded', color = 'tab:blue', alpha = 0.3)
zac_u_pre[variable].plot(ax = ax[1], label = 'Discarded', color = 'tab:orange', alpha = 0.3)
zac_a_pre[variable].plot(ax = ax[2], label = 'Discarded', color = 'tab:pink', alpha = 0.3)

ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel(ylabel)
ax[1].set_ylabel(ylabel)
ax[2].set_ylabel(ylabel)
#ax[0].set_ylim(0,100)
#ax[1].set_ylim(0,100)
#ax[2].set_ylim(0,100)

ax[0].legend()
ax[1].legend()
ax[2].legend()
fig.savefig('QCfigs/WindSpeed_compare.png')

#+END_SRC


***** Drift 
#+BEGIN_SRC python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_wind_speed.csv', parse_dates = True, index_col=0)

variable = 'wspd'
ylabel = 'm/s'



fig,ax = plt.subplots(3,1,figsize = (10,10), sharex=True)
zac_l[variable].plot(ax = ax[0], label = 'Wind_speed at ZAC_L', color = 'tab:blue')
zac_u[variable].plot(ax = ax[1], label = 'Wind_speed at ZAC_U', color = 'tab:orange')
zac_a[variable].plot(ax = ax[2], label = 'Wind_speed at ZAC_A', color = 'tab:pink')
ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel(ylabel)
ax[1].set_ylabel(ylabel)
ax[2].set_ylabel(ylabel)
ax[0].legend()
ax[1].legend()
ax[2].legend()
fig.savefig('QCfigs/wind_speed_drift.png')

#+END_SRC



***** Gradients

#+BEGIN_SRC python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_wind_speed.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_wind_speed.csv', parse_dates = True, index_col=0)

variable = 'wspd'
ylabel = 'm/s'
<<plot_gradients_full_period>>

disc_data_l = (zac_l_pre[variable].count()-zac_l[variable].count())/zac_l_pre[variable].count()*100
disc_data_u = (zac_u_pre[variable].count()-zac_u[variable].count())/zac_u_pre[variable].count()*100
disc_data_a = (zac_a_pre[variable].count()-zac_a[variable].count())/zac_a_pre[variable].count()*100
print('The total percentage of discarded data at: ')
print('zac_l is '+ str(disc_data_l))
print('zac_u is '+ str(disc_data_u))
print('zac_a is '+ str(disc_data_a))
fig.savefig('QCfigs/Wind_Speed_final.png')

#+END_SRC



#+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
wspd_l = pd.read_csv(datapath+'zac_l_hour_wind_speed.csv', parse_dates = True, index_col=0)
wspd_u = pd.read_csv(datapath+'zac_u_hour_wind_speed.csv', parse_dates = True, index_col=0)
wspd_a = pd.read_csv(datapath+'zac_a_hour_wind_speed.csv', parse_dates = True, index_col=0)

#+END_SRC


**** Checking for outliers

It seems like windspeed is pretty solid
#+BEGIN_SRC python
fig, ax = plt.subplots(3,1,figsize=(10,10))
wspd_l.plot(ax= ax[0])
wspd_u.plot(ax= ax[1])
wspd_a.plot(ax= ax[2])


#+END_SRC


***** Drift
#+BEGIN_SRC python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_wind_speed.csv', parse_dates = True, index_col=0)

variable = 'wspd'
ylabel = 'm/s'



fig,ax = plt.subplots(3,1,figsize = (10,10), sharex=True)
zac_l[variable].plot(ax = ax[0], label = 'Wind speed at ZAC_L', color = 'tab:blue')
zac_u[variable].plot(ax = ax[1], label = 'Wind speed at ZAC_U', color = 'tab:orange')
zac_a[variable].plot(ax = ax[2], label = 'Wind speed at ZAC_A', color = 'tab:pink')
ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel(ylabel)
ax[1].set_ylabel(ylabel)
ax[2].set_ylabel(ylabel)
ax[0].legend()
ax[1].legend()
ax[2].legend()
fig.savefig('QCfigs/wind_speed_drift.png')

#+END_SRC



***** Gradients
     #+BEGIN_SRC python
<<import_libraries>>
# meta data

#zac_l_elev = 644
#zac_u_elev = 877
#zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
variable = 'dsr_corr'

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rad_l = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

#<<read_in_hourly_preQC_radiation>>
#<<plot_rad_gradients_full_period>>
startdate=datetime.datetime(2008,1,1)
enddate= datetime.datetime(2008,12,31)
# We will remove 2011-03-15 13:00 to 2011-03-15 23:00

d_l_u = (rad_l-rad_u)
d_u_a = (rad_u-rad_a)
d_l_a = (rad_l-rad_a)


for year in range(2008, 2022+1):
    fig,ax = plt.subplots(4,1,figsize = (10,10), sharex=True)
    if year >= 2009 and year <2020:
        rad_a[variable][str(year)].plot(ax = ax[0], label = 'zac_a')
    ax[0].legend()
    ax[0].set_ylim(0,600)
    rad_u[variable][str(year)].plot(ax = ax[0], label = 'zac_u')
    rad_l[variable][str(year)].plot(ax = ax[0], label = 'zac_l')
    
    


    d_l_u[variable][str(year)].plot(ax=ax[1], label = 'zac_l minus zac_u')
    d_u_a[variable][str(year)].plot(ax=ax[2], label = 'zac_u minus zac_a')
    d_l_a[variable][str(year)].plot(ax=ax[3], label = 'zac_l minus zac_a')
    ax[1].legend()
    ax[2].legend()
    ax[3].legend()
    fig.savefig('QCfigs/'+variable+'_'+str(year)+'.png')


#+END_SRC



**** Gradients old
#+BEGIN_SRC python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (wspd_l-wspd_u)
d_u_a = (wspd_u-wspd_a)
d_l_a = (wspd_l-wspd_a)

wspd_l['wspd'].plot(ax= ax[0], label = 'zac_l')
wspd_u['wspd'].plot(ax= ax[0], label = 'zac_u')
wspd_a['wspd'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])


#+END_SRC


***** The issue at zac_u in 2020 (station tilting)

#+BEGIN_SRC python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (wspd_l-wspd_u)
d_u_a = (wspd_u-wspd_a)
d_l_a = (wspd_l-wspd_a)

wspd_l['wspd'].plot(ax= ax[0], label = 'zac_l')
wspd_u['wspd'].plot(ax= ax[0], label = 'zac_u')
wspd_a['wspd'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2020,1,1),datetime.datetime(2022,4,25))
startday = datetime.datetime(2020,8,15)
endday = datetime.datetime(2022,4,21)
ymin,ymax = ax[0].get_ylim()
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC


*** Pressure

#+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
p_l = pd.read_csv(datapath+'zac_l_hour_pressure.csv', parse_dates = True, index_col=0)
p_u = pd.read_csv(datapath+'zac_u_hour_pressure.csv', parse_dates = True, index_col=0)
p_a = pd.read_csv(datapath+'zac_a_hour_pressure.csv', parse_dates = True, index_col=0)

#+END_SRC



**** Post QC check

***** Drift 
#+BEGIN_SRC python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_pressure.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_pressure.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_pressure.csv', parse_dates = True, index_col=0)

variable = 'p'
ylabel = '%'



fig,ax = plt.subplots(3,1,figsize = (10,10), sharex=True)
zac_l['p'].plot(ax = ax[0], label = 'Air pressure at ZAC_L', color = 'tab:blue')
zac_u['p'].plot(ax = ax[1], label = 'Air pressure at ZAC_U', color = 'tab:orange')
zac_a['p'].plot(ax = ax[2], label = 'Air pressure at ZAC_A', color = 'tab:pink')
ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel('hPa')
ax[1].set_ylabel('hPa')
ax[2].set_ylabel('hPa')
ax[0].legend()
ax[1].legend()
ax[2].legend()
fig.savefig('QCfigs/Pressure_drift.png')

#+END_SRC


***** Gradient
#+BEGIN_SRC python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_pressure.csv', parse_dates = True, index_col=0)

zac_u = pd.read_csv(datapath+'zac_u_hour_pressure.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_pressure.csv', parse_dates = True, index_col=0)

# Only outliers are removed - and I will not plot them here - so I just plotting the post QC data twice
zac_l_pre = pd.read_csv(datapath+'zac_l_hour_pressure.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'zac_u_hour_pressure.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'zac_a_hour_pressure.csv', parse_dates = True, index_col=0)

variable = 'p'
ylabel = 'hPa'

zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan
<<plot_gradients_full_period>>
ax[1].set_ylim(-13,-10)
ax[2].set_ylim(-13,-10)
ax[3].set_ylim(-13,-10)

disc_data_l = (zac_l_pre[variable].count()-zac_l[variable].count())/zac_l_pre[variable].count()*100
disc_data_u = (zac_u_pre[variable].count()-zac_u[variable].count())/zac_u_pre[variable].count()*100
disc_data_a = (zac_a_pre[variable].count()-zac_a[variable].count())/zac_a_pre[variable].count()*100
print('The total percentage of discarded data at: ')
print('zac_l is '+ str(disc_data_l))
print('zac_u is '+ str(disc_data_u))
print('zac_a is '+ str(disc_data_a))
fig.savefig('QCfigs/pressure_final.png')

#+END_SRC



**** Checking for outliers

It seems like windspeed is pretty solid
#+BEGIN_SRC python
fig, ax = plt.subplots(3,1,figsize=(10,10))
p_l.plot(ax= ax[0])
p_u.plot(ax= ax[1])
p_a.plot(ax= ax[2])


#+END_SRC


**** Gradients
#+BEGIN_SRC python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (p_l-p_u)
d_u_a = (p_u-p_a)
d_l_a = (p_l-p_a)

p_l['p'].plot(ax= ax[0], label = 'zac_l')
p_u['p'].plot(ax= ax[0], label = 'zac_u')
p_a['p'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])


#+END_SRC


***** The issue in 2016 at zac_l
#+BEGIN_SRC python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (p_l-p_u)
d_u_a = (p_u-p_a)
d_l_a = (p_l-p_a)

p_l['p'].plot(ax= ax[0], label = 'zac_l')
p_u['p'].plot(ax= ax[0], label = 'zac_u')
p_a['p'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2016,1,1),datetime.datetime(2016,5,1))
startday = datetime.datetime(2016,2,26)
endday = datetime.datetime(2016,3,1)
ymin,ymax = ax[0].get_ylim()
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[2].get_ylim()
ax[2].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[3].get_ylim()
ax[3].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC



***** The issue in 2016 at zac_u
#+BEGIN_SRC python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (p_l-p_u)
d_u_a = (p_u-p_a)
d_l_a = (p_l-p_a)

p_l['p'].plot(ax= ax[0], label = 'zac_l')
p_u['p'].plot(ax= ax[0], label = 'zac_u')
p_a['p'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2016,1,1),datetime.datetime(2016,5,1))
startday = datetime.datetime(2016,4,5)
endday = datetime.datetime(2016,4,18)
ymin,ymax = ax[0].get_ylim()
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[2].get_ylim()
ax[2].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[3].get_ylim()
ax[3].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC






***** The issue at zac_l in 2016-2017
#+BEGIN_SRC python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (p_l-p_u)
d_u_a = (p_u-p_a)
d_l_a = (p_l-p_a)

p_l['p'].plot(ax= ax[0], label = 'zac_l')
p_u['p'].plot(ax= ax[0], label = 'zac_u')
p_a['p'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2016,10,1),datetime.datetime(2017,4,1))
startday = datetime.datetime(2017,1,5)
endday = datetime.datetime(2017,2,22)
ymin,ymax = ax[0].get_ylim()
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[2].get_ylim()
ax[2].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[3].get_ylim()
ax[3].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC


***** The issue at zac_u in 2016-2017
#+BEGIN_SRC python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (p_l-p_u)
d_u_a = (p_u-p_a)
d_l_a = (p_l-p_a)

p_l['p'].plot(ax= ax[0], label = 'zac_l')
p_u['p'].plot(ax= ax[0], label = 'zac_u')
p_a['p'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2016,10,1),datetime.datetime(2017,4,1))
startday = datetime.datetime(2017,1,23)
endday = datetime.datetime(2017,3,2)
ymin,ymax = ax[0].get_ylim()
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[2].get_ylim()
ax[2].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[3].get_ylim()
ax[3].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC



***** The issue at zac_l in 2018
#+BEGIN_SRC python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (p_l-p_u)
d_u_a = (p_u-p_a)
d_l_a = (p_l-p_a)

p_l['p'].plot(ax= ax[0], label = 'zac_l')
p_u['p'].plot(ax= ax[0], label = 'zac_u')
p_a['p'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2018,1,1),datetime.datetime(2018,6,1))
startday = datetime.datetime(2018,2,21)
endday = datetime.datetime(2018,2,28)
ymin,ymax = ax[0].get_ylim()
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[2].get_ylim()
ax[2].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[3].get_ylim()
ax[3].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC



*** Instrument height
**** post QC check
Gradients
boom_a.rolling(24,center=True).median().plot()
#+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
variable = 'z_boom'
boom_l = pd.read_csv(datapath+'zac_l_hour_boom_height.csv', parse_dates = True, index_col=0)
boom_u = pd.read_csv(datapath+'zac_u_hour_boom_height.csv', parse_dates = True, index_col=0)
boom_a = pd.read_csv(datapath+'zac_a_hour_boom_height.csv', parse_dates = True, index_col=0)

fig, ax = plt.subplots(3,1,figsize=(10,10), sharex = True, sharey = True)
boom_l.plot(ax=ax[0])
boom_l.rolling(24,center=True).median().rolling(24*7,center=True).median().plot(ax = ax[0])

boom_u.plot(ax=ax[1])
boom_u.rolling(24,center=True).median().plot(ax = ax[1])
boom_a.plot(ax=ax[2])
boom_a.rolling(24,center=True).median().plot(ax = ax[2])
ax[2].set_xlim(datetime.datetime(2008,1,1),datetime.datetime(2022,12,31))
#+END_SRC


#+BEGIN_SRC python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
variable = 'z_boom'
boom_l = pd.read_csv(datapath+'zac_l_day_boom_height.csv', parse_dates = True, index_col=0)
boom_u = pd.read_csv(datapath+'zac_u_day_boom_height.csv', parse_dates = True, index_col=0)
boom_a = pd.read_csv(datapath+'zac_a_day_boom_height.csv', parse_dates = True, index_col=0)

fig, ax = plt.subplots(3,1,figsize=(10,10), sharex = True, sharey = True)
boom_l.plot(ax=ax[0])
boom_l.rolling(3,center=True).median().plot(ax = ax[0])

boom_u.plot(ax=ax[1])
boom_u.rolling(3,center=True).median().plot(ax = ax[1])
boom_a.plot(ax=ax[2])
boom_a.rolling(3,center=True).median().plot(ax = ax[2])
ax[2].set_xlim(datetime.datetime(2008,1,1),datetime.datetime(2022,12,31))
#+END_SRC


**** QC


#+BEGIN_SRC python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
variable = 'z_boom'
boom_l = pd.read_csv(datapath+'preQC/zac_l_hour_boom_height.csv', parse_dates = True, index_col=0)
#boom_u = pd.read_csv(datapath+'preQC/zac_u_hour_boom_height.csv', parse_dates = True, index_col=0)
#boom_a = pd.read_csv(datapath+'preQC/zac_a_hour_boom_height.csv', parse_dates = True, index_col=0)

boom_l.plot()
#boom_u.plot()
#boom_a.plot()

#.rolling(144,center=True).mean()
#+END_SRC


**** The issue at zac_l from 2010 to 2011
#+BEGIN_SRC python
fig, ax = plt.subplots(1,1,figsize=(10,5))

boom_l['2009':'2012']['z_boom'].plot(ax= ax, label = 'zac_l')
ax.legend()

startday = datetime.datetime(2011,1,25)
endday = datetime.datetime(2011,5,3)
ymin,ymax = ax.get_ylim()
ax.vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')


#+END_SRC


**** The issue at zac_u from 2019 to 2021
#+BEGIN_SRC python
fig, ax = plt.subplots(1,1,figsize=(10,5))

boom_u['2018':'2022']['z_boom'].plot(ax= ax, label = 'zac_u')
ax.legend()

startday = datetime.datetime(2019,6,20)
endday = datetime.datetime(2021,7,26)
ymin,ymax = ax.get_ylim()
ax.vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')


#+END_SRC


**** The issue at zac_u from 2012 to 2016
#+BEGIN_SRC python
fig, ax = plt.subplots(1,1,figsize=(10,5))

boom_u['2012':'2020']['z_boom'].plot(ax= ax, label = 'zac_u')
ax.legend()

startday = datetime.datetime(2012,1,1)
endday = datetime.datetime(2016,4,20)
ymin,ymax = ax.get_ylim()
ax.vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')


#+END_SRC


**** The issue at zac_a in 2012 and 2013

#+BEGIN_SRC python
fig, ax = plt.subplots(1,1,figsize=(10,5))

boom_a['2012':'2013']['z_boom'].plot(ax= ax, label = 'zac_a')
ax.legend()

startday = datetime.datetime(2012,4,16)
endday = datetime.datetime(2013,8,28)
ymin,ymax = ax.get_ylim()
ax.vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')


#+END_SRC


**** The issue at zac_a in dec 2013 to April 2014
#+BEGIN_SRC python
fig, ax = plt.subplots(1,1,figsize=(10,5))

boom_a['December-2013':'April-2014']['z_boom'].plot(ax= ax, label = 'zac_a')
ax.legend()

startday = datetime.datetime(2013,12,20)
endday = datetime.datetime(2014,4,22)
ymin,ymax = ax.get_ylim()
ax.vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')


#+END_SRC



**** The issue at zac_a in Jan 2015 to April 2016
#+BEGIN_SRC python
fig, ax = plt.subplots(1,1,figsize=(10,5))

boom_a['December-2014':'April-2016']['z_boom'].plot(ax= ax, label = 'zac_a')
ax.legend()

startday = datetime.datetime(2015,1,2)
endday = datetime.datetime(2016,4,22)
ymin,ymax = ax.get_ylim()
ax.vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')


#+END_SRC

**** The issue at zac_a in April 2018 
#+BEGIN_SRC python
fig, ax = plt.subplots(1,1,figsize=(10,5))

boom_a['April-2018':'May-2018']['z_boom'].plot(ax= ax, label = 'zac_a')
ax.legend()

startday = datetime.datetime(2018,4,1)
endday = datetime.datetime(2018,4,23)
ymin,ymax = ax.get_ylim()
ax.vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')


#+END_SRC





** Utilities


#+NAME: data_file_paths
#+BEGIN_SRC python :session
station = 'zac_l'
filename = 'zac_l-2008-2022.nc'
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/'+station+'/'
zac_l_path = datapath+filename

station = 'zac_u'
filename = 'zac_u-2008-2022.nc'
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/'+station+'/'
zac_u_path = datapath+filename

station = 'zac_a'
filename = 'zac_a-2009-2020.nc'
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/'+station+'/'
zac_a_path = datapath+filename
#+END_SRC

#+NAME: Load_transmitted_data
#+BEGIN_SRC python
datadir = '/home/shl@geus.dk/OneDrive/projects/glaciobasis/data/aws_transmitted/'
station = 'zac_a'
filename = 'AWS_300234061218540.txt'
diagnostic = 'AWS_300234061218540-D.txt'
transmitted = pd.read_csv(datadir+station+'/'+filename,header=0,skiprows=[1,2,3],sep=',',engine='python')
transmitted.index = pd.to_datetime(transmitted[' timestamp'])
transmitted = transmitted.drop(' timestamp', axis = 1)
diag = pd.read_csv(datadir+station+'/'+diagnostic,header=0,skiprows=[1],sep=',',engine='python')
#print(data.keys().tolist())
#print(diag.keys())



#+END_SRC

#+NAME: load_transmitted_trusted
#+BEGIN_SRC python
<<Load_transmitted_data>>
transmitted = transmitted[:'2019-August-30'].astype(float)


# The transmitted data is divided into timestamps so 
timestep = transmitted.index[1:]-transmitted.index[0:-1]

timestep_hour = pd.DataFrame(timestep.components.hours)
timestep_hour.index =  transmitted.index[0:-1]
transmitted_hour = transmitted[0:-1][(timestep_hour==1).values]

timestep_day = timestep.components.days
timestep_day.index =  transmitted.index[0:-1]
transmitted_day = transmitted[0:-1][(timestep_day==1).values]


#+END_SRC


#+BEGIN_SRC python
fig, ax = plt.subplots(2,1,figsize=(10,5))
ax[0].plot(transmitted.index[1:], timestep_hour )
ax[0].plot(transmitted.index[1:], timestep_day )
ax[1].plot(transmitted_hour['temperature'])
ax[1].plot(transmitted_day['temperature'])


   #+END_SRC


** Pre 2022 database delivery

*** Near surface climate

 #+BEGIN_SRC python
 import xarray as xr

 destdir = '/home/shl@geus.dk/OneDrive/projects/glaciobasis/gem_database/data/2022/'

 filename = 'data_v1.0/L1/zac_l/zac_l-2008-2021.nc'
 destname = 'GlacioBasis_Zackenberg_Near_surface_weather_AP_Olsen_AWS_Zack_M'
 <<extract_the_near_surface_climate_columns>>
 data_to_file.to_csv(destdir+destname+'.txt',sep = '\t', index = False, float_format = '%g')

 filename = 'data_v1.0/L1/zac_u/zac_u-2008-2021.nc'
 destname = 'GlacioBasis_Zackenberg_Near_surface_weather_AP_Olsen_AWS_Zack_S'
 <<extract_the_near_surface_climate_columns>>
 data_to_file.to_csv(destdir+destname+'.txt',sep = '\t', index = False, float_format = '%g')

 filename = 'data_v1.0/L1/zac_a/zac_a-2009-2020.nc'
 destname = 'GlacioBasis_Zackenberg_Near_surface_weather_AP_Olsen_AWS_Zack_T'
 <<extract_the_near_surface_climate_columns>>
 data_to_file.to_csv(destdir+destname+'.txt',sep = '\t', index = False, float_format = '%g')
 #+END_SRC

 

 #+NAME: extract_the_near_surface_climate_columns
 #+BEGIN_SRC python
 with xr.open_dataset(filename) as ds:
     data_ds = ds[['p','t_1','rh','wspd','wdir','dsr_corr','usr_corr','dlr','ulr','t_rad','tilt_x','tilt_y']]

 data = data_ds.to_dataframe()
 data['datetime'] = data.index
 data.index.name = 'datetime1'
 data["date"] = data["datetime"].dt.date
 data["time"] = data["datetime"].dt.time

 data.rename(columns = {'p':'p_atm','t_1':'T_air','rh':'RH_water','wspd':'ws','wdir':'wd','dsr_corr':'SW_in','usr_corr':'SW_out','dlr':'LW_in','ulr':'LW_out'}, inplace = True)

 data_to_file = data[['date','time','p_atm', 'T_air', 'RH_water', 'ws','wd','SW_in','SW_out','LW_in','LW_out']]

 #+END_SRC


*** Snow sonic ranger

 date	time	h_snow_M	h_snow_M_qual	h_snow_S	h_snow_S_qual	h_snow_T	h_snow_T_qual
 GlacioBasis_Zackenberg_Snow_cover_AP_Olsen_Snow_sonic_ranger_height.txt
 z_boom
 z_boom_q

 #+BEGIN_SRC python
 import xarray as xr
 import pandas as pd

 destdir = '/home/shl@geus.dk/OneDrive/projects/glaciobasis/gem_database/data/2022/'

 filename = 'data_v1.0/L1/zac_l/zac_l-2008-2021.nc'
 destname = 'GlacioBasis_Zackenberg_Snow_cover_AP_Olsen_Snow_sonic_ranger_height'
 with xr.open_dataset(filename) as ds:
     data_ds = ds[['z_boom','z_boom_q']]

 data = data_ds.to_dataframe()
 data['datetime'] = pd.to_datetime(data.index)
 data_M = data.rename(columns = {'z_boom':'h_snow_M','z_boom_q':'h_snow_M_qual'})


 filename = 'data_v1.0/L1/zac_u/zac_u-2008-2021.nc'
 with xr.open_dataset(filename) as ds:
     data_ds = ds[['z_boom','z_boom_q']]

 data = data_ds.to_dataframe()
 data['datetime'] = pd.to_datetime(data.index)
 data_S = data.rename(columns = {'z_boom':'h_snow_S','z_boom_q':'h_snow_S_qual'})

 filename = 'data_v1.0/L1/zac_a/zac_a-2009-2020.nc'
 with xr.open_dataset(filename) as ds:
     data_ds = ds[['z_boom','z_boom_q']]

 data = data_ds.to_dataframe()
 data['datetime'] = pd.to_datetime(data.index)
 data_T = data.rename(columns = {'z_boom':'h_snow_T','z_boom_q':'h_snow_T_qual'})

 data_to_file = pd.merge(data_M,data_T, on = 'datetime', how = 'outer')
 data_to_file = pd.merge(data_to_file,data_S,on = 'datetime', how = 'outer')
 data_to_file["date"] = data_to_file["datetime"].dt.date
 data_to_file["time"] = data_to_file["datetime"].dt.time
 data_to_file = data_to_file[['date','time','h_snow_M','h_snow_M_qual','h_snow_S','h_snow_S_qual','h_snow_T','h_snow_T_qual']]
 #data_to_file = data_M.merge(data_S, on = 'datetime', how = 'outer').merge(data_T,on='datetime', how='outer')

 data_to_file.to_csv(destdir+destname+'.txt',sep = '\t', index = False, float_format = '%g')
 #+END_SRC




 #+NAME: extract_the_SR50_snow_height
 #+BEGIN_SRC python
 with xr.open_dataset(filename) as ds:
     data_ds = ds[['z_boom','z_boom_q']]

 data = data_ds.to_dataframe()
 data['datetime'] = data.index
 data.index.name = 'datetime1'
 data["date"] = data["datetime"].dt.date
 data["time"] = data["datetime"].dt.time



 #+END_SRC

 
* Assemble variables in one netcdf and one csv per site

During the QC the nead headers and metadata got a bit messed up, so I re-add it here. This should be fixed. 

** Pre QC (supplementary)

#+BEGIN_SRC python
import pandas as pd
import xarray as xr
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rcParams
import matplotlib.dates as mdates

rcParams.update({'font.size': 8})
workingfolder = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/preQC/'

#zac_l
site = 'zac_l'
temp = pd.read_csv(workingfolder + site + '_hour_temperature.csv', index_col = 0)
rh = pd.read_csv(workingfolder + site + '_hour_relative_humidity.csv', index_col = 0)
p = pd.read_csv(workingfolder + site + '_hour_pressure.csv', index_col = 0)
rad = pd.read_csv(workingfolder + site + '_hour_radiation.csv', index_col = 0)
tilt = pd.read_csv(workingfolder + site + '_hour_tilt.csv', index_col = 0)
ws = pd.read_csv(workingfolder + site + '_hour_wind_speed.csv', index_col = 0)
height = pd.read_csv(workingfolder + site + '_hour_boom_height.csv', index_col = 0)
#surf = pd.read_csv(workingfolder + site + '_hour_lowering.csv', index_col = 0)
ablation = pd.read_csv(workingfolder + site + '_hour_ice_ablation.csv', index_col = 0)

merged = temp.join(rh, how='outer').join(p, how='outer').join(rad, how='outer').join(tilt, how='outer').join(ws, how='outer').join(height, how='outer').join(ablation, how='outer')

merged.rename(columns = {'t_1':'t_u', 'rh': 'rh_u_corr', 'p': 'p_u', 'dsr':'dsr', 'usr':'usr','dsr_corr':'dsr_corr', 'usr_corr':'usr_corr', 'dlr':'dlr', 'ulr':'ulr','cloud_cov':'cloud_cover','t_surf':'t_surf', 'wspd':'wspd', 'z_boom':'z_boom', 'ice_ablation':'ice_ablation' }, inplace = True)
#merged.plot(subplots = True, figsize = (12,12))

cols = ['t_u', 'rh_u_corr', 'p_u', 'dsr_corr', 'usr_corr', 'dlr','ulr', 'wspd', 'z_boom','ice_ablation','tilt_x','tilt_y']

# Identify where all columns (except the timestamp) are NaN
first_valid_index = merged[cols].dropna(how='all').index[0]
# Filter the dataframe from that point onwards
merged_slim = merged[merged.index >= first_valid_index]
# Identify where all columns (except the timestamp) are NaN at the tail
last_valid_index = merged[cols].dropna(how='all').index[-1]
# Filter the dataframe to end at that point
merged_slim = merged_slim[merged_slim.index <= last_valid_index]

ax = merged_slim[cols].plot(subplots = True, figsize = (6.5,5))

ax[0].set_ylabel('$\circ$C')
ax[1].set_ylabel('%')
ax[2].set_ylabel('hPa')
ax[3].set_ylabel('W/m')
ax[4].set_ylabel('W/m')
ax[5].set_ylabel('W/m')
ax[6].set_ylabel('W/m')
ax[7].set_ylabel('m/s')
ax[8].set_ylabel('m')
ax[9].set_ylabel('m')

ax[0].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[1].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[2].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[3].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[4].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[5].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[6].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[7].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[8].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[9].legend(loc='upper left', bbox_to_anchor=(1, 1))

#plt.tight_layout()
plt.savefig('ZAC_L_Measurement_success_rate.png', dpi = 300, bbox_inches='tight')
merged_slim.to_csv('/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/ZAC_L_preQC.csv', index = True)
#+END_SRC

#+RESULTS:
: None


#+BEGIN_SRC python
import pandas as pd
import xarray as xr
import numpy as np

workingfolder = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/preQC/'

#zac_u
site = 'zac_u'
temp = pd.read_csv(workingfolder + site + '_hour_temperature.csv', index_col = 0)
rh = pd.read_csv(workingfolder + site + '_hour_relative_humidity.csv', index_col = 0)
p = pd.read_csv(workingfolder + site + '_hour_pressure.csv', index_col = 0)
rad = pd.read_csv(workingfolder + site + '_hour_radiation.csv', index_col = 0)
tilt = pd.read_csv(workingfolder + site + '_hour_tilt.csv', index_col = 0)
ws = pd.read_csv(workingfolder + site + '_hour_wind_speed.csv', index_col = 0)
height = pd.read_csv(workingfolder + site + '_hour_boom_height.csv', index_col = 0)
#surf = pd.read_csv(workingfolder + site + '_hour_lowering.csv', index_col = 0)
ablation = pd.read_csv(workingfolder + site + '_hour_ice_ablation.csv', index_col = 0)

merged = temp.join(rh, how='outer').join(p, how='outer').join(rad, how='outer').join(tilt, how='outer').join(ws, how='outer').join(height, how='outer').join(ablation, how='outer')

merged.rename(columns = {'t_1':'t_u', 'rh': 'rh_u_corr', 'p': 'p_u', 'dsr':'dsr', 'usr':'usr','dsr_corr':'dsr_corr', 'usr_corr':'usr_corr', 'dlr':'dlr', 'ulr':'ulr','cloud_cov':'cloud_cover','t_surf':'t_surf', 'wspd':'wspd', 'z_boom':'z_boom', 'ice_ablation':'ice_ablation' }, inplace = True)
#merged.plot(subplots = True, figsize = (12,12))

cols = ['t_u', 'rh_u_corr', 'p_u', 'dsr_corr', 'usr_corr', 'dlr','ulr', 'wspd','tilt_x','tilt_y', 'z_boom','ice_ablation']

# Identify where all columns (except the timestamp) are NaN
first_valid_index = merged[cols].dropna(how='all').index[0]
# Filter the dataframe from that point onwards
merged_slim = merged[merged.index >= first_valid_index]
# Identify where all columns (except the timestamp) are NaN at the tail
last_valid_index = merged[cols].dropna(how='all').index[-1]
# Filter the dataframe to end at that point
merged_slim = merged_slim[merged_slim.index <= last_valid_index]

ax = merged_slim[cols].plot(subplots = True, figsize = (6.5,5))

#merged_slim.index = pd.to_datetime(merged_slim.index)
#years = range(merged_slim.index.year.min(), merged_slim.index.year.max() + 1)

#ax[0].set_xticks([pd.to_datetime(str(year) + '-01-01') for year in years])

#ax[0].set_xticklabels(years)


ax[0].set_ylabel('$\circ$C')
ax[1].set_ylabel('%')
ax[2].set_ylabel('hPa')
ax[3].set_ylabel('W/m')
ax[4].set_ylabel('W/m')
ax[5].set_ylabel('W/m')
ax[6].set_ylabel('W/m')
ax[7].set_ylabel('m/s')
ax[8].set_ylabel('m')
ax[9].set_ylabel('m')

ax[0].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[1].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[2].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[3].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[4].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[5].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[6].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[7].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[8].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[9].legend(loc='upper left', bbox_to_anchor=(1, 1))


plt.savefig('ZAC_U_Measurement_success_rate.png', dpi = 300, bbox_inches='tight')

#merged.plot(subplots = True, figsize = (12,12))
merged_slim.to_csv('/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/ZAC_U_preQC.csv', index = True)
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
workingfolder = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/preQC/'
#zac_a
site = 'zac_a'
temp = pd.read_csv(workingfolder + site + '_hour_temperature.csv', index_col = 0)
rh = pd.read_csv(workingfolder + site + '_hour_relative_humidity.csv', index_col = 0)
p = pd.read_csv(workingfolder + site + '_hour_pressure.csv', index_col = 0)
rad = pd.read_csv(workingfolder + site + '_hour_radiation.csv', index_col = 0)
tilt = pd.read_csv(workingfolder + site + '_hour_tilt.csv', index_col = 0)
ws = pd.read_csv(workingfolder + site + '_hour_wind_speed.csv', index_col = 0)
height = pd.read_csv(workingfolder + site + '_hour_boom_height.csv', index_col = 0)


merged = temp.join(rh, how='outer').join(p, how='outer').join(rad, how='outer').join(tilt, how='outer').join(ws, how='outer').join(height, how='outer')

merged.rename(columns = {'t_1':'t_u', 'rh': 'rh_u_corr', 'p': 'p_u', 'dsr':'dsr', 'usr':'usr','dsr_corr':'dsr_corr', 'usr_corr':'usr_corr', 'dlr':'dlr', 'ulr':'ulr','cloud_cov':'cloud_cover','t_surf':'t_surf', 'wspd':'wspd', 'z_boom':'z_boom' }, inplace = True)
#merged.plot(subplots = True, figsize = (12,12))

cols = ['t_u', 'rh_u_corr', 'p_u', 'dsr_corr', 'usr_corr', 'dlr','ulr', 'wspd', 'z_boom','tilt_x','tilt_y']

# Identify where all columns (except the timestamp) are NaN
first_valid_index = merged[cols].dropna(how='all').index[0]
# Filter the dataframe from that point onwards
merged_slim = merged[merged.index >= first_valid_index]
# Identify where all columns (except the timestamp) are NaN at the tail
last_valid_index = merged[cols].dropna(how='all').index[-1]
# Filter the dataframe to end at that point
merged_slim = merged_slim[merged_slim.index <= last_valid_index]

#merged_slim = drop_nan_rows(merged)

ax = merged_slim[cols].plot(subplots = True, figsize = (6.5,5))



ax[0].set_ylabel('$\circ$C')
ax[1].set_ylabel('%')
ax[2].set_ylabel('hPa')
ax[3].set_ylabel('W/m')
ax[4].set_ylabel('W/m')
ax[5].set_ylabel('W/m')
ax[6].set_ylabel('W/m')
ax[7].set_ylabel('m/s')
ax[8].set_ylabel('m')
#ax[9].set_ylabel('m')

ax[0].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[1].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[2].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[3].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[4].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[5].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[6].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[7].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[8].legend(loc='upper left', bbox_to_anchor=(1, 1))
#ax[9].legend(loc='upper left', bbox_to_anchor=(1, 1))


plt.savefig('ZAC_A_Measurement_success_rate.png', dpi = 300, bbox_inches='tight')


#merged.plot(subplots = True, figsize = (12,12))
merged_slim.to_csv('/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/ZAC_A_preQC.csv', index = True)


#+END_SRC

#+RESULTS:
: None



** Post QC


#+BEGIN_SRC python
import pandas as pd
import xarray as xr
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rcParams
import matplotlib.dates as mdates

rcParams.update({'font.size': 8})
workingfolder = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

#zac_l
site = 'zac_l'
temp = pd.read_csv(workingfolder + site + '_hour_temperature.csv', index_col = 0)
rh = pd.read_csv(workingfolder + site + '_hour_relative_humidity.csv', index_col = 0)
p = pd.read_csv(workingfolder + site + '_hour_pressure.csv', index_col = 0)
rad = pd.read_csv(workingfolder + site + '_hour_radiation.csv', index_col = 0)
tilt = pd.read_csv(workingfolder + site + '_hour_tilt.csv', index_col = 0)
ws = pd.read_csv(workingfolder + site + '_hour_wind_speed.csv', index_col = 0)
height = pd.read_csv(workingfolder + site + '_hour_boom_height.csv', index_col = 0)
#surf = pd.read_csv(workingfolder + site + '_hour_lowering.csv', index_col = 0)
ablation = pd.read_csv(workingfolder + site + '_hour_ice_ablation.csv', index_col = 0)

merged = temp.join(rh, how='outer').join(p, how='outer').join(rad, how='outer').join(tilt, how='outer').join(ws, how='outer').join(height, how='outer').join(ablation, how='outer')

merged.rename(columns = {'t_1':'t_u', 'rh': 'rh_u_corr', 'p': 'p_u', 'dsr':'dsr', 'usr':'usr','dsr_corr':'dsr_corr', 'usr_corr':'usr_corr', 'dlr':'dlr', 'ulr':'ulr','cloud_cov':'cloud_cover','t_surf':'t_surf', 'wspd':'wspd', 'z_boom':'z_boom', 'ice_ablation':'ice_ablation' }, inplace = True)
#merged.plot(subplots = True, figsize = (12,12))

cols = ['t_u', 'rh_u_corr', 'p_u', 'dsr_corr', 'usr_corr', 'dlr','ulr', 'wspd', 'z_boom','ice_ablation','tilt_x','tilt_y']

# Identify where all columns (except the timestamp) are NaN
first_valid_index = merged[cols].dropna(how='all').index[0]
# Filter the dataframe from that point onwards
merged_slim = merged[merged.index >= first_valid_index]
# Identify where all columns (except the timestamp) are NaN at the tail
last_valid_index = merged[cols].dropna(how='all').index[-1]
# Filter the dataframe to end at that point
merged_slim = merged_slim[merged_slim.index <= last_valid_index]

ax = merged[cols].plot(subplots = True, figsize = (6.5,5))

merged.index = pd.to_datetime(merged.index)
years = range(merged.index.year.min(), merged.index.year.max() + 1)

#ax[0].set_xticks([pd.to_datetime(str(year) + '-01-01') for year in years])

#ax[0].set_xticklabels(years)


ax[0].set_ylabel('$\circ$C')
ax[1].set_ylabel('%')
ax[2].set_ylabel('hPa')
ax[3].set_ylabel('W/m')
ax[4].set_ylabel('W/m')
ax[5].set_ylabel('W/m')
ax[6].set_ylabel('W/m')
ax[7].set_ylabel('m/s')
ax[8].set_ylabel('m')
ax[9].set_ylabel('m')

ax[0].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[1].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[2].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[3].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[4].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[5].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[6].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[7].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[8].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[9].legend(loc='upper left', bbox_to_anchor=(1, 1))

#plt.tight_layout()
plt.savefig('ZAC_L_Measurement_success_rate.png', dpi = 300, bbox_inches='tight')
#merged_slim.to_csv('/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'+site+'_2008_2022_QC_final.csv', index = True)
merged_slim.to_csv('/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/ZAC_L.csv', index = True)
#+END_SRC

#+RESULTS:
: None


#+BEGIN_SRC python
import pandas as pd
import xarray as xr
import numpy as np

workingfolder = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

#zac_u
site = 'zac_u'
temp = pd.read_csv(workingfolder + site + '_hour_temperature.csv', index_col = 0)
rh = pd.read_csv(workingfolder + site + '_hour_relative_humidity.csv', index_col = 0)
p = pd.read_csv(workingfolder + site + '_hour_pressure.csv', index_col = 0)
rad = pd.read_csv(workingfolder + site + '_hour_radiation.csv', index_col = 0)
tilt = pd.read_csv(workingfolder + site + '_hour_tilt.csv', index_col = 0)
ws = pd.read_csv(workingfolder + site + '_hour_wind_speed.csv', index_col = 0)
height = pd.read_csv(workingfolder + site + '_hour_boom_height.csv', index_col = 0)
#surf = pd.read_csv(workingfolder + site + '_hour_lowering.csv', index_col = 0)
ablation = pd.read_csv(workingfolder + site + '_hour_ice_ablation.csv', index_col = 0)

merged = temp.join(rh, how='outer').join(p, how='outer').join(rad, how='outer').join(tilt, how='outer').join(ws, how='outer').join(height, how='outer').join(ablation, how='outer')

merged.rename(columns = {'t_1':'t_u', 'rh': 'rh_u_corr', 'p': 'p_u', 'dsr':'dsr', 'usr':'usr','dsr_corr':'dsr_corr', 'usr_corr':'usr_corr', 'dlr':'dlr', 'ulr':'ulr','cloud_cov':'cloud_cover','t_surf':'t_surf', 'wspd':'wspd', 'z_boom':'z_boom', 'ice_ablation':'ice_ablation' }, inplace = True)
#merged.plot(subplots = True, figsize = (12,12))

cols = ['t_u', 'rh_u_corr', 'p_u', 'dsr_corr', 'usr_corr', 'dlr','ulr', 'wspd', 'z_boom','ice_ablation','tilt_x','tilt_y']

# Identify where all columns (except the timestamp) are NaN
first_valid_index = merged[cols].dropna(how='all').index[0]
# Filter the dataframe from that point onwards
merged_slim = merged[merged.index >= first_valid_index]
# Identify where all columns (except the timestamp) are NaN at the tail
last_valid_index = merged[cols].dropna(how='all').index[-1]
# Filter the dataframe to end at that point
merged_slim = merged_slim[merged_slim.index <= last_valid_index]

ax = merged[cols].plot(subplots = True, figsize = (6.5,5))

merged.index = pd.to_datetime(merged.index)
years = range(merged.index.year.min(), merged.index.year.max() + 1)

#ax[0].set_xticks([pd.to_datetime(str(year) + '-01-01') for year in years])

#ax[0].set_xticklabels(years)


ax[0].set_ylabel('$\circ$C')
ax[1].set_ylabel('%')
ax[2].set_ylabel('hPa')
ax[3].set_ylabel('W/m')
ax[4].set_ylabel('W/m')
ax[5].set_ylabel('W/m')
ax[6].set_ylabel('W/m')
ax[7].set_ylabel('m/s')
ax[8].set_ylabel('m')
ax[9].set_ylabel('m')

ax[0].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[1].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[2].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[3].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[4].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[5].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[6].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[7].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[8].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[9].legend(loc='upper left', bbox_to_anchor=(1, 1))


plt.savefig('ZAC_U_Measurement_success_rate.png', dpi = 300, bbox_inches='tight')

#merged.plot(subplots = True, figsize = (12,12))
#merged_slim.to_csv('/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'+site+'_2008_2022_QC_final.csv', index = True)
merged_slim.to_csv('/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/ZAC_U.csv', index = True)
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
import pandas as pd
import matplotlib.pyplot as plt
workingfolder = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
#zac_a
site = 'zac_a'
temp = pd.read_csv(workingfolder + site + '_hour_temperature.csv', index_col = 0, parse_dates = True)
rh = pd.read_csv(workingfolder + site + '_hour_relative_humidity.csv', index_col = 0, parse_dates = True)
p = pd.read_csv(workingfolder + site + '_hour_pressure.csv', index_col = 0, parse_dates = True)
rad = pd.read_csv(workingfolder + site + '_hour_radiation.csv', index_col = 0, parse_dates = True)
tilt = pd.read_csv(workingfolder + site + '_hour_tilt.csv', index_col = 0, parse_dates = True)
ws = pd.read_csv(workingfolder + site + '_hour_wind_speed.csv', index_col = 0, parse_dates = True)
height = pd.read_csv(workingfolder + site + '_hour_boom_height.csv', index_col = 0, parse_dates = True)


merged = temp.join(rh, how='outer').join(p, how='outer').join(rad, how='outer').join(tilt, how='outer').join(ws, how='outer').join(height, how='outer')

merged.rename(columns = {'t_1':'t_u', 'rh': 'rh_u_corr', 'p': 'p_u', 'dsr':'dsr', 'usr':'usr','dsr_corr':'dsr_corr', 'usr_corr':'usr_corr', 'dlr':'dlr', 'ulr':'ulr','cloud_cov':'cloud_cover','t_surf':'t_surf', 'wspd':'wspd', 'z_boom':'z_boom' }, inplace = True)
#merged.plot(subplots = True, figsize = (12,12))

cols = ['t_u', 'rh_u_corr', 'p_u', 'dsr_corr', 'usr_corr', 'dlr','ulr', 'wspd', 'z_boom','tilt_x','tilt_y']

# Identify where all columns (except the timestamp) are NaN
first_valid_index = merged[cols].dropna(how='all').index[0]
# Filter the dataframe from that point onwards
merged_slim = merged[merged.index >= first_valid_index]
# Identify where all columns (except the timestamp) are NaN at the tail
last_valid_index = merged[cols].dropna(how='all').index[-1]
# Filter the dataframe to end at that point
merged_slim = merged_slim[merged_slim.index <= last_valid_index]

ax = merged_slim[cols].plot(subplots = True, figsize = (6.5,5))

ax[0].set_ylabel('$\circ$C')
ax[1].set_ylabel('%')
ax[2].set_ylabel('hPa')
ax[3].set_ylabel('W/m')
ax[4].set_ylabel('W/m')
ax[5].set_ylabel('W/m')
ax[6].set_ylabel('W/m')
ax[7].set_ylabel('m/s')
ax[8].set_ylabel('m')
ax[8].set_ylabel('degree')
ax[8].set_ylabel('degree')
#ax[9].set_ylabel('m')

ax[0].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[1].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[2].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[3].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[4].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[5].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[6].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[7].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[8].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[9].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[10].legend(loc='upper left', bbox_to_anchor=(1, 1))
#ax[9].legend(loc='upper left', bbox_to_anchor=(1, 1))


plt.savefig('ZAC_A_Measurement_success_rate.png', dpi = 300, bbox_inches='tight')


#merged.plot(subplots = True, figsize = (12,12))
#merged_slim.to_csv('/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'+site+'_2008_2022_QC_final', index = True)
merged_slim.to_csv('/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/ZAC_A.csv', index = True)

#+END_SRC

#+RESULTS:
: None


** Add nead headers


Write out the headers from the csv file
#+BEGIN_SRC python :session :results output
import pandas as pd
import nead
import matplotlib.pyplot as plt
site = 'zac_l'
df = pd.read_csv('/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/ZAC_L.csv')
print(df.keys())
#+END_SRC

#+RESULTS:
: Index(['time', 't_u', 'rh_u_corr', 'p_u', 'dsr', 'usr', 'dsr_corr', 'usr_corr',
:        'dlr', 'ulr', 'albedo', 'cloud_cover', 't_surf', 'I', 'tilt_x',
:        'tilt_y', 'wspd', 'z_boom', 'ice_ablation'],
:       dtype='object')


Manually create NEAD header files and add them here:

#+BEGIN_SRC sh :session
filename=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/ZAC_L
filestorage=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/
neadfile=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/zac_l_nead_header.csv

cp $filename.csv ${filename}_tmp.csv
sed -i '1d' ${filename}_tmp.csv
cat $neadfile ${filename}_tmp.csv > ${filestorage}ZAC_L_nead.csv
rm ${filename}_tmp.csv
#+END_SRC

#+RESULTS:

#+BEGIN_SRC sh :session
filename=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/ZAC_U
filestorage=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/
neadfile=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/zac_u_nead_header.csv

cp $filename.csv ${filename}_tmp.csv
sed -i '1d' ${filename}_tmp.csv
cat $neadfile ${filename}_tmp.csv > ${filestorage}ZAC_U_nead.csv
rm ${filename}_tmp.csv

#+END_SRC

#+RESULTS:

#+BEGIN_SRC sh :session
filename=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/ZAC_A
filestorage=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/
neadfile=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/zac_a_nead_header.csv

cp $filename.csv ${filename}_tmp.csv
sed -i '1d' ${filename}_tmp.csv
cat $neadfile ${filename}_tmp.csv > ${filestorage}ZAC_A_nead.csv
rm ${filename}_tmp.csv




#+END_SRC

#+RESULTS:

Example for dataverse - using nead
#+BEGIN_SRC python 
import nead
import matplotlib.pyplot as plt

filename = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/ZAC_U_nead.csv'
ds = nead.read(filename, index_col=0)
# Ignoring all metadata and plotting
df = ds.to_pandas()
df.plot(subplots = True)
plt.show()
#+END_SRC

#+RESULTS:
: None


Examples for dataverse - using pandas

#+BEGIN_SRC python 
import pandas as pd
import matplotlib.pyplot as plt

filename = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/ZAC_L.csv'
df = pd.read_csv(filename, index_col = 0, parse_dates = True)
df.plot(subplots=True)
plt.show()
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python

filename = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/ZAC_U.csv'
df = pd.read_csv(filename, index_col = 0, parse_dates = True)
df.plot(subplots=True)
plt.show()

filename = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/ZAC_A.csv'
df = pd.read_csv(filename, index_col = 0, parse_dates = True)
df.plot(subplots=True)
plt.show()

#+END_SRC

#+RESULTS:
: None



