#+TITLE: AWS processing
#+AUTHOR: Signe Hillerup Larsen
#+EMAIL: shl@geus.dk
#+DATE: {{{time(%Y-%m-%d)}}}
#+DESCRIPTION: Getting the GlacioBasis raw data into usefull formats
#+KEYWORDS:
#+OPTIONS:   H:4 num:4 toc:nil \n:nil ::t |:t ^:{} -:t f:t *:t <:t
#+EXCLUDE_TAGS: noexport
#+ARCHIVE: ::* Archive

#+PROPERTY: header-args :session *aws_processing_v1.0-shell* :noweb yes :eval yes



* Code to process all downloaded data

** Freya

#+BEGIN_SRC ipython
<<load_libraries>>

#define working folder and load in dataframe
raw = '/home/shl/OneDrive/projects/glaciobasis/fieldwork/zac_2022/data/'
filename = 'freya_TableMem.dat'
df = pd.read_csv(raw + filename, header=1,skiprows=[2,3],sep=',',engine='python')

# Set index
df.index = pd.to_datetime(df.TIMESTAMP)
df = df.drop(['TIMESTAMP'], axis=1)
df.index.name = 'time'
df.sort_index(inplace=True)
df = df.replace('NAN',np.NaN)

# Remove INF
for column in df.keys():
    df.loc[df[column]==np.inf] = np.nan

# Correct pressure unit
mmHg2hPa = 1.33322368
df['pressure'] =df.AirPressure_Avg.astype(float)*mmHg2hPa

# Ajust pressure transducer according to density of the liquid
pt_z_coef = 0.28013
rho_af = 1092
#rho_af = 1145
pt_z_p_coef = 1012   # Air pressure at which the pt was calibrated [hPa]
pt_z_factor = 2.5   # Unitless. Scale for logger voltage measurement range (2.5 for CR1000, 1 for CR10X)

hose_cor = df['IceHeight_Avg'] * pt_z_coef * pt_z_factor * 998.0 / rho_af + (pt_z_p_coef - df['pressure']) / (rho_af * 9.81)

#Plotting
hose_cor = hose_cor -10.4
sr50 = sr50 - sr50.values[0]

ax = hose_cor.plot()
sr50.plot(ax = ax)
ax.set_ylim(0,2)

#+END_SRC

#+RESULTS:
:results:
# Out [47]: 
# text/plain
: (0.0, 2.0)

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/63114eb169eac10176f0dce4d8f1d75f4f4e7e40.png]]
:end:


  
** Create L0 files: extract raw data and unify headers and correct special cases
Reading in raw files and writing them out as L0 files with nice headers and special case issues solved
*** Workflow for generating all L0 files

#+BEGIN_SRC ipython
<<run_all_L0_zac_l>>
<<run_all_L0_zac_u>>
<<run_all_L0_zac_a>>

#+END_SRC

*** Libraries
#+NAME: load_libraries
#+BEGIN_SRC ipython
from glob import glob
import pandas as pd
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
import os as os
#+END_SRC


*** Constants

#+NAME: define_constants
#+BEGIN_SRC ipython
mmHg2hPa = 1.33322368
#+END_SRC

*** working folders

#+NAME: define_working_folders
#+BEGIN_SRC ipython
raw = '/home/shl/OneDrive/projects/glaciobasis/data/aws_raw/'
destination = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0/'
#+END_SRC


*** Define which headers to pass forward
#+NAME: headers_to_pass_forward
#+BEGIN_SRC ipython
new_headers = ['rec', 'p', 't_1', 't_2', 'rh', 'wspd', 'wdir', 'wd_std', 'dsr', 'usr', 'dlr', 'ulr', 't_rad', 'z_boom', 'z_boom_q', 'z_stake', 'z_stake_q', 'z_pt', 't_i_1', 't_i_2', 't_i_3', 't_i_4', 't_i_5', 't_i_6', 't_i_7', 't_i_8', 'tilt_x', 'tilt_y', 'gps_time', 'gps_lat', 'gps_lon', 'gps_alt', 'gps_geoid', 'gps_q', 'gps_numsat', 'gps_hdop', 't_log', 'fan_dc']


#+END_SRC


*** Each station, year by year

**** Take a look at all the table mem files and find the long one

#+BEGIN_SRC ipython
<<load_libraries>>
station_id = 'zac_l'
ystart = '2017'
yend = '2018'

filelist = glob(raw + station_id + '/' + ystart + '-' + yend +'/*TableMem*')
for f in filelist:
    df = pd.read_csv(f, header=1,skiprows=[2,3],sep=',',engine='python')
    df.index = pd.to_datetime(df.TIMESTAMP)
    df = df.drop(['TIMESTAMP'], axis=1)
    df.index.name = 'time'
    df.sort_index(inplace=True)
    df = df.replace('NAN',np.NaN)
    print(f)
    print(df['AS_T_Avg'])
    #df['AS_T_Avg'].plot()
    
#+END_SRC

#+RESULTS:
:results:
# Out [83]: 
# output
/home/shl/OneDrive/projects/glaciobasis/data/aws_raw/zac_l/2017-2018/TOA5_13745.TableMem.dat
time
2017-04-24 16:30:00    -2.714936
2017-04-24 16:40:00    -13.24721
2017-04-24 16:50:00    -13.14341
2017-04-24 17:00:00      -13.461
2017-04-24 17:10:00    -13.51251
                         ...    
2018-04-14 14:00:00    -3.442842
2018-04-14 14:10:00    -2.958845
2018-04-14 14:20:00    -2.451514
2018-04-14 14:30:00    -2.114108
2018-04-14 14:40:00    -2.296419
Name: AS_T_Avg, Length: 35461, dtype: object

:end:


**** zac_l

#+NAME: run_all_L0_zac_l
#+BEGIN_SRC ipython
<<zac_l_2008_2010>>
<<zac_l_2010_2011>>
<<zac_l_2011_2014>>
<<zac_l_2012_2013>>
<<zac_l_2014_2015>>
<<zac_l_2015_2016>>
<<zac_l_2016_2017>>
<<zac_l_2017_2018>>
<<zac_l_2018_2020>>
<<zac_l_2020_2021>>
<<zac_l_2021_2022>>
#+END_SRC

#+RESULTS: run_all_L0_zac_l
:results:
# Out [1]: 
# output
Index(['rec', 'p', 't_1', 't_2', 'rh', 'wspd', 'wdir', 'wd_std', 'dsr', 'usr',
       'dlr', 'ulr', 't_rad', 'z_boom', 'z_boom_q', 'z_stake', 'z_stake_q',
       'z_pt', 't_i_1', 't_i_2', 't_i_3', 't_i_4', 't_i_5', 't_i_6', 't_i_7',
       't_i_8', 'tilt_x', 'tilt_y', 'gps_time', 'gps_lat', 'gps_lon',
       'gps_alt', 'gps_geoid', 'gps_q', 'gps_numsat', 'gps_hdop', 't_log',
       'fan_dc'],
      dtype='object')
Index(['rec', 'p', 't_1', 't_2', 'rh', 'wspd', 'wdir', 'wd_std', 'dsr', 'usr',
       'dlr', 'ulr', 't_rad', 'z_boom', 'z_boom_q', 'z_stake', 'z_stake_q',
       'z_pt', 't_i_1', 't_i_2', 't_i_3', 't_i_4', 't_i_5', 't_i_6', 't_i_7',
       't_i_8', 'tilt_x', 'tilt_y', 'gps_time', 'gps_lat', 'gps_lon',
       'gps_alt', 'gps_geoid', 'gps_q', 'gps_numsat', 'gps_hdop', 't_log',
       'fan_dc'],
      dtype='object')

:end:

***** zac_l_2008_2010


#+NAME: zac_l_2008_2010
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2008'
yend = '2010'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'ZACK M_TableMem.dat.backup'
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<read_named_TableMem_file>>
<<fix_headers_type0_SR50_switched>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)



df['p'] = df['p']*mmHg2hPa
# Radiation from mV to 10^-5 V
#variables = ['dsr','usr','dlr','ulr']
#df[variables] = df[variables].astype(float)
#sel = df.index > '2010-May-12 20:50:00' 
#df.loc[sel,variables] = df.loc[sel,variables]/100

df.to_csv(outfilename,sep=',', index=True)
#+END_SRC


#+RESULTS: zac_l_2008_2010
:results:
# Out [69]: 
:end:

Read in and concatenate the raw files

***** zac_l_2010_2011

#+NAME: zac_l_2010_2011
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2010'
yend = '2011'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13745.TableMem.dat'
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type0_SR50_switched>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)



df['p'] = df['p']*mmHg2hPa
# Radiation from mV to 10^-5 V
variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100

df.to_csv(outfilename,sep=',', index=True)
#+END_SRC

#+RESULTS: zac_l_2010_2011
:results:
# Out [72]: 
:end:

***** zac_l_2011_2014

#+NAME: zac_l_2011_2014
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2011'
yend = '2014'

yend1 = '2012'
ystart2 = '2013'

outfilename1 = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend1 + '.csv'
outfilename2 = destination+station_id + '/' + station_id + '-' + ystart2 +'-' + yend + '.csv'
filename = 'ZACK M_TableMem.dat.7.backup'
<<read_named_TableMem_file>>

#<<read_and_concat_all_TableMem_files_in_raw_folder>>
df_orig = df

df=df_orig[:'17-April-2012'].copy()
<<fix_headers_type0_SR50_switched>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

df['p'] = df['p']*mmHg2hPa
df.to_csv(outfilename1,sep=',', index=True)


df=df_orig['1-May-2013':].copy()
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)


df['p'] = df['p']*mmHg2hPa
df.to_csv(outfilename2,sep=',', index=True)


#+END_SRC

#+RESULTS: zac_l_2011_2014
:results:
# Out [74]: 
:end:

***** zac_l_2012_2013

#+NAME: zac_l_2012_2013
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2012'
yend = '2013'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13745.TableMem.dat'
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type0_SR50_switched>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)


df['p'] = df['p']*mmHg2hPa

df.to_csv(outfilename,sep=',', index=True)
#+END_SRC

#+RESULTS: zac_l_2012_2013
:results:
# Out [76]: 
:end:

***** zac_l_2014_2015

#+NAME: zac_l_2014_2015
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2014'
yend = '2015'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'ZACK M_TableMem.dat.14.backup'
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

df['p'] = df['p']*mmHg2hPa

df.to_csv(outfilename,sep=',', index=True)
#+END_SRC

#+RESULTS: zac_l_2014_2015
:results:
# Out [78]: 
:end:

***** zac_l_2015_2016

#+NAME: zac_l_2015_2016
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2015'
yend = '2016'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13745.TableMem.dat'
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

df['p'] = df['p']*mmHg2hPa

df.to_csv(outfilename,sep=',', index=True)
#+END_SRC

#+RESULTS: zac_l_2015_2016
:results:
# Out [80]: 
:end:

***** zac_l_2016_2017

#+NAME: zac_l_2016_2017
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2016'
yend = '2017'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13745.TableMem.dat'
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

df['p'] = df['p']*mmHg2hPa

df.to_csv(outfilename,sep=',', index=True)
#+END_SRC

#+RESULTS: zac_l_2016_2017
:results:
# Out [82]: 
:end:

***** zac_l_2017_2018

#+NAME: zac_l_2017_2018
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2017'
yend = '2018'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13745.TableMem.dat'
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

df['p'] = df['p']*mmHg2hPa

df.to_csv(outfilename,sep=',', index=True)
#+END_SRC

#+RESULTS: zac_l_2017_2018
:results:
# Out [84]: 
:end:

***** zac_l_2018_2020

#+NAME: zac_l_2018_2020
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2018'
yend = '2020'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13745.TableMem.dat'                                                               
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df['p'] = df['p']*mmHg2hPa


df.to_csv(outfilename,sep=',', index=True)
#+END_SRC


***** zac_l_2020_2021

#+NAME: zac_l_2020_2021
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2020'
yend = '2021'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'CR1000_nn_TableMem.dat.backup'
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<read_named_TableMem_file>>
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

df['p'] = df['p']*mmHg2hPa

print(df.keys())
df.to_csv(outfilename,sep=',', index=True)
#+END_SRC

#+RESULTS: zac_l_2020_2021
:results:
# Out [1]: 
# output
---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
/tmp/ipykernel_9289/2143890737.py in <module>
     28 #
     29 #
---> 30 df = pd.read_csv(raw + station_id + '/' + ystart + '-' + yend +'/'+filename, header=1,skiprows=[2,3],sep=',',engine='python')
     31 df.index = pd.to_datetime(df.TIMESTAMP)
     32 df = df.drop(['TIMESTAMP'], axis=1)

~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/io/parsers.py in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)
    684     )
    685 
--> 686     return _read(filepath_or_buffer, kwds)
    687 
    688 

~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds)
    450 
    451     # Create the parser.
--> 452     parser = TextFileReader(fp_or_buf, **kwds)
    453 
    454     if chunksize or iterator:

~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds)
    934             self.options["has_index_names"] = kwds["has_index_names"]
    935 
--> 936         self._make_engine(self.engine)
    937 
    938     def close(self):

~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/io/parsers.py in _make_engine(self, engine)
   1177                     'are "c", "python", or "python-fwf")'
   1178                 )
-> 1179             self._engine = klass(self.f, **self.options)
   1180 
   1181     def _failover_to_python(self):

~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/io/parsers.py in __init__(self, f, **kwds)
   2370         self._comment_lines = []
   2371 
-> 2372         f, handles = get_handle(
   2373             f,
   2374             "r",

~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/io/common.py in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)
    494         elif is_text:
    495             # No explicit encoding
--> 496             f = open(path_or_buf, mode, errors="replace", newline="")
    497         else:
    498             # Binary mode

FileNotFoundError: [Errno 2] No such file or directory: '/home/shl/OneDrive/projects/glaciobasis/data/aws_raw/zac_l/2020-2021/zac-l_TableMem.dat'
:end:


***** zac_l_2020_2021

#+NAME: zac_l_2021_2022
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2021'
yend = '2022'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'zac-l_TableMem.dat'
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<read_named_TableMem_file>>
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

df['p'] = df['p']*mmHg2hPa

print(df.keys())
df.to_csv(outfilename,sep=',', index=True)
#+END_SRC

#+RESULTS: zac_l_2021_2022
:results:
# Out [3]: 
# output
Index(['rec', 'p', 't_1', 't_2', 'rh', 'wspd', 'wdir', 'wd_std', 'dsr', 'usr',
       'dlr', 'ulr', 't_rad', 'z_boom', 'z_boom_q', 'z_stake', 'z_stake_q',
       'z_pt', 't_i_1', 't_i_2', 't_i_3', 't_i_4', 't_i_5', 't_i_6', 't_i_7',
       't_i_8', 'tilt_x', 'tilt_y', 'gps_time', 'gps_lat', 'gps_lon',
       'gps_alt', 'gps_geoid', 'gps_q', 'gps_numsat', 'gps_hdop', 't_log',
       'fan_dc'],
      dtype='object')

:end:


**** zac_u
#+NAME: run_all_L0_zac_u
#+BEGIN_SRC ipython
<<zac_u_2008_2010>>
<<zac_u_2010_2011>>
<<zac_u_2011_2012>>
<<zac_u_2012_2013>>
<<zac_u_2014_2015>>
<<zac_u_2015_2016>>
<<zac_u_2016_2017>>
<<zac_u_2017_2019>>
<<zac_u_2019_2020>>
<<zac_u_2020_2021>>
<<zac_u_2021_2022>>
#+END_SRC

#+RESULTS: run_all_L0_zac_u
:results:
# Out [2]: 
:end:

***** zac_u_2008_2010
#+NAME: zac_u_2008_2010
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2008'
yend = '2010'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'

<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type0>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2008_2010
:results:
# Out [153]: 
:end:

***** zac_u_2010_2011
#+NAME: zac_u_2010_2011
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2010'
yend = '2011'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13744.TableMem.dat'

<<read_named_TableMem_file>>
<<fix_headers_type0>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2010_2011
:results:
# Out [154]: 
:end:

***** zac_u_2011_2012
#+NAME: zac_u_2011_2012
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2011'
yend = '2012'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'ZACK S_TableMem.dat'

<<read_named_TableMem_file>>

df.index = df.index - pd.to_timedelta('1 day')

<<fix_headers_type0>>

df = df[new_headers] # pressure is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

df['p'] = df['p']*mmHg2hPa
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2011_2012
:results:
# Out [107]: 
:end:

***** zac_u_2012_2013
#+NAME: zac_u_2012_2013
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2012'
yend = '2013'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
df.index = df.index - pd.to_timedelta('1 day')
<<fix_headers_type1>>

df = df[new_headers] # radiation and tilt is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2012_2013
:results:
# Out [158]: 
:end:

***** zac_u_2013_2014
#+NAME: zac_u_2013_2014
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2013'
yend = '2014'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
df.index = df.index - pd.to_timedelta('1 day')

<<fix_headers_type1>>

df = df[new_headers] # radiation and tilt is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2013_2014
:results:
# Out [159]: 
:end:

***** zac_u_2014_2015
#+NAME: zac_u_2014_2015
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2014'
yend = '2015'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type1>>

df = df[new_headers] # radiation and tilt is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data


variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2014_2015
:results:
# Out [160]: 
:end:


***** zac_u_2015_2016
#+NAME: zac_u_2015_2016
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2015'
yend = '2016'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_E2101.TableMem.dat'
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<read_named_TableMem_file>>

<<fix_headers_type1>>

df = df[new_headers] # radiation and tilt is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2015_2016
:results:
# Out [1]: 
:end:

***** zac_u_2016_2017
#+NAME: zac_u_2016_2017
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2016'
yend = '2017'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type1>>

df = df[new_headers] # radiation and tilt is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2016_2017
:results:
# Out [163]: 
:end:


***** zac_u_2017_2019
#+NAME: zac_u_2017_2019
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2017'
yend = '2019'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type1>>

df = df[new_headers] # pressure transducer is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2017_2019
:results:
# Out [164]: 
:end:

***** zac_u_2019_2020
#+NAME: zac_u_2019_2020
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2019'
yend = '2020'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type1>>

df = df[new_headers] # thermistor is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2019_2020
:results:
# Out [165]: 
:end:

***** zac_u_2020_2021
#+NAME: zac_u_2020_2021
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2020'
yend = '2021'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_E2101.TableMem.dat'

<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type1>>

df = df[new_headers] # thermistor is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2020_2021
:results:
# Out [2]: 
:end:

***** zac_u_2021_2022
#+NAME: zac_u_2021_2022
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2021'
yend = '2022'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'zac-u_TableMem.dat'

<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type1>>

df = df[new_headers] # thermistor is added

new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2021_2022
:results:
# Out [93]: 
:end:



**** zac_a
#+NAME: run_all_L0_zac_a
#+BEGIN_SRC ipython
<<zac_a_2009_2010>>
<<zac_a_2010_2011>>
<<zac_a_2011_2012>>
<<zac_a_2012_2013>>
<<zac_a_2014_2015>>
<<zac_a_2015_2016>>
<<zac_a_2016_2017>>
<<zac_a_2017_2018>>
<<zac_a_2018_2019>>
#+END_SRC

#+RESULTS: run_all_L0_zac_a
:results:
# Out [3]: 
# output
Index(['rec', 'p', 't_1', 't_2', 'rh', 'wspd', 'wdir', 'wd_std', 'dsr', 'usr',
       'dlr', 'ulr', 't_rad', 'z_boom', 'z_boom_q', 'z_stake', 'z_stake_q',
       'z_pt', 't_i_1', 't_i_2', 't_i_3', 't_i_4', 't_i_5', 't_i_6', 't_i_7',
       't_i_8', 'tilt_x', 'tilt_y', 'gps_time', 'gps_lat', 'gps_lon',
       'gps_alt', 'gps_geoid', 'gps_q', 'gps_numsat', 'gps_hdop', 't_log',
       'fan_dc'],
      dtype='object')

:end:


***** zac_a_2009_2010
#+NAME: zac_a_2009_2010
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2009'
yend = '2010'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100


df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_a_2009_2010
:results:
# Out [13]: 
:end:

***** zac_a_2010_2011
#+NAME: zac_a_2010_2011
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2010'
yend = '2011'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)

df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100

df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_a_2010_2011
:results:
# Out [2]: 
:end:

***** zac_a_2011_2012
#+NAME: zac_a_2011_2012
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2011'
yend = '2012'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>
df.index = df.index - pd.to_timedelta('1 day')

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100

df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_a_2011_2012
:results:
# Out [3]: 
:end:


***** zac_a_2012_2013
#+NAME: zac_a_2012_2013
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2012'
yend = '2013'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100

df.to_csv(outfilename,sep=',', index=True)
#+END_SRC



***** zac_a_2013_2014
#+NAME: zac_a_2013_2014
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2013'
yend = '2014'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100

df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_a_2013_2014
:results:
# Out [5]: 
:end:


***** zac_a_2014_2015
#+NAME: zac_a_2014_2015
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2014'
yend = '2015'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100

df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_a_2014_2015
:results:
# Out [6]: 
:end:

***** zac_a_2015_2016
#+NAME: zac_a_2015_2016
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2015'
yend = '2016'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100

df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_a_2015_2016
:results:
# Out [7]: 
:end:
***** zac_a_2016_2017
#+NAME: zac_a_2016_2017
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2016'
yend = '2017'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100

df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_a_2016_2017
:results:
# Out [8]: 
:end:

***** zac_a_2017_2018
#+NAME: zac_a_2017_2018
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2017'
yend = '2018'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100

print(df.keys())
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_a_2017_2018
:results:
# Out [10]: 
# output
Index(['rec', 'p', 't_1', 't_2', 'rh', 'wspd', 'wdir', 'wd_std', 'dsr', 'usr',
       'dlr', 'ulr', 't_rad', 'z_boom', 'z_boom_q', 'z_stake', 'z_stake_q',
       'z_pt', 't_i_1', 't_i_2', 't_i_3', 't_i_4', 't_i_5', 't_i_6', 't_i_7',
       't_i_8', 'tilt_x', 'tilt_y', 'gps_time', 'gps_lat', 'gps_lon',
       'gps_alt', 'gps_geoid', 'gps_q', 'gps_numsat', 'gps_hdop', 't_log',
       'fan_dc'],
      dtype='object')

:end:



***** zac_a_2018_2019
#+NAME: zac_a_2018_2019
#+BEGIN_SRC ipython
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2018'
yend = '2019'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1a>>


df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100

df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_a_2018_2019
:results:
# Out [13]: 
:end:





**** Code
#+NAME: read_and_concat_all_TableMem_files_in_raw_folder
#+BEGIN_SRC ipython

filelist = glob(raw + station_id + '/' + ystart + '-' + yend +'/*TableMem*')
df = pd.concat((pd.read_csv(f, header=1,skiprows=[2,3],sep=',',engine='python') for f in filelist))
df.index = pd.to_datetime(df.TIMESTAMP)
df = df.drop(['TIMESTAMP'], axis=1)
df.index.name = 'time'
df.sort_index(inplace=True)
df = df.replace('NAN',np.NaN)


#+END_SRC

#+NAME: read_named_TableMem_file
#+BEGIN_SRC ipython
df = pd.read_csv(raw + station_id + '/' + ystart + '-' + yend +'/'+filename, header=1,skiprows=[2,3],sep=',',engine='python')
df.index = pd.to_datetime(df.TIMESTAMP)
df = df.drop(['TIMESTAMP'], axis=1)
df.index.name = 'time'
df.sort_index(inplace=True)
df = df.replace('NAN',np.NaN)
#+END_SRC


#+NAME: fix_headers_type0_SR50_switched
#+BEGIN_SRC ipython

df = df.rename(columns = {'TIMESTAMP':'time', 'RECORD':'rec', 'BP_mmHg_Avg':'p','AS_Pt100_Avg':'t_1', 'AS_T_Avg':'t_2', 'AS_RH_Avg':'rh', 'WS_ms_S_WVT':'wspd', 'WindDir_D1_WVT':'wdir', 'WindDir_SD1_WVT':'wd_std', 'CNR1_SWin_Avg':'dsr', 'CNR1_SWout_Avg':'usr', 'CNR1_LWin_Avg':'dlr', 'CNR1_LWout_Avg':'ulr','CNR1_Pt100_Avg':'t_rad', 'SnowHeight':'z_stake', 'SnowHeightQuality':'z_stake_q', 'Ablation':'z_boom', 'AblationQuality':'z_boom_q', 'Ablation_meter_Avg':'z_pt', 'Thermistor_1':'t_i_1', 'Thermistor_2':'t_i_2', 'Thermistor_3':'t_i_3', 'Thermistor_4':'t_i_4', 'Thermistor_5':'t_i_5', 'Thermistor_6':'t_i_6','Thermistor_7':'t_i_7', 'Thermistor_8':'t_i_8', 'Xtilt_Avg':'tilt_x', 'Ytilt_Avg':'tilt_y', 'TIME':'gps_time', 'LAT':'gps_lat', 'LONGI':'gps_lon', 'ALTDE':'gps_alt', 'GIODAL':'gps_geoid', 'QUAL':'gps_q', 'NUMSATS':'gps_numsat', 'HDP':'gps_hdop', 'PTemp_C_Avg':'t_log', 'Fan_current_avg':'fan_dc' })

#df = df.drop(columns = ['HEMINS','HEMIEW','ALTUNIT','GEOUNIT'])
#+END_SRC

#+NAME: fix_headers_type0
#+BEGIN_SRC ipython

df = df.rename(columns = {'TIMESTAMP':'time', 'RECORD':'rec', 'BP_mmHg_Avg':'p','AS_Pt100_Avg':'t_1', 'AS_T_Avg':'t_2', 'AS_RH_Avg':'rh', 'WS_ms_S_WVT':'wspd', 'WindDir_D1_WVT':'wdir', 'WindDir_SD1_WVT':'wd_std', 'CNR1_SWin_Avg':'dsr', 'CNR1_SWout_Avg':'usr', 'CNR1_LWin_Avg':'dlr', 'CNR1_LWout_Avg':'ulr','CNR1_Pt100_Avg':'t_rad', 'SnowHeight':'z_boom', 'SnowHeightQuality':'z_boom_q', 'Ablation':'z_stake', 'AblationQuality':'z_stake_q', 'Ablation_meter_Avg':'z_pt', 'Thermistor_1':'t_i_1', 'Thermistor_2':'t_i_2', 'Thermistor_3':'t_i_3', 'Thermistor_4':'t_i_4', 'Thermistor_5':'t_i_5', 'Thermistor_6':'t_i_6','Thermistor_7':'t_i_7', 'Thermistor_8':'t_i_8', 'Xtilt_Avg':'tilt_x', 'Ytilt_Avg':'tilt_y', 'TIME':'gps_time', 'LAT':'gps_lat', 'LONGI':'gps_lon', 'ALTDE':'gps_alt', 'GIODAL':'gps_geoid', 'QUAL':'gps_q', 'NUMSATS':'gps_numsat', 'HDP':'gps_hdop', 'PTemp_C_Avg':'t_log', 'Fan_current_avg':'fan_dc' })

#df = df.drop(columns = ['HEMINS','HEMIEW','ALTUNIT','GEOUNIT'])
#+END_SRC

#+NAME: fix_headers_type1
#+BEGIN_SRC ipython
df = df.rename(columns = {'RECORD':'rec', 'AirPressure_Avg':'p','Temperature_Avg':'t_1', 'Temperature2_Avg':'t_2', 'RelativeHumidity_Avg':'rh', 'WindSpeed':'wspd', 'WindDirection':'wdir', 'WindDirection_SD':'wd_std', 'ShortwaveRadiationIn_Avg':'dsr', 'ShortwaveRadiationOut_Avg':'usr','LongwaveRadiationIn_Avg':'dlr', 'LongwaveRadiationOut_Avg':'ulr','TemperatureRadSensor_Avg':'t_rad', 'SnowHeight':'z_boom', 'SnowHeightQuality':'z_boom_q', 'SurfaceHeight':'z_stake', 'SurfaceHeightQuality':'z_stake_q', 'IceHeight_Avg':'z_pt', 'TemperatureIce1m_Avg':'t_i_1', 'TemperatureIce2m_Avg':'t_i_2', 'TemperatureIce3m_Avg':'t_i_3', 'TemperatureIce4m_Avg':'t_i_4', 'TemperatureIce5m_Avg':'t_i_5', 'TemperatureIce6m_Avg':'t_i_6','TemperatureIce7m_Avg':'t_i_7', 'TemperatureIce10m_Avg':'t_i_8', 'TiltX_Avg':'tilt_x', 'TiltY_Avg':'tilt_y', 'TimeGPS':'gps_time', 'Latitude':'gps_lat', 'Longitude':'gps_lon', 'Altitude':'gps_alt', 'Giodal':'gps_geoid', 'Quality':'gps_q', 'NumberSatellites':'gps_numsat', 'HDOP':'gps_hdop', 'TemperatureLogger_Avg':'t_log', 'Fan_current_avg':'fan_dc' })
#+END_SRC

#+NAME: fix_headers_type1a
#+BEGIN_SRC ipython
df = df.rename(columns = {'RECORD':'rec', 'AirPressure_Avg':'p','Temperature_Avg':'t_1', 'Temperature2_Avg':'t_2', 'RelativeHumidity_Avg':'rh', 'WindSpeed':'wspd', 'WindDirection':'wdir', 'WindDirection_SD':'wd_std', 'ShortwaveRadiationIn_Avg':'dsr', 'ShortwaveRadiationOut_Avg':'usr','LongwaveRadiationIn_Avg':'dlr', 'LongwaveRadiationOut_Avg':'ulr','TemperatureRadSensor_Avg':'t_rad', 'SnowHeight':'z_boom', 'SnowHeightQuality':'z_boom_q', 'SurfaceHeight':'z_stake', 'SurfaceHeightQuality':'z_stake_q', 'IceHeight_Avg':'z_pt', 'TemperatureIce1m_Avg':'t_i_1', 'TemperatureIce2m_Avg':'t_i_2', 'TemperatureIce3m_Avg':'t_i_3', 'TemperatureIce4m_Avg':'t_i_4', 'TemperatureIce5m_Avg':'t_i_5', 'TemperatureIce6m_Avg':'t_i_6','TemperatureIce7m_Avg':'t_i_7', 'TemperatureIce10m_Avg':'t_i_8', 'TiltX_Avg':'tilt_x', 'TiltY_Avg':'tilt_y', 'TimeGPS':'gps_time', 'Latitude':'gps_lat', 'Longitude':'gps_lon', 'Altitude':'gps_alt', 'Giodal':'gps_geoid', 'Quality':'gps_q', 'NumberSatellites':'gps_numsat', 'HDOP':'gps_hdop', 'TemperatureLogger_Avg':'t_log', 'FanCurrent_Avg':'fan_dc' })
#+END_SRC


*** Take a look at L0 data 


**** zac_l
#+BEGIN_SRC ipython
<<load_libraries>>
folder = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0/'
station_id = 'zac_l'
variables = ['t_1', 'rh', 'usr']
fig, ax = plt.subplots(3,1, figsize = (10,10))
filelist = glob(folder+station_id+'/*')
df = pd.concat((pd.read_csv(f,index_col = 0, parse_dates = True,low_memory=False) for f in filelist))
df.sort_index(inplace=True)
df = df.drop(df.index[df.index < datetime(2007,1,1)])


for index,key in enumerate(variables):
    ax[index].plot(df[key])
    ax[index].set_title(key)
    ax[index].plot(df[key])
    ax[index].set_title(key)
    ax[index].plot(df[key])
    ax[index].set_title(key)


    
    

#+END_SRC

#+RESULTS:
:results:
# Out [6]: 
# text/plain
: <Figure size 720x720 with 3 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/fad2afd5dad1e1c83bd3179e4f2c678aa6ca2331.png]]
:end:



**** zac_u
#+BEGIN_SRC ipython
<<load_libraries>>
folder = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0/'
station_id = 'zac_u'
variables = ['t_1','p', 'rh', 'usr']
fig, ax = plt.subplots(4,1, figsize = (10,10))
filelist = glob(folder+station_id+'/*')
df = pd.concat((pd.read_csv(f,index_col = 0, parse_dates = True,low_memory=False) for f in filelist))
df.sort_index(inplace=True)
#df = df.drop(df.index[df.index < datetime(2007,1,1)])


for index,key in enumerate(variables):
    ax[index].plot(df[key])
    ax[index].set_title(key)
    ax[index].plot(df[key])
    ax[index].set_title(key)
    ax[index].plot(df[key])
    ax[index].set_title(key)


    
    

#+END_SRC

#+RESULTS:
:results:
# Out [7]: 
# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/ea3f8f206569b1f87534f4d70e73774649c4c228.png]]
:end:


** Create L0M files: add nead header
L0 files are converted into files with the nead header - this is in order to ensure the correct meta data follows each file
The nead header template is filled out manually as 01-nead header files

# fields: time, rec, p, t_1, t_2, rh, wspd, wdir, wd_std, dsr, usr,dlr, ulr, t_rad, z_boom, z_boom_q, z_stake, z_stake_q, z_pt, t_i_1, t_i_2, t_i_3, t_i_4, t_i_5, t_i_6, t_i_7,t_i_8, tilt_x, tilt_y, gps_time, gps_lat, gps_lon,gps_alt, gps_geoid, gps_q, gps_numsat, gps_hdop, t_log, fan_dc


*** zac_l

concatenate the data with the nead header file, rename and put in L0M folder
#+NAME: make_L0M_zac_l
#+BEGIN_SRC bash :eval yes
station_id=zac_l
sourcedir=/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0/$station_id
destdir=/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/$station_id

years=2008-2010
endyear=2010 
<<concat_nead_and_data>>

years=2010-2011
endyear=2011 
<<concat_nead_and_data>>

years=2011-2012
endyear=2012 
<<concat_nead_and_data>>

years=2012-2013
endyear=2013 
<<concat_nead_and_data>>

years=2013-2014
endyear=2014 
<<concat_nead_and_data>>

years=2014-2015
endyear=2015 
<<concat_nead_and_data>>

years=2015-2016
endyear=2016 
<<concat_nead_and_data>>

years=2016-2017
endyear=2017 
<<concat_nead_and_data>>

years=2017-2018
endyear=2018 
<<concat_nead_and_data>>

years=2018-2020
endyear=2020 
<<concat_nead_and_data>>

years=2020-2021
endyear=2021 
<<concat_nead_and_data>>

years=2021-2022
endyear=2022 
<<concat_nead_and_data>>

#+END_SRC

#+RESULTS: make_L0M_zac_l

#+NAME: concat_nead_and_data
#+BEGIN_SRC bash :eval yes
cp $sourcedir/$station_id-${years}.csv $destdir/$station_id-${years}.csv
sed -i '1d' $destdir/$station_id-${years}.csv
cat $destdir/01_nead_$station_id-$years.csv $destdir/$station_id-${years}.csv > $destdir/$station_id-$endyear.csv
rm $destdir/$station_id-${years}.csv

#+END_SRC

#+RESULTS: concat_nead_and_data


*** zac_u
I start by taking a look at the pngs of the files to figure out the quality of each file and if any of them should be split up.

None of them needs to be split-up.

Then I manually copy the files to the L0M folder 
#+NAME: make_L0M_zac_u
#+BEGIN_SRC bash :eval yes

station_id=zac_u
sourcedir=/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0/$station_id
destdir=/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/$station_id

years=2008-2010
endyear=2010 
<<concat_nead_and_data>>

years=2010-2011
endyear=2011 
<<concat_nead_and_data>>

years=2011-2012
endyear=2012 
<<concat_nead_and_data>>

years=2012-2013
endyear=2013 
<<concat_nead_and_data>>

years=2013-2014
endyear=2014 
<<concat_nead_and_data>>

years=2014-2015
endyear=2015 
<<concat_nead_and_data>>

years=2015-2016
endyear=2016 
<<concat_nead_and_data>>

years=2016-2017
endyear=2017 
<<concat_nead_and_data>>

years=2017-2019
endyear=2019 
<<concat_nead_and_data>>

years=2019-2020
endyear=2020 
<<concat_nead_and_data>>

years=2020-2021
endyear=2021 
<<concat_nead_and_data>>

years=2021-2022
endyear=2022 
<<concat_nead_and_data>>
#+END_SRC

#+RESULTS: make_L0M_zac_u


#+BEGIN_SRC bash :eval yes
ls $destdir
#+END_SRC

#+RESULTS:
| 01_nead_zac_u-2008-2010.csv | 01_nead_zac_u-2015-2016.csv | zac_u-2010.csv | zac_u-2016.csv |
| 01_nead_zac_u-2010-2011.csv | 01_nead_zac_u-2016-2017.csv | zac_u-2011.csv | zac_u-2017.csv |
| 01_nead_zac_u-2011-2012.csv | 01_nead_zac_u-2017-2019.csv | zac_u-2012.csv | zac_u-2019.csv |
| 01_nead_zac_u-2012-2013.csv | 01_nead_zac_u-2019-2020.csv | zac_u-2013.csv | zac_u-2020.csv |
| 01_nead_zac_u-2013-2014.csv | 01_nead_zac_u-2020-2021.csv | zac_u-2014.csv | zac_u-2021.csv |
| 01_nead_zac_u-2014-2015.csv | 01_nead_zac_u-2021-2022.csv | zac_u-2015.csv | zac_u-2022.csv |




*** zac_a

time,rec,p,t_1,t_2,rh,wspd,wdir,wd_std,dsr,usr,dlr,ulr,t_rad,z_boom,z_boom_q,z_stake,z_stake_q,t_i_1,t_i_2,t_i_3,t_i_4,t_i_5,t_i_6,t_i_7,t_i_8,tilt_x,tilt_y,gps_time,gps_lat,gps_lon,gps_alt,gps_geoid,gps_q,gps_numsat,gps_hdop,t_log,fan_dc
#+NAME: make_L0M_zac_a
#+BEGIN_SRC bash
station_id=zac_a
sourcedir=/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0/$station_id
destdir=/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/$station_id

years=2009-2010
endyear=2010 

<<concat_nead_and_data>>

years=2010-2011
endyear=2011 
<<concat_nead_and_data>>

years=2011-2012
endyear=2012 
<<concat_nead_and_data>>

years=2012-2013
endyear=2013 
<<concat_nead_and_data>>


years=2013-2014
endyear=2014 
<<concat_nead_and_data>>


years=2014-2015
endyear=2015 
<<concat_nead_and_data>>

years=2015-2016
endyear=2016 
<<concat_nead_and_data>>

years=2016-2017
endyear=2017 
<<concat_nead_and_data>>

years=2017-2018
endyear=2018 
<<concat_nead_and_data>>

years=2018-2019
endyear=2019 
<<concat_nead_and_data>>


#+END_SRC

#+RESULTS: make_L0M_zac_a




** Create L1 files: convert to physical values 
   
*** Workflow

NB I use the NEAD data format for L0 data. Program installed via:
pip install --upgrade git+https://github.com/GEUS-PROMICE/pyNEAD.git


#+BEGIN_SRC ipython
import nead
import pandas as pd
import numpy as np
import os
import glob

workingdir ='/home/shl/OneDrive/projects/aws_processing_v1.0/'  
station = 'zac_l'
<<convert_to_physical_values>>


station = 'zac_u'
<<convert_to_physical_values>>

station = 'zac_a'
<<convert_to_physical_values>>

#+END_SRC

#+RESULTS:
:results:
# Out [4]: 
# output
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2017.csv
/tmp/ipykernel_21849/1467182618.py:38: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:135: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2020.csv
/tmp/ipykernel_21849/1467182618.py:38: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:135: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2015.csv
/tmp/ipykernel_21849/1467182618.py:38: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:135: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2016.csv
/tmp/ipykernel_21849/1467182618.py:38: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:135: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2012.csv
/tmp/ipykernel_21849/1467182618.py:38: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:135: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2014.csv
/tmp/ipykernel_21849/1467182618.py:38: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:135: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2018.csv
/tmp/ipykernel_21849/1467182618.py:38: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:135: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2021.csv
/tmp/ipykernel_21849/1467182618.py:38: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:135: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2013.csv
/tmp/ipykernel_21849/1467182618.py:38: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:135: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2022.csv
/tmp/ipykernel_21849/1467182618.py:38: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:135: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2011.csv
/tmp/ipykernel_21849/1467182618.py:38: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:135: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2010.csv
/tmp/ipykernel_21849/1467182618.py:38: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:135: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2017.csv
/tmp/ipykernel_21849/1467182618.py:373: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:470: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/tmp/ipykernel_21849/1467182618.py:564: RuntimeWarning: divide by zero encountered in true_divide
  phi_sensor_rad = -np.pi/2-np.arctan(Y/X) # spherical coordinate
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2016.csv
/tmp/ipykernel_21849/1467182618.py:373: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:470: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2019.csv
/tmp/ipykernel_21849/1467182618.py:373: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:470: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/tmp/ipykernel_21849/1467182618.py:564: RuntimeWarning: divide by zero encountered in true_divide
  phi_sensor_rad = -np.pi/2-np.arctan(Y/X) # spherical coordinate
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2020.csv
/tmp/ipykernel_21849/1467182618.py:373: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:470: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/tmp/ipykernel_21849/1467182618.py:564: RuntimeWarning: divide by zero encountered in true_divide
  phi_sensor_rad = -np.pi/2-np.arctan(Y/X) # spherical coordinate
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2010.csv
/tmp/ipykernel_21849/1467182618.py:373: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:470: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/tmp/ipykernel_21849/1467182618.py:564: RuntimeWarning: invalid value encountered in true_divide
  phi_sensor_rad = -np.pi/2-np.arctan(Y/X) # spherical coordinate
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2021.csv
/tmp/ipykernel_21849/1467182618.py:373: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:470: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2013.csv
/tmp/ipykernel_21849/1467182618.py:373: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:470: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2012.csv
/tmp/ipykernel_21849/1467182618.py:373: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:470: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/tmp/ipykernel_21849/1467182618.py:564: RuntimeWarning: invalid value encountered in true_divide
  phi_sensor_rad = -np.pi/2-np.arctan(Y/X) # spherical coordinate
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2011.csv
/tmp/ipykernel_21849/1467182618.py:373: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:470: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/tmp/ipykernel_21849/1467182618.py:564: RuntimeWarning: invalid value encountered in true_divide
  phi_sensor_rad = -np.pi/2-np.arctan(Y/X) # spherical coordinate
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2014.csv
/tmp/ipykernel_21849/1467182618.py:373: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:470: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2015.csv
/tmp/ipykernel_21849/1467182618.py:373: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:470: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2022.csv
/tmp/ipykernel_21849/1467182618.py:373: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:470: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2011.csv
/tmp/ipykernel_21849/1467182618.py:707: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:804: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2016.csv
/tmp/ipykernel_21849/1467182618.py:707: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:804: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2012.csv
/tmp/ipykernel_21849/1467182618.py:707: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:804: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2018.csv
/tmp/ipykernel_21849/1467182618.py:707: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:804: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2014.csv
/tmp/ipykernel_21849/1467182618.py:707: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:804: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2017.csv
/tmp/ipykernel_21849/1467182618.py:707: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:804: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2013.csv
/tmp/ipykernel_21849/1467182618.py:707: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:804: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2015.csv
/tmp/ipykernel_21849/1467182618.py:707: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:804: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2010.csv
/tmp/ipykernel_21849/1467182618.py:707: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:804: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)
/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2019.csv
/tmp/ipykernel_21849/1467182618.py:707: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
/tmp/ipykernel_21849/1467182618.py:804: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  s = np.int(win_size/2)

:end:


#+NAME: convert_to_physical_values
#+BEGIN_SRC ipython
filelist = glob.glob(workingdir+'data_v1.0/L0M/'+station+'/'+station+'**.csv')

for infile in filelist:
    print(str(infile))
    <<read_infile_from_filelist>>
    <<add_variable_metadata>>
    ds = add_variable_metadata(ds)
    <<raw_to_phys>>
    <<correct_rh_and_rad>>
    <<write_out_L1_nc>>
    
   
#+END_SRC


*** code
**** Debugging

#+NAME: Debugging
#+BEGIN_SRC ipython
import nead
import pandas as pd
import numpy as np
import os
import glob
import matplotlib.pyplot as plt


station = 'zac_l'

workingdir ='/home/shl/Dropbox/GEUS/projects/glaciobasis/aws_processing/'  

#filelist = glob.glob(workingdir+'data/L0M/'+station+'/'+station+'**.csv')

infile = workingdir+'data/L0M/'+station+'/'+station+'-2011.csv'
print(str(infile))
<<read_infile_from_filelist>>
<<add_variable_metadata>>
ds = add_variable_metadata(ds)

<<raw_to_phys>>

#+END_SRC

#+RESULTS: Debugging
:results:
# Out [8]: 
# output
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
/tmp/ipykernel_13278/2143753574.py in <module>
      7 
      8 
----> 9 ds = nead.read(infile)

~/miniconda3/envs/py38/lib/python3.8/site-packages/nead/nead.py in read(neadfile, MKS, multi_index, index_col)
     54             key_eq_val = line.split("#")[1].strip()
     55             if key_eq_val == '' or key_eq_val == None: continue  # Line is just "#" or "# " or "#   #"...
---> 56             assert("=" in key_eq_val), print(line, key_eq_val)
     57             key = key_eq_val.split("=")[0].strip()
     58             val = key_eq_val.split("=")[1].strip()

AssertionError: None
:end:

#+BEGIN_SRC ipython

<<plot_dsr_usr>>

<<correct_rh_and_rad>>
<<plot_dsr_usr>>

<<write_out_L1_nc>>
    
   
#+END_SRC

#+NAME: plot_dsr_usr
#+BEGIN_SRC ipython
fig, ax = plt.subplots(1,2)
ds.usr.plot(ax = ax[0])
ds.dsr.plot(ax = ax[1])
#+END_SRC

#+BEGIN_SRC ipython
workingdir ='/home/shl/Dropbox/GEUS/projects/glaciobasis/aws_processing/'  
station = 'zac_u'
infile = workingdir+'data/L0M/'+station+'/'+station+'-2013.csv'

ds = nead.read(infile)
liste = list(ds.keys())
print(liste)
#+END_SRC

#+RESULTS:
:results:
# Out [33]: 
# output
['time', 'rec', 'p', 't_1', 't_2', 'rh', 'wspd', 'wdir', 'wd_std', 'dsr', 'usr', 'dlr', 'ulr', 't_rad', 'z_boom', 'z_boom_q', 'tilt_x', 'tilt_y', 't_log', 'fan_dc']

:end:

#+NAME: plot_ds_all_variab
#+BEGIN_SRC ipython
liste = list(ds.keys())
data = {}
for variab in liste:
    data[variab] = ds[variab]


data_df = pd.DataFrame(data)    
data_df.index = pd.to_datetime(data_df['time'])


#data_df.loc[data_df.index > '2014-April-25' ,'t_i_1'].plot()
data_df.plot(subplots=True, layout=(6,7), figsize=(30,30))
#print(ds['z_pt'].dtype)
#ds['z_pt_corr'].plot()
#+END_SRC

#+RESULTS:
:results:
# Out [34]: 
# text/plain
: array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fe08bbd4bb0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0ab0e1e50>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a81c80d0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a7225d30>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a71d9310>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a72038b0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a720f850>],
:        [<matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a71bae20>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a718d9a0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a7139f40>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a70ee520>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a7099ac0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a70cf0a0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a7079640>],
:        [<matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a7024be0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a6fdc1c0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a7004760>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a6f2fd00>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a6ee52e0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a6d9c760>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a6dc6040>],
:        [<matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a6d6e790>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a6cd7f10>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a6d0c6d0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a079be50>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a038f610>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a033ad90>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a036d550>],
:        [<matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a0317cd0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a02cc490>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a02f6c10>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a02ac3d0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a0253b50>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a020a310>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a0231a90>],
:        [<matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a01e7250>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a01909d0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a0147190>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a016e910>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a01260d0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a00ce850>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a0046040>]],
:       dtype=object)

# text/plain
: <Figure size 2160x2160 with 42 Axes>

# image/png
[[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/e83682bc5d7086478aec9ea5649bfef18eb458c9.png]]
:end:



**** read files and add metadata
#+NAME: read_infile_from_filelist
#+BEGIN_SRC ipython
#infile = filelist[0]
ds = nead.read(infile)
ds = ds.set_coords(['time'])
ds = ds.set_index({'index':'time'})
ds = ds.rename({'index':'time'})
ds['time'] = pd.to_datetime(ds.time.values)
ds['n'] = (('time'), np.arange(ds.time.size)+1)

# Remove duplicate dates
_, index_dublicates = np.unique(ds['time'], return_index=True)
ds = ds.isel(time=index_dublicates)

# Remove inf
for column in ds.keys():
    ds[column][ds[column]==np.inf] = np.nan


#+END_SRC


#+NAME: add_variable_metadata
#+BEGIN_SRC ipython
def add_variable_metadata(ds):
    """Uses the variable DB (variables.csv) to add metadata to the xarray dataset."""
    df = pd.read_csv("./variables.csv", index_col=0, comment="#")

    for v in df.index:
        if v == 'time': continue # coordinate variable, not normal var
        if v not in list(ds.variables): continue
        for c in ['standard_name', 'long_name', 'units']:
            if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
            ds[v].attrs[c] = df[c][v]
            
    return ds
#+END_SRC


**** Dataprocessing code

***** raw_to_phys
#+NAME: raw_to_phys
#+BEGIN_SRC ipython
import re
T_0 = 273.15

# Calculate pressure transducer fluid density

if 'z_pt' in ds:
    if ds.attrs['pt_antifreeze'] == 50:
        rho_af = 1092
    elif ds.attrs['pt_antifreeze'] == 100:
        rho_af = 1145
    else:
        rho_af = np.nan
        print("ERROR: Incorrect metadata: 'pt_antifreeze =' ", ds.attrs['pt_antifreeze'])
        print("Antifreeze mix only supported at 50 % or 100%")
        # assert(False)
    
for v in ['gps_geounit','min_y']:
    if v in list(ds.variables): ds = ds.drop_vars(v)


# convert radiation from engineering to physical units
if 'dsr' in ds:
        
    ds['dsr'] = (ds['dsr']*10) / ds.attrs['dsr_eng_coef'] * 100
    ds['usr'] = (ds['usr']*10) / ds.attrs['usr_eng_coef'] * 100
    ds['dlr'] = ((ds['dlr']*1000) / ds.attrs['dlr_eng_coef']) + 5.67*10**(-8)*(ds['t_rad'] + T_0)**4 
    ds['ulr'] = ((ds['ulr']*1000) / ds.attrs['ulr_eng_coef']) + 5.67*10**(-8)*(ds['t_rad'] + T_0)**4

    ds['tilt_x'] = ds['tilt_x'] / 100
    ds['tilt_y'] = ds['tilt_y'] / 100

# Adjust sonic ranger readings for sensitivity to air temperature
if 'z_boom' in ds:
    ds['z_boom'] = ds['z_boom'] * ((ds['t_1'] + T_0)/T_0)**0.5 
if 'z_stake' in ds:
    ds['z_stake'] = ds['z_stake'] * ((ds['t_1'] + T_0)/T_0)**0.5

# Adjust pressure transducer due to fluid properties
if 'z_pt' in ds:
    #print('z_pt_corr is produced in' + str(infile) )
    ds['z_pt'] = ds['z_pt'] * ds.attrs['pt_z_coef'] * ds.attrs['pt_z_factor'] * 998.0 / rho_af

    # Calculate pressure transducer depth
    ds['z_pt_corr'] = ds['z_pt'] * np.nan # new 'z_pt_corr' copied from 'z_pt'
    ds['z_pt_corr'].attrs['long_name'] = ds['z_pt'].long_name + " corrected"
    ds['z_pt_corr'] = ds['z_pt'] * ds.attrs['pt_z_coef'] * ds.attrs['pt_z_factor'] * 998.0 / rho_af \
        + 100 * (ds.attrs['pt_z_p_coef'] - ds['p']) / (rho_af * 9.81)


# Decode GPS
if 'gps_lat' in ds:
    if ds['gps_lat'].dtype.kind == 'O': # not a float. Probably has "NH"
        #assert('NH' in ds['gps_lat'].dropna(dim='time').values[0])
        for v in ['gps_lat','gps_lon','gps_time']:
            a = ds[v].attrs # store
            str2nums = [re.findall(r"[-+]?\d*\.\d+|\d+", _) if isinstance(_, str) else [np.nan] for _ in ds[v].values]
            ds[v][:] = pd.DataFrame(str2nums).astype(float).T.values[0]
            ds[v] = ds[v].astype(float)
            ds[v].attrs = a # restore

    if np.any((ds['gps_lat'] <= 90) & (ds['gps_lat'] > 0)):  # Some stations only recorded minutes, not degrees
        xyz = np.array(re.findall("[-+]?[\d]*[.][\d]+", ds.attrs['geometry'])).astype(float)
        x=xyz[0]; y=xyz[1]; z=xyz[2] if len(xyz) == 3 else 0
        p = shapely.geometry.Point(x,y,z)
        # from IPython import embed; embed()
        # assert(False) # should p be ints rather than floats here?
        # ds['gps_lat'] = ds['gps_lat'].where(
        ds['gps_lat'] = ds['gps_lat'] + 100*p.y
    if np.any((ds['gps_lon'] <= 90) & (ds['gps_lon'] > 0)):
        ds['gps_lon'] = ds['gps_lon'] + 100*p.x

    for v in ['gps_lat','gps_lon']:
        a = ds[v].attrs # store
        ds[v] = np.floor(ds[v] / 100) + (ds[v] / 100 - np.floor(ds[v] / 100)) * 100 / 60
        ds[v].attrs = a # restore


# Correct winddir due to boom_azimuth

# ds['ws'].

# tilt-o-meter voltage to degrees
# if transmitted ne 'yes' then begin
#    tiltX = smooth(tiltX,7,/EDGE_MIRROR,MISSING=-999) & tiltY = smooth(tiltY,7,/EDGE_MIRROR, MISSING=-999)
# endif

# Should just be
# if ds.attrs['PROMICE_format'] != 'TX': dstxy = dstxy.rolling(time=7, win_type='boxcar', center=True).mean()
# but the /EDGE_MIRROR makes it a bit more complicated...

if 'tilt_x' in ds:
    win_size=7
    s = np.int(win_size/2)
    tdf = ds['tilt_x'].to_dataframe()
    ds['tilt_x'] = (('time'), tdf.iloc[:s][::-1].append(tdf).append(tdf.iloc[-s:][::-1]).rolling(win_size, win_type='boxcar',     center=True).mean()[s:-s].values.flatten())
    tdf = ds['tilt_y'].to_dataframe()
    ds['tilt_y'] = (('time'), tdf.iloc[:s][::-1].append(tdf).append(tdf.iloc[-s:][::-1]).rolling(win_size, win_type='boxcar',    center=True).mean()[s:-s].values.flatten())

    # # notOKtiltX = where(tiltX lt -100, complement=OKtiltX) & notOKtiltY = where(tiltY lt -100, complement=OKtiltY)
    notOKtiltX = (ds['tilt_x'] < -100)
    OKtiltX = (ds['tilt_x'] >= -100)
    notOKtiltY = (ds['tilt_y'] < -100)
    OKtiltY = (ds['tilt_y'] >= -100)

    # tiltX = tiltX/10.
    ds['tilt_x'] = ds['tilt_x'] / 10
    ds['tilt_y'] = ds['tilt_y'] / 10

    # tiltnonzero = where(tiltX ne 0 and tiltX gt -40 and tiltX lt 40)
    # if n_elements(tiltnonzero) ne 1 then tiltX[tiltnonzero] = tiltX[tiltnonzero]/abs(tiltX[tiltnonzero])*(-0.49*(abs(tiltX[tiltnonzero]))^4 +   3.6*(abs(tiltX[tiltnonzero]))^3 - 10.4*(abs(tiltX[tiltnonzero]))^2 +21.1*(abs(tiltX[tiltnonzero])))

    # tiltY = tiltY/10.
    # tiltnonzero = where(tiltY ne 0 and tiltY gt -40 and tiltY lt 40)
    # if n_elements(tiltnonzero) ne 1 then tiltY[tiltnonzero] = tiltY[tiltnonzero]/abs(tiltY[tiltnonzero])*(-0.49*(abs(tiltY[tiltnonzero]))^4 + 3.6*(abs(tiltY[tiltnonzero]))^3 - 10.4*(abs(tiltY[tiltnonzero]))^2 +21.1*(abs(tiltY[tiltnonzero])))

    dstx = ds['tilt_x']
    nz = (dstx != 0) & (np.abs(dstx) < 40)
    dstx = dstx.where(~nz, other = dstx / np.abs(dstx) * (-0.49 * (np.abs(dstx))**4 + 3.6 * (np.abs(dstx))**3 - 10.4 * (np.abs(dstx))**2 + 21.1 * (np.abs(dstx))))
    ds['tilt_x'] = dstx

    dsty = ds['tilt_y']
    nz = (dsty != 0) & (np.abs(dsty) < 40)
    dsty = dsty.where(~nz, other = dsty / np.abs(dsty) * (-0.49 * (np.abs(dsty))**4 + 3.6 * (np.abs(dsty))**3 - 10.4 * (np.abs(dsty))**2 + 21.1 * (np.abs(dsty))))
    ds['tilt_y'] = dsty

    # if n_elements(OKtiltX) gt 1 then tiltX[notOKtiltX] = interpol(tiltX[OKtiltX],OKtiltX,notOKtiltX) ; Interpolate over gaps for radiation correction; set to -999 again below.
    # if n_elements(OKtiltY) gt 1 then tiltY[notOKtiltY] = interpol(tiltY[OKtiltY],OKtiltY,notOKtiltY) ; Interpolate over gaps for radiation correction; set to -999 again below.

    ds['tilt_x'] = ds['tilt_x'].where(~notOKtiltX)
    ds['tilt_y'] = ds['tilt_y'].where(~notOKtiltY)
    ds['tilt_x'] = ds['tilt_x'].interpolate_na(dim='time')
    ds['tilt_y'] = ds['tilt_y'].interpolate_na(dim='time')

# ds['tilt_x'] = ds['tilt_x'].ffill(dim='time')
# ds['tilt_y'] = ds['tilt_y'].ffill(dim='time')


deg2rad = np.pi / 180
ds['wdir'] = ds['wdir'].where(ds['wspd'] != 0)
ds['wspd_x'] = ds['wspd'] * np.sin(ds['wdir'] * deg2rad)
ds['wspd_y'] = ds['wspd'] * np.cos(ds['wdir'] * deg2rad)
    
 #+END_SRC




#+BEGIN_SRC ipython
import matplotlib.pyplot as plt
#sel = (dates.index > pd.to_datetime('2009-05-01', format = '%Y-%m-%d')) & (dates.index < pd.to_datetime('2009-05-04', format = '%Y-%m-%d'))
sel = (dates.index.year==2009) & (dayofyear > 200) & (dayofyear < 203)
#plt.plot(dates.index.values[sel],theta_sensor_rad[sel])
plt.plot(dates.index.values[sel],ds['tilt_x'][sel])

#+END_SRC

#+RESULTS:
:results:
# Out [53]: 
# text/plain
: [<matplotlib.lines.Line2D at 0x7f1021f8e6d0>]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/9331103c48eebae828ac3eb2a391dcbe0fb7c304.png]]
:end:


***** Correct rh and rad

 #+NAME: correct_rh_and_rad
 #+BEGIN_SRC ipython
<<correct_RH>>

if 'dsr' in ds:
    <<correct_longwave>>
    <<correct_short_wave_radiation_for_tilt>>

 #+END_SRC

 #+RESULTS: correct_rh_and_rad
 :results:
 # Out [2]: 
 :end:


****** Correct RH water to be relative to ice when T_air is freezing :noexport:
  Relative humidity is recorded as over water - it needs to be corrected as it is over ice when the surface is freezing

 #+NAME: correct_RH
  #+BEGIN_SRC ipython
below = ds['t_1'].values < 0
T = ds['t_1'].values + 273.15

ew = 10**(-7.90298*(373.16/T-1)+5.02808*np.log10(373.16/T)-(1.3816*10**(-7))*(10**(11.344*(1-T/373.16))-1)+(8.1328*10**(-3))*(10**(-349149*(373.16/T-1))-1)+np.log10(1013.246))

ew = 10**(-7.90298*(373.16/T-1)+5.02808*np.log10(373.16/T)-(1.3816*10**(-7))*(10**(11.344*(1-T/373.16))-1)+(8.1328*10**(-3))*(10**(-349149*(373.16/T-1))-1)+np.log10(1013.246))

ei = 10**(-9.09718*(273.16/T-1)-3.56654*np.log10(273.16/T)+0.876793*(1-T/273.16)+np.log10(6.1071))

rh_ice = ds['rh'].values*ew/ei

rh = ds['rh'].copy()
rh[below] = rh_ice[below]
rh[rh>100] = 100
rh[rh<0] = 0
ds['rh_corr'] = rh.copy()
ds['rh_corr'].attrs['long_name'] = ds['rh'].long_name + " corrected"          

  #+END_SRC

  #+RESULTS: correct_RH
  :results:
  # Out [35]: 
  :end:

 #+BEGIN_SRC ipython
ds['rh'].plot()
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [67]: 
 # text/plain
 : [<matplotlib.lines.Line2D at 0x7f5bac7e1f70>]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/bdf2167b88409bfb17ca4dde11cfeee2cdca87b5.png]]
 :end:

 #+BEGIN_SRC ipython
ds['rh_corr'].plot()
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [47]: 
 # text/plain
 : [<matplotlib.lines.Line2D at 0x7f5bac66e850>]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/3dfff018b48d608bcaaf993d9a28cbb5df998eff.png]]
 :end:


****** calc_t_surf                                                 :noexport:


 #+NAME: calc_t_surf
 #+BEGIN_SRC ipython
T_0 = 273.15
LWin_cor = ds['dlr']#+5.67*10**(-8)*(ds['t_rad']+273.15)**4
LWout_cor = ds['ulr']#+5.67*10**(-8)*(ds['t_rad']+273.15)**4

#ds = ds.assign({'dlr_corr':LWin_cor})
#ds['dlr_corr']=LWin_cor
#ds['dlr_corr'].attrs['long_name'] = ds['dlr'].long_name + " corrected"   
#ds = ds.assign({'ulr_corr':LWout_cor})
#ds['ulr_corr'].attrs['long_name'] = ds['ulr'].long_name + " corrected"   
epsilon = 0.97
sigma = 5.67*10**(-8)
Tsurf = ((ds['ulr'].values-(1-epsilon)*ds['dlr'].values)/(epsilon*sigma))**0.25 -T_0
ds = ds.assign({'t_surf':Tsurf})
#ds['t_surf'] = Tsurf
 #+END_SRC


 #+BEGIN_SRC ipython
ds['ulr'].plot()
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [80]: 
 # text/plain
 : [<matplotlib.lines.Line2D at 0x7f5bac172af0>]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/96df1794dd22d0e34b20b35ed84d2e352f179bcb.png]]
 :end:


 #+BEGIN_SRC ipython
plt.plot(LWin_cor)
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [74]: 
 # text/plain
 : [<matplotlib.lines.Line2D at 0x7f5bac8bfe80>]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/14231135e34bfd0c4c40ba7e489a57846a3b30f6.png]]
 :end:

****** Correct short wave radiation for tilt                       :noexport:
 Correcting Short wave radiation for tilt according to MacWhorter and Weller 1990
 This code is adapted from the promice processing idl code version 2012.

 #+NAME: correct_short_wave_radiation_for_tilt
 #+BEGIN_SRC ipython
<<cloud_cover_tsurf>>
<<tilt_angle_rotate>>
<<zenith_and_hour_angle>>
<<correction_factor_for_direct_beam_radiation>>
<<corr_srin_for_tilt>>
<<ok_albedos>>
<<corr_sr_diffuse_radiation>>
<<corr_large_zenith_angles>>
<<corr_srin_upper_sensor_not_in_sight>>
<<removing_spikes>>
<<adding_columns>>
 #+END_SRC



******* Code                                                       :noexport:
 Calculate cloud cover for SRin correction and surface temperature
 #+NAME: cloud_cover_tsurf
 #+BEGIN_SRC ipython :tangle correct_sw_for_tilt.py
# Calculate cloud cover
T_0 = 273.15
eps_overcast = 1
eps_clear = 9.36508e-6
LR_overcast = eps_overcast*5.67*10**(-8)*(ds['t_1']+T_0)**4 # assumption
LR_clear = eps_clear*5.67*10**(-8)*(ds['t_1']+T_0)**6 # Swinbank (1963)
CloudCov = (ds['dlr'].values-LR_clear)/(LR_overcast-LR_clear)

overcast = CloudCov > 1
Clear = CloudCov < 0
CloudCov[overcast] = 1
CloudCov[Clear] = 0
DifFrac = 0.2+0.8*CloudCov


 #+END_SRC

 #+RESULTS: cloud_cover_tsurf
 :results:
 # Out [32]: 
 :end:



#+BEGIN_SRC ipython
plt.plot(CloudCov)
#+END_SRC

#+RESULTS:
:results:
# Out [34]: 
# text/plain
: [<matplotlib.lines.Line2D at 0x7f160554b310>]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/91f647bcc404b549585361d9618b3381c35ea7ea.png]]
:end:

 Calculating the tilt angle and direction of senson and rotating to a north-south aligned coordinate system

 #+NAME: tilt_angle_rotate
 #+BEGIN_SRC ipython :tangle correct_sw_for_tilt.py
# Calculating the tilt angle and direction of senson and rotating to a north-south aligned coordinate system
deg2rad = np.pi / 180
tiltX_rad = ds['tilt_x'].values*deg2rad
tiltY_rad = ds['tilt_y'].values*deg2rad

X = np.sin(tiltX_rad)*np.cos(tiltX_rad)*(np.sin(tiltY_rad))**2 + np.sin(tiltX_rad)*(np.cos(tiltY_rad))**2 # Cartesian coordinate
Y = np.sin(tiltY_rad)*np.cos(tiltY_rad)*(np.sin(tiltX_rad))**2 + np.sin(tiltY_rad)*(np.cos(tiltX_rad))**2 # Cartesian coordinate
Z = np.cos(tiltX_rad)*np.cos(tiltY_rad) + (np.sin(tiltX_rad))**2*(np.sin(tiltY_rad))**2 # Cartesian coordinate
phi_sensor_rad = -np.pi/2-np.arctan(Y/X) # spherical coordinate

phi_sensor_rad[X > 0] = phi_sensor_rad[X > 0]+np.pi
phi_sensor_rad[(X == 0) & (Y < 0)] = np.pi
phi_sensor_rad[(X == 0) & (Y >= 0)] = 0
phi_sensor_rad[phi_sensor_rad < 0] = phi_sensor_rad[phi_sensor_rad < 0]+2*np.pi

phi_sensor_deg = phi_sensor_rad*180/np.pi # radians to degrees
theta_sensor_rad = np.arccos(Z/(X**2+Y**2+Z**2)**0.5) # spherical coordinate (or actually total tilt of the sensor, i.e. 0 when horizontal)
theta_sensor_deg = theta_sensor_rad*180/np.pi # radians to degrees



 #+END_SRC

 #+RESULTS: tilt_angle_rotate
 :results:
 # Out [35]: 
 # output
 <ipython-input-35-807541dbfffd>:8: RuntimeWarning: divide by zero encountered in true_divide
   phi_sensor_rad = -np.pi/2-np.arctan(Y/X) # spherical coordinate

 :end:
#+BEGIN_SRC ipython

plt.plot(theta_sensor_deg)
#plt.plot(X)
#ds['dlr_corr'].plot()
#+END_SRC

#+RESULTS:
:results:
# Out [50]: 
# text/plain
: [<matplotlib.lines.Line2D at 0x7f16053c5790>]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/db35d548c9db588cbf02bf7929edf49622f02527.png]]
:end:



 #+BEGIN_SRC ipython 
import matplotlib.pyplot as plt
#sel = (dates.index > pd.to_datetime('2009-05-01', format = '%Y-%m-%d')) & (dates.index < pd.to_datetime('2009-05-04', format = '%Y-%m-%d'))
sel = (dates.index.year==2009) & (dayofyear > 200) & (dayofyear < 203)
#plt.plot(dates.index.values[sel],theta_sensor_rad[sel])
plt.plot(dates.index.values,ds['tilt_x'])

 #+END_SRC

 #+RESULTS:
 :results:
 # Out [38]: 
 # text/plain
 : [<matplotlib.lines.Line2D at 0x7f1023055d60>]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/c9fb39dc3f980d2cb9cb3f629ba7328bb89b7cf1.png]]
 :end:

 Calculating zenith and hour angle of the sun
 #+NAME: zenith_and_hour_angle
 #+BEGIN_SRC ipython :tangle correct_sw_for_tilt.py
# Calculating zenith and hour angle of the sun
lat = float(ds.geometry[13:19]) #ds['gps_lat'].median().values
lon = float(ds.geometry[6:12]) #ds['gps_lon'].median().values
dates = ds.time.to_dataframe()
dates.index = pd.to_datetime(dates['time'])
dayofyear =dates.index.dayofyear.values
hour = dates.index.hour.values
minute = dates.index.minute.values

d0_rad = 2*np.pi*(dayofyear+(hour+minute/60)/24-1)/365
Declination_rad = np.arcsin(0.006918-0.399912*np.cos(d0_rad)+0.070257*np.sin(d0_rad)-0.006758*np.cos(2*d0_rad)+0.000907*np.sin(2*d0_rad)-0.002697*np.cos(3*d0_rad)+0.00148*np.sin(3*d0_rad))

HourAngle_rad = 2*np.pi*(((hour+minute/60.)/24-0.5))# - lon/360) #- 15.*timezone/360.) ; NB: Make sure time is in UTC and longitude is positive when west! Hour angle should be 0 at noon.
DirectionSun_deg = HourAngle_rad*180/np.pi-180 # This is 180 deg at noon (NH), as opposed to HourAngle.
DirectionSun_deg[DirectionSun_deg < 0] = DirectionSun_deg[DirectionSun_deg < 0]+360
DirectionSun_deg[DirectionSun_deg < 0] = DirectionSun_deg[DirectionSun_deg < 0]+360

ZenithAngle_rad = np.arccos(np.cos(lat*np.pi/180)*np.cos(Declination_rad)*np.cos(HourAngle_rad) + np.sin(lat*np.pi/180)*np.sin(Declination_rad))
ZenithAngle_deg = ZenithAngle_rad*180/np.pi
sundown = ZenithAngle_deg >= 90
SRtoa = 1372*np.cos(ZenithAngle_rad) # SRin at the top of the atmosphere
SRtoa[sundown] = 0


 #+END_SRC

 #+RESULTS: zenith_and_hour_angle
 :results:
 # Out [51]: 
 :end:


 #+BEGIN_SRC ipython
#import matplotlib.pyplot as plt
#sel = (dates.index > pd.to_datetime('2009-05-01', format = '%Y-%m-%d')) & (dates.index < pd.to_datetime('2009-05-04', format = '%Y-%m-%d'))
plt.plot(ZenithAngle_rad)
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [54]: 
 # text/plain
 : [<matplotlib.lines.Line2D at 0x7f16052eea60>]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/1f4e35464b8de951ddc3ab5bf7fd1a1999e258c1.png]]
 :end:



 http://solardat.uoregon.edu/SolarRadiationBasics.html


% Calculating the correction factor for direct beam radiation (http://solardat.uoregon.edu/SolarRadiationBasics.html)
CorFac = sin(Declination_rad).*sin(lat*pi/180.).*cos(theta_sensor_rad)...
        -sin(Declination_rad).*cos(lat*pi/180.).*sin(theta_sensor_rad).*cos(phi_sensor_rad+pi) ...
        +cos(Declination_rad).*cos(lat*pi/180.).*cos(theta_sensor_rad).*cos(HourAngle_rad) ...
        +cos(Declination_rad).*sin(lat*pi/180.).*sin(theta_sensor_rad).*cos(phi_sensor_rad+pi).*cos(HourAngle_rad) ...
        +cos(Declination_rad).*sin(theta_sensor_rad).*sin(phi_sensor_rad+pi).*sin(HourAngle_rad);





 #+NAME: correction_factor_for_direct_beam_radiation
 #+BEGIN_SRC ipython :tangle correct_sw_for_tilt.py
# correction factor for direct beam radiation
CorFac = np.sin(Declination_rad) * np.sin(lat*np.pi/180.) * np.cos(theta_sensor_rad) \
         -np.sin(Declination_rad) * np.cos(lat*np.pi/180.) * np.sin(theta_sensor_rad) * np.cos(phi_sensor_rad+np.pi) \
        +np.cos(Declination_rad) * np.cos(lat*np.pi/180.) * np.cos(theta_sensor_rad) * np.cos(HourAngle_rad) \
        +np.cos(Declination_rad) * np.sin(lat*np.pi/180.) * np.sin(theta_sensor_rad) * np.cos(phi_sensor_rad+np.pi) * np.cos(HourAngle_rad) \
        +np.cos(Declination_rad) * np.sin(theta_sensor_rad)*np.sin(phi_sensor_rad+np.pi)*np.sin(HourAngle_rad)

CorFac = np.cos(ZenithAngle_rad)/CorFac
no_correction = (CorFac <= 0) | ( ZenithAngle_deg > 90) # sun out of field of view upper sensor
CorFac[no_correction] = 1
  #+END_SRC

 #+RESULTS: correction_factor_for_direct_beam_radiation
 :results:
 # Out [33]: 
 :end:

 #+BEGIN_SRC ipython
plt.plot(CorFac)
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [57]: 
 # text/plain
 : [<matplotlib.lines.Line2D at 0x7f16052278e0>]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/01b726a9adda83cfb8e06d570738da3b2988f3b1.png]]
 :end:


 Calculating SRin over a horizontal surface corrected for station/sensor tilt
 #+NAME: corr_srin_for_tilt
 #+BEGIN_SRC ipython :tangle correct_sw_for_tilt.py
 # Calculating SRin over a horizontal surface corrected for station/sensor tilt
CorFac_all = CorFac/(1-DifFrac+CorFac*DifFrac)
SRin_cor = ds['dsr']*CorFac_all

 #+END_SRC

 #+RESULTS: corr_srin_for_tilt
 :results:
 # Out [58]: 
 :end:

 #+BEGIN_SRC ipython
plt.plot(SRin_cor)
#plt.ylim([0,1000])
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [59]: 
 # text/plain
 : [<matplotlib.lines.Line2D at 0x7f160517eaf0>]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/c289450a32d53aef4d0a34439312cae89f91754c.png]]
 :end:



 Calculating albedo based on albedo values when sun is in sight of the upper sensor
 #+NAME: ok_albedos
 #+BEGIN_SRC ipython :tangle correct_sw_for_tilt.py
# Calculating albedo based on albedo values when sun is in sight of the upper sensor
AngleDif_deg = 180/np.pi*np.arccos(np.sin(ZenithAngle_rad)*np.cos(HourAngle_rad+np.pi)*np.sin(theta_sensor_rad)*np.cos(phi_sensor_rad) \
                             +np.sin(ZenithAngle_rad)*np.sin(HourAngle_rad+np.pi)*np.sin(theta_sensor_rad)*np.sin(phi_sensor_rad) \
                             +np.cos(ZenithAngle_rad)*np.cos(theta_sensor_rad)) # angle between sun and sensor


albedo = ds['usr']/SRin_cor


OKalbedos = (AngleDif_deg < 70) & (ZenithAngle_deg < 70) & (albedo < 1) & (albedo > 0)

notOKalbedos = (AngleDif_deg >= 70) | (ZenithAngle_deg >= 70) | (albedo >= 1) | (albedo <= 0)

albedo[notOKalbedos] = np.nan
#albedo = albedo.ffill('time')
#albedo = interp1(datenumber,albedo,datenumber,'pchip') ;% interpolate over gaps - gives problems for discontinuous data sets, but is not the end of the world

 #+END_SRC

 #+RESULTS: ok_albedos
 :results:
 # Out [60]: 
 :end:

#+BEGIN_SRC ipython
print(OKalbedos)
#+END_SRC

#+RESULTS:
:results:
# Out [62]: 
# output
<xarray.DataArray (time: 52852)>
array([False, False, False, ...,  True, False,  True])
Coordinates:
  * time     (time) datetime64[ns] 2016-04-21T17:00:00 ... 2017-04-26T15:40:00

:end:


 #+BEGIN_SRC ipython
plt.plot(OKalbedos)
#plt.ylim([0,1000])
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [63]: 
 # text/plain
 : [<matplotlib.lines.Line2D at 0x7f1605127340>]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/0db119809caeda1c79694d1444edce09c974fbf3.png]]
 :end:

 Correcting SR using SRin when sun is in field of view of lower sensor assuming sensor measures only diffuse radiation

 #+NAME: corr_sr_diffuse_radiation
 #+BEGIN_SRC ipython :tangle correct_sw_for_tilt.py
# Correcting SR using SRin when sun is in field of view of lower sensor assuming sensor measures only diffuse radiation
sunonlowerdome = (AngleDif_deg >= 90) & (ZenithAngle_deg <= 90)
SRin_cor[sunonlowerdome] = ds['dsr'][sunonlowerdome].values/DifFrac[sunonlowerdome]

SRout_cor = ds['usr']
SRout_cor[sunonlowerdome] = albedo[sunonlowerdome]*ds['dsr'][sunonlowerdome].values/DifFrac[sunonlowerdome]

 #+END_SRC

 #+RESULTS: corr_sr_diffuse_radiation
 :results:
 # Out [64]: 
 :end:

#+BEGIN_SRC ipython
plt.plot(SRout_cor)
#+END_SRC

#+RESULTS:
:results:
# Out [67]: 
# text/plain
: [<matplotlib.lines.Line2D at 0x7f1605648220>]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/426e37f59fff783f18cd28f42d2c4dfc023f4b7a.png]]
:end:

 Setting SRin and SRout to zero for solar zenith angles larger than 95 deg or either SRin or SRout are (less than) zero

 #+NAME: corr_large_zenith_angles
 #+BEGIN_SRC ipython :tangle correct_sw_for_tilt.py
# Setting SRin and SRout to zero for solar zenith angles larger than 95 deg or either SRin or SRout are (less than) zero
no_SR = (ZenithAngle_deg > 95) | (SRin_cor <= 0) | (SRout_cor <= 0)

SRin_cor[no_SR] = 0
SRout_cor[no_SR] = 0

 #+END_SRC

 #+RESULTS: corr_large_zenith_angles
 :results:
 # Out [68]: 
 :end:
 #+BEGIN_SRC ipython
plt.plot(SRin_cor)
#+END_SRC

#+RESULTS:
:results:
# Out [69]: 
# text/plain
: [<matplotlib.lines.Line2D at 0x7f160511a820>]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/b9b8276bdd74614b4c886175a5fe2e1bdfd58f05.png]]
:end:


 % Correcting SRin using more reliable SRout when sun not in sight of upper sensor

 This gives problems for some  of the years
 #+NAME: corr_srin_upper_sensor_not_in_sight
 #+BEGIN_SRC ipython :tangle correct_sw_for_tilt.py
# Correcting SRin using more reliable SRout when sun not in sight of upper sensor

#SRin_cor[no_correction] = SRout_cor[no_correction]/albedo[no_correction]
SRin_cor[~notOKalbedos] = SRout_cor[~notOKalbedos]/albedo[~notOKalbedos]
#SRin_cor = SRout_cor/albedo # What is done in the IDL code
#albedo[notOKalbedos] = -999
 #+END_SRC

 #+RESULTS: corr_srin_upper_sensor_not_in_sight
 :results:
 # Out [70]: 
 :end:

#+BEGIN_SRC ipython
plt.plot(SRin_cor)
#+END_SRC

#+RESULTS:
:results:
# Out [71]: 
# text/plain
: [<matplotlib.lines.Line2D at 0x7f1604fa7a60>]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/b9b8276bdd74614b4c886175a5fe2e1bdfd58f05.png]]
:end:


 % Removing spikes by interpolation based on a simple top-of-the-atmosphere limitation

 #+NAME: removing_spikes
 #+BEGIN_SRC ipython :tangle correct_sw_for_tilt.py
# Removing spikes by interpolation based on a simple top-of-the-atmosphere limitation
TOA_crit_nopass = SRin_cor > 0.9*SRtoa+10

SRin_cor[TOA_crit_nopass] = np.nan
SRout_cor[TOA_crit_nopass] = np.nan

 #+END_SRC

 #+RESULTS: removing_spikes
 :results:
 # Out [72]: 
 :end:

 #+BEGIN_SRC ipython
plt.plot(SRin_cor)
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [76]: 
 # text/plain
 : [<matplotlib.lines.Line2D at 0x7f1604dfc760>]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/8947f12e7cf13df68082ba45b0e87225e9807a99.png]]
 :end:


 #+NAME: adding_columns
 #+BEGIN_SRC ipython :tangle correct_sw_for_tilt.py
# Assign columns to ds file
ds = ds.assign({'albedo':albedo, 'dsr_corr':SRin_cor, 'usr_corr':SRout_cor, 'cloud_cov':CloudCov})
#ds['dsr_corr']=SRin_cor
ds['dsr_corr'].attrs['long_name'] = ds['dsr'].long_name + " corrected"   
#ds['usr_corr']=SRout_cor.copy()
ds['usr_corr'].attrs['long_name'] = ds['usr'].long_name + " corrected"   
#ds['cloud_cover']=CloudCov.copy()
 #+END_SRC

 #+RESULTS: adding_columns
 :results:
 # Out [163]: 
 :end:


 #+BEGIN_SRC ipython
ds['dsr_corr'].plot()
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [182]: 
 # text/plain
 : [<matplotlib.lines.Line2D at 0x7f101bf60250>]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/ca54238e7f6e2a4c1885edbc24df45d269ef0c01.png]]
 :end:


**** Filter bad data



**** Write out L1 files
 #+NAME: write_out_L1_nc
 #+BEGIN_SRC ipython

outpath = 'data_v1.0/L1/'+station+'/'
outfile = infile[-14:-4]
ds = ds.sel(time=ds.time.notnull())

outpathfile = outpath + outfile + ".nc"
if os.path.exists(outpathfile): os.remove(outpathfile)
ds.to_netcdf(outpathfile, mode='w', format='NETCDF4', compute=True)

 #+END_SRC

 #+RESULTS:
 :results:
 # Out [89]: 
 # output
 ---------------------------------------------------------------------------
 ValueError                                Traceback (most recent call last)
 /tmp/ipykernel_13278/2713352631.py in <module>
       1 new_index = pd.date_range(start = ds.time.data[0], end =ds.time.data[-1], freq = '10min')
 ----> 2 ds.reindex(time=new_index, fill_value=np.nan, inplace = True)
       3 print(ds)
       4 #ds = ds.dropna(dim='time', how = 'all')
       5 

 ~/miniconda3/envs/py38/lib/python3.8/site-packages/xarray/core/dataset.py in reindex(self, indexers, method, tolerance, copy, fill_value, **indexers_kwargs)
    2941 
    2942         """
 -> 2943         return self._reindex(
    2944             indexers,
    2945             method,

 ~/miniconda3/envs/py38/lib/python3.8/site-packages/xarray/core/dataset.py in _reindex(self, indexers, method, tolerance, copy, fill_value, sparse, **indexers_kwargs)
    2968         bad_dims = [d for d in indexers if d not in self.dims]
    2969         if bad_dims:
 -> 2970             raise ValueError(f"invalid reindex dimensions: {bad_dims}")
    2971 
    2972         variables, indexes = alignment.reindex_variables(

 ValueError: invalid reindex dimensions: ['inplace']
 :end:

 #+RESULTS: write_out_L1_nc
 :results:
 # Out [87]: 
 # output
 <xarray.Dataset>
 Dimensions:     (time: 51501)
 Coordinates:
   * time        (time) datetime64[ns] 2010-05-13T03:10:00 ... 2011-05-05T18:4...
 Data variables: (12/47)
     rec         (time) float64 ...
     p           (time) float64 ...
     t_1         (time) float64 ...
     t_2         (time) float64 ...
     rh          (time) float64 ...
     wspd        (time) float64 ...
     ...          ...
     wspd_y      (time) float64 ...
     rh_corr     (time) float64 ...
     albedo      (time) float64 ...
     dsr_corr    (time) float64 ...
     usr_corr    (time) float64 ...
     cloud_cov   (time) float64 ...
 Attributes: (12/16)
     station_id:       zac_u
     field_delimiter:  ,
     nodata:           -9999
     srid:             EPSG:4326
     geometry:         POINT(-21.47, 74.64)
     timezone:         0
     ...               ...
     ulr_eng_coef:     0
     pt_z_coef:        0
     pt_z_p_coef:      0
     pt_z_factor:      0
     pt_antifreeze:    50
     boom_azimuth:     240

 :end:

#+BEGIN_SRC ipython
print(ds.time[0].data)
#+END_SRC

#+RESULTS:
:results:
# Out [78]: 
# output
2010-05-13T03:10:00.000000000

:end:


*** Make one single netcdf per station
#+BEGIN_SRC ipython
import pandas as pd
import xarray as xr

datadir = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/'
with xr.open_dataset(datadir+'zac_u/zac_u-2016.nc') as ds:
    #s.time.diff(dim = 'time').plot()
    print(ds.time[-1])
    #print(ds['time'].values)
    #plt.plot(ds['time'].values)

with xr.open_dataset(datadir+'zac_u/zac_u-2017.nc') as ds:
    #s.time.diff(dim = 'time').plot()
    print(ds.time[0])
    #rint(ds.time[-1])
#+END_SRC

#+RESULTS:
:results:
# Out [157]: 
# output
<xarray.DataArray 'time' ()>
array('2016-04-21T16:50:00.000000000', dtype='datetime64[ns]')
Coordinates:
    time     datetime64[ns] 2016-04-21T16:50:00
<xarray.DataArray 'time' ()>
array('2014-04-21T19:50:00.000000000', dtype='datetime64[ns]')
Coordinates:
    time     datetime64[ns] 2014-04-21T19:50:00

:end:



#+BEGIN_SRC ipython
import pandas as pd
import xarray as xr
from glob import glob
import matplotlib.pyplot as plt
import os

datadir = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/'
outfile = datadir+'zac_l/zac_l-2008-2022.nc'
with xr.open_mfdataset(datadir+'zac_l/zac_l-20??.nc') as ds:
    ds.to_netcdf(outfile)
    ds.to_dataframe().to_csv(datadir+'zac_l/zac_l-2008-2022.txt')

outfile = datadir+'zac_u/zac_u-2008-2022.nc'
with xr.open_mfdataset(datadir+'zac_u/zac_u-20??.nc') as ds:
    ds.to_netcdf(outfile)
    ds.to_dataframe().to_csv(datadir+'zac_u/zac_u-2008-2022.txt')



outfile = datadir+'zac_a/zac_a-2009-2020.nc'
with xr.open_mfdataset(datadir+'zac_a/zac_a-20??.nc') as ds:
    ds.to_netcdf(outfile)
    ds.to_dataframe().to_csv(datadir+'zac_a/zac_a-2009-2020.txt')


#+END_SRC

#+RESULTS:
:results:
# Out [4]: 
:end:

#+BEGIN_SRC ipython

if os.path.isfile(outfile):
    os.remove(outfile)

ds.to_netcdf(outfile)


infile = glob(datadir+'zac_u/zac_u-20??.nc')
print(infile)
outfile = datadir+'zac_u/zac_u-2008-2022.nc'

ds = xr.open_dataset(infile[0]).load().dropna(dim='time', how='all')
for f in infile[1:]:
    tmp = xr.open_mfdataset(f).load().dropna(dim='time', how='all')
    ds = ds.combine_first(tmp)

if os.path.isfile(outfile):
    os.remove(outfile)

ds.to_netcdf(outfile)    



infile = glob(datadir+'zac_a/zac_a-20??.nc')
outfile = datadir+'zac_a/zac_a-2009-2020.nc'

ds = xr.open_mfdataset(infile[0]).load().dropna(dim='time', how='all')
for f in infile[1:]:
    tmp = xr.open_mfdataset(f).load().dropna(dim='time', how='all')
    ds = ds.combine_first(tmp)

if os.path.isfile(outfile):
    os.remove(outfile)

ds.to_netcdf(outfile)    


#+END_SRC

#+RESULTS:
:results:
# Out [12]: 
# output
['/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2013.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2016.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2011.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2015.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2014.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2017.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2010.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2021.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2022.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2020.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2012.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2019.nc']

:end:



*** Quality check plots


**** dataseries examples
#+BEGIN_SRC ipython 
import xarray as xr
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from glob import glob

filename = 'data_v1.0/L1/zac_a/zac_a-2009-2020.nc'

ds = xr.open_dataset(filename)

    
ds_filtered = ds.copy()
ds_filtered['p'] = ds['p'].copy()
ds_filtered['p'] = ds['p'].where(ds['p'] > (ds['p'].median() - 100))
ds_filtered['t_1'] = ds['t_1'].where(ds['t_1'] > -50)
ds_filtered['t_2'] = ds['t_2'].where(ds['t_2'] > -50)


fig, ax = plt.subplots(7,1,figsize=(20,20))
ds['dsr_corr'].plot(ax = ax[0])
ds['usr_corr'].plot(ax = ax[0])
ds['dlr'].plot(ax = ax[1])
ds['ulr'].plot(ax = ax[1])
ax[1].set_ylim([0,400])
ds_filtered['p'].plot(ax=ax[2])
ds_filtered['t_1'].plot(ax=ax[3])
ds_filtered['t_2'].plot(ax=ax[3])
ds['rh_corr'].plot(ax=ax[4])
#+END_SRC

#+RESULTS:
:results:
# Out [11]: 
# text/plain
: [<matplotlib.lines.Line2D at 0x7fdb14a999a0>]

# text/plain
: <Figure size 1440x1440 with 7 Axes>

# image/png
[[file:obipy-resources/9a75e1f66bb62abb9073492c1adee0276ae958cd/cb3c704b3b9fccc4606e0ffb16ce77414f383691.png]]
:end:


#+BEGIN_SRC ipython 
import xarray as xr
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from glob import glob

filename = 'data_v1.0/L1/zac_l/zac_l-2008-2022.nc'

ds = xr.open_dataset(filename)
    
ds_filtered = ds.copy()
ds_filtered['p'] = ds['p'].copy()
ds_filtered['p'] = ds['p'].where(ds['p'] > (ds['p'].median() - 100))
ds_filtered['t_1'] = ds['t_1'].where((ds['t_1'] > -50) & (ds['t_1'] < 50))
ds_filtered['t_2'] = ds['t_2'].where((ds['t_2'] > -50) & (ds['t_2'] < 50))


fig, ax = plt.subplots(5,1,figsize=(20,10))
ds['dsr_corr'].plot(ax = ax[0])
ds['usr_corr'].plot(ax = ax[0])
ds['dlr'].plot(ax = ax[1])
ds['ulr'].plot(ax = ax[1])
ax[1].set_ylim([0,400])
ds_filtered['p'].plot(ax=ax[2])
ds_filtered['t_1'].plot(ax=ax[3])
#ds_filtered['t_2'].plot(ax=ax[3])
ds['rh_corr'].plot(ax=ax[4])

#+END_SRC

#+RESULTS:
:results:
# Out [7]: 
# text/plain
: [<matplotlib.lines.Line2D at 0x7f0ff9809940>]

# text/plain
: <Figure size 1440x720 with 5 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/c08b04ec46e73a8101c21f2d1fd06752ba5e7fae.png]]
:end:


**** Get positions for all three stations
#+BEGIN_SRC ipython 
import xarray as xr
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from glob import glob

filename = 'data_v1.0/L1/zac_l/zac_l-2008-2020.nc'
with xr.open_dataset(filename) as ds:
    lat_l = ds.gps_lat.mean().values
    lon_l = ds.gps_lon.mean().values
    alt_l = ds.gps_alt.mean().values
    

filename = 'data_v1.0/L1/zac_u/zac_u-2008-2020.nc'
with xr.open_dataset(filename) as ds:
    lat_u = ds.gps_lat.mean().values
    lon_u = ds.gps_lon.mean().values
    alt_u = ds.gps_alt.mean().values
filename = 'data_v1.0/L1/zac_a/zac_a-2009-2020.nc'
with xr.open_dataset(filename) as ds:
    lat_a = ds.gps_lat.mean().values
    lon_a = ds.gps_lon.mean().values
    alt_a = ds.gps_alt.mean().values



stations = ['zac_l','zac_u','zac_a']
lat = [+lat_l,+lat_u,+lat_a]
lon = [-lon_l,-lon_u,-lon_a]
alt = [+alt_l,+alt_u, +alt_a]

positions = pd.DataFrame({'station':stations, 'lat':lat, 'lon':lon, 'elev':alt})
positions.set_index('station', inplace= True)
#positions = pd.to_numeric(positions)
positions.to_csv('GlacioBasis_Zackenberg_station_positions.csv', index=True, float_format = '%.4f')
print(positions)
#+END_SRC

#+RESULTS:
:results:
# Out [30]: 
# output
               lat        lon         elev
station                                   
zac_l    74.624558 -21.375218   644.513575
zac_u    74.644020 -21.469426   877.588662
zac_a    74.647475 -21.651841  1476.043991

:end:

#+BEGIN_SRC ipython

    
ds_filtered = ds.copy()
ds_filtered['p'] = ds['p'].copy()
ds_filtered['p'] = ds['p'].where(ds['p'] > (ds['p'].median() - 100))
ds_filtered['t_1'] = ds['t_1'].where(ds['t_1'] > -50)
ds_filtered['t_2'] = ds['t_2'].where(ds['t_2'] > -50)


fig, ax = plt.subplots(7,1,figsize=(20,20))
ds['dsr_corr'].plot(ax = ax[0])
ds['usr_corr'].plot(ax = ax[0])
ds['dlr'].plot(ax = ax[1])
ds['ulr'].plot(ax = ax[1])
ax[1].set_ylim([0,400])
ds_filtered['p'].plot(ax=ax[2])
ds_filtered['t_1'].plot(ax=ax[3])
ds_filtered['t_2'].plot(ax=ax[3])
ds['rh_corr'].plot(ax=ax[4])
#+END_SRC


**** Gradients
#+BEGIN_SRC ipython 
import xarray as xr
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from glob import glob


zac_l= xr.open_dataset('data_v1.0/L1/zac_l/zac_l-2008-2021.nc')
zac_u = xr.open_dataset('data_v1.0/L1/zac_u/zac_u-2008-2020.nc')
zac_a = xr.open_dataset('data_v1.0/L1/zac_a/zac_a-2009-2020.nc')



fig, ax = plt.subplots(2,1,figsize=(10,10))
zac_l['t_1'].plot(ax = ax[0])
zac_u['t_1'].plot(ax = ax[0])
zac_a['t_1'].plot(ax = ax[0])
zac_l['p'].plot(ax = ax[1])
zac_u['p'].plot(ax = ax[1])
zac_a['p'].plot(ax = ax[1])


#+END_SRC

#+RESULTS:
:results:
# Out [21]: 
# text/plain
: [<matplotlib.lines.Line2D at 0x7f2a2f6224f0>]

# text/plain
: <Figure size 720x720 with 2 Axes>

# image/png
[[file:obipy-resources/a20e1397f28347830bf5429083571472c40cf69b/f5351ace8090705aff1c663bec9fa140aed502f0.png]]
:end:




* Code for database delivery

** Python libs

#+NAME: load_libs
#+BEGIN_SRC ipython
import numpy as np
import pandas as pd
import xarray as xr
import matplotlib.pyplot as plt
import datetime
#+END_SRC

#+RESULTS: load_libs
:results:
# Out [68]: 
:end:


** From 2022 and on

 Key variables to deliver:
 Temperature
 RH
 Pressure
 Wind speed
 Radiation
 SR50 boom height

All the code blocs need to be run in order to get the correct output. This is in order to understand all the steps that have been taken to create the dataseries

Flags??
['2020-08-15']:['2021-08-01'] Station was tilted

*** zac_a
**** Pre QC: adding the transmitted data
There are so many quality issues with the transmitted data - that I will omit it for now...
***** Transmitted data - what data do we have
Plotting  all the transmitted data:
  #+BEGIN_SRC ipython
<<load_libs>>
<<Load_transmitted_data>>

transmitted_reduced = transmitted.drop(' timestamp', axis = 1)
transmitted_reduced = transmitted_reduced.dropna(axis = 1, how = 'all')
transmitted_reduced.astype(float).plot(subplots=True, figsize = (20,30), layout = (25,2))
  #+END_SRC

Plotting the transmitted data that makes sense:

  #+BEGIN_SRC ipython
fig, ax = plt.subplots(6,1,figsize=(10,10))
transmitted['temperature.1'].plot(ax=ax[0], label = 'temperature.1')
transmitted['temperature2'].plot(ax=ax[0] , label = 'temperature2')
transmitted['temperature'].plot(ax=ax[0], label = 'temperature')
ax[0].legend()
#ax[0].set_ylim(-40,20)
transmitted['airpressure'].plot(ax=ax[1], label = 'airpressure')
ax[1].legend()
transmitted['relativehumidity'].plot(ax=ax[2])
ax[2].legend()

transmitted['shortwaveradiationin'].plot(ax=ax[3], label = 'shortwaveradiationin')
transmitted['shortwaveradiationout'].plot(ax=ax[3], label = 'shortwaveradiationout')
ax[3].legend()

transmitted['windspeed_1'].plot(ax=ax[4], label = 'windspeed')
ax[4].legend()

transmitted['windspeed_2'].plot(ax=ax[5], label = 'windspeed_2 believed to be direction')
ax[5].legend()

  #+END_SRC

  #+RESULTS:
  :results:
  # Out [31]: 


  # text/plain
  : <Figure size 720x720 with 6 Axes>

  # image/png
  [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/c1d4097c9472cc09bd92af6ce1e0fda68d5b8938.png]]
  :end:


  
***** Transmitted data what is actually usable (when was the station buried)
 We look at the incoming shortwave radiation to define when the station is buried
 #+BEGIN_SRC ipython
 #transmitted_trusted = transmitted[:'2019-August-30']
 <<load_libs>>
 <<data_file_paths>>
 <<Load_transmitted_data>>

 fig, ax = plt.subplots(1,1,figsize = (10,5))
 with xr.open_dataset(zac_a_path) as ds:
     dsr = ds['dsr'].where(ds['dsr'] != -9999.).to_dataframe()
      #print(t_1)

 new_index = pd.date_range(dsr.index[0],dsr.index[-1], freq = '10min')
 dsr = dsr.reindex(new_index)
 ax.plot(dsr.index.dayofyear, dsr)
 ax.plot(transmitted.index.dayofyear,transmitted['shortwaveradiationin'].astype(float))
 dayofinterest=242 # 30 August 2019
 ax.set_ylim(0,1100)
 ymin,ymax = ax.get_ylim()
 ax.vlines(dayofinterest,ymin,ymax, color = 'black', linestyle = '--' )


 #+END_SRC

 #+RESULTS:
 :results:
 # Out [3]: 


 # text/plain
 : <Figure size 720x360 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/998d2f2f707a85173574e02e892be356c447cf86.png]]
 :end:


***** Temperature
First load in libraries and data
 #+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>
<<load_transmitted_trusted>>
  

fig, ax = plt.subplots(1,1,figsize = (10,5))
with xr.open_dataset(zac_a_path) as ds:
    df = ds[['t_1']].to_dataframe()

count10min = df.resample('H').count()
temp_hour = df.resample('H').mean()
temp_hour[count10min<6] = np.nan
count_hours = temp_hour.resample('D').count()
count_hours.plot()
count10min.plot()
temp_day = temp_hour.resample('D').mean()
temp_day[count_hours<24 ] = np.nan

temp_day.plot()
temp_hour.plot(ax=ax)

temp_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_temperature.csv', index = True, float_format = '%g')
temp_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_temperature.csv', index = True, float_format = '%g')

 #+END_SRC

 #+RESULTS:
 :results:
 # Out [183]: 
 # text/plain
 : <Figure size 720x360 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/ed93017186a51340562c5fa8fb287a7c894fd633.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/752b99ca255333fa6c58b84ef91d21f30588eef7.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/94573e3b437b1b46c8361a0ac1e4195571d048b4.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/8d6196d0ef52f6f9afa5370f67570ce4ddc88c9c.png]]
 :end:



****** Adding the transmitted data 


So we trust that the data for now and we can concatenate the hourly and the daily temperature record

Merging the data
   #+BEGIN_SRC ipython
temperature_hour_full = pd.concat((t_1_hour['t_1'],transmitted['temperature']), axis=1)
temperature_hour_full['merged'] = temperature_hour_full[['t_1','temperature']].sum(axis=1, min_count=1)
temp_hour = pd.DataFrame(temperature_hour_full['merged'].copy())
temp_hour.rename(columns={'merged':'Air temperature, C'}, inplace = True)
temp_hour.index.name = 'Timestamp'
 
#If we had any daily transmissions
#temp_day_1 = pd.DataFrame(temp_hour['Air temperature, C'].resample('D').mean())
#temperature_day_full = pd.concat((temp_day_1['Air temperature, C'],transmitted_day['temperature']), axis=1)
#temperature_day_full['merged'] = temperature_day_full[['Air temperature, C','temperature']].sum(axis=1, min_count=1)
#temp_day = pd.DataFrame(temperature_day_full['merged'].copy())
#temp_day.rename(columns={'merged':'Air temperature, C'}, inplace = True)
#temp_day.index.name = 'Timestamp'
temp_day = temp_day.resample('D').mean()
temp_day.plot()
 
temp_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_temperature.csv', index = True, float_format = '%g')
temp_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_temperature.csv', index = True, float_format = '%g')
   #+END_SRC

   #+RESULTS:
   :results:
   # Out [34]: 
   # text/plain
   : <Figure size 432x288 with 1 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/433a9db40cc02596fd4b6ef66dfc1ae6ff4cfedf.png]]
   :end:



***** Radiation 
The transmitted radiation data does not look right - and since we do not have the tilt, I think I will declare this data too uncertain to use. 
So we only use the downloaded data:

  #+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>
    
fig, ax = plt.subplots(1,1,figsize = (10,5))
with xr.open_dataset(zac_a_path) as ds:
    ds = ds[['dsr','usr','dlr','ulr']].to_dataframe()
    ds['dsr'] = ds['dsr'].where(ds['dsr'] != -9999.)
    ds['usr'] = ds['usr'].where(ds['usr'] != -9999.)
    ds['dlr'] = ds['dlr'].where(ds['dlr'] != -9999.)
    ds['ulr'] = ds['ulr'].where(ds['ulr'] != -9999.)
    

count10min = ds.resample('H').count()
rad_hour = ds.resample('H').mean()
rad_hour[count10min<6] = np.nan
count_hours = rad_hour.resample('D').count()
rad_day = rad_hour.resample('D').mean()
rad_day[count_hours<24 ] = np.nan

rad_day.plot(ax=ax)

rad_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_radiation.csv', index = True, float_format = '%g')
rad_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_radiation.csv', index = True, float_format = '%g')


#+END_SRC

#+RESULTS:
:results:
# Out [176]: 
# text/plain
: <Figure size 720x360 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/dae6319b48ac917664f7a8666c3d9128b97b3e7c.png]]
:end:

****** Looking into the possibilities of merging with transmitted data
First load in libraries and data
  #+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>
<<load_transmitted_trusted>>
# Converting transmitted radiation to physical units
# From the nead header 
dsr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
usr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
#dlr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
#ulr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)

dsr_trans = (transmitted['shortwaveradiationin']*10) / dsr_eng_coef #* 100
usr_trans= (transmitted['shortwaveradiationout']*10) / usr_eng_coef #* 100
#ds['dlr'] = ((ds['dlr']*1000) / dlr_eng_coef) + 5.67*10**(-8)*(ds['t_rad'] + T_0)**4 
#    ds['ulr'] = ((ds['ulr']*1000) / ulr_eng_coef) + 5.67*10**(-8)*(ds['t_rad'] + T_0)**4


    
fig, ax = plt.subplots(1,1,figsize = (10,5))
with xr.open_dataset(zac_a_path) as ds:

    dsr = ds['dsr'].where(ds['dsr'] != -9999.).to_dataframe()
    usr = ds['usr'].where(ds['usr'] !=  -9999.).to_dataframe()
    dlr = ds['dlr'].where(ds['dlr'] != -9999.).to_dataframe()
    ulr = ds['ulr'].where(ds['ulr'] !=  -9999.).to_dataframe()
    #print(t_1)


    


new_index = pd.date_range(dsr.index[0],t_1.index[-1], freq = '10min')

#dsr = dsr.reindex(new_index)
#usr = usr.reindex(new_index)
#dlr = dlr.reindex(new_index)
#ulr = ulr.reindex(new_index)



#fig,ax = plt.subplots(1,1,figsize = (10,5))
dsr.plot(ax= ax)
usr.plot(ax=ax)

#+END_SRC

#+RESULTS:
:results:
# Out [48]: 
# text/plain
: <AxesSubplot:xlabel='time'>

# text/plain
: <Figure size 720x360 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/63eed35ae21f8ca75549ff6a8fac58cad7d70027.png]]
:end:

#+BEGIN_SRC ipython
dsr_hour = dsr.resample('H').mean()
dsr_hour_full = pd.concat((dsr_hour['dsr'], dsr_trans), axis = 1)
dsr_hour_full['dsr'].plot(ax=ax)
dsr_hour_full['shortwaveradiationin'].plot(ax=ax)
#print(dsr_hour_full)
dsr_hour_full.plot()
#+END_SRC

#+RESULTS:
:results:
# Out [49]: 
# text/plain
: <AxesSubplot:>

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/d6a163229cb70f4dd527f578424b340331c0636d.png]]
:end:






***** Relative humidity

#+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_a_path) as ds:
    df = ds[['rh']].to_dataframe()

count10min = df.resample('H').count()
rh_hour = df.resample('H').mean()
rh_hour[count10min<6] = np.nan
count_hours = rh_hour.resample('D').count()
count_hours.plot()
count10min.plot()
rh_day = rh_hour.resample('D').mean()
rh_day[count_hours<24 ] = np.nan

rh_day.plot()
rh_hour.plot()

rh_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_relative_humidity.csv', index = True, float_format = '%g')
rh_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_relative_humidity.csv', index = True, float_format = '%g')
    
#+END_SRC

#+RESULTS:
:results:
# Out [4]: 
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/c787b469422a2b9041ddef4ccf11e695b0203a7c.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/c5f0d949b4c82a827ddaa2ad521d2dc5ac20e23b.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/6baec137cbde1dee20e1725072092b7ed008a497.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/dc5c5f282d1a78cca7fd8431570e3b1b1c119147.png]]
:end:


***** Windspeed

#+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_a_path) as ds:
    df = ds[['wspd']].to_dataframe()

count10min = df.resample('H').count()
wspd_hour = df.resample('H').mean()
wspd_hour[count10min<6] = np.nan
count_hours = wspd_hour.resample('D').count()
count_hours.plot()
count10min.plot()
wspd_day = wspd_hour.resample('D').mean()
wspd_day[count_hours<24 ] = np.nan

wspd_day.plot()
wspd_hour.plot()

wspd_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_wind_speed.csv', index = True, float_format = '%g')
wspd_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_wind_speed.csv', index = True, float_format = '%g')
    
#+END_SRC

#+RESULTS:
:results:
# Out [56]: 
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/99f67ea6506aee5ea5a3d0be0b52f58a1f6bbe4c.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/25e685dc49b8d27b42be59ef213586b01caf67d2.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/ac31fe396fea775950e8e83551066c0ebf3b48b1.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/6e9f9bfaf86c2a956b000a488283425f3a6df749.png]]
:end:

***** pressure

#+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_a_path) as ds:
    df = ds[['p']].to_dataframe()

count10min = df.resample('H').count()
p_hour = df.resample('H').mean()
p_hour[count10min<6] = np.nan
count_hours = p_hour.resample('D').count()
count_hours.plot()
count10min.plot()
p_day = p_hour.resample('D').mean()
p_day[count_hours<24 ] = np.nan

p_day.plot()
p_hour.plot()

p_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_pressure.csv', index = True, float_format = '%g')
p_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_pressure.csv', index = True, float_format = '%g')
    
#+END_SRC

#+RESULTS:
:results:
# Out [84]: 
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/d513de89470a2b5df497d438279e6ba9a6319806.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/d172f5826f47d143054a3436fe58db72c59f805c.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/b6c4138501f9e83dacd7dab238ee2db465c759e9.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/0ee61f8980a301a220416da13eb168b15d1fc723.png]]
:end:


**** Post QC

***** Temperature
#+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

temp_hour = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)


# Bad data deleted
temp_hour['2015-01-05':'2015-05-01'] = np.nan
temp_hour[:'2009-08-08 21:00'] = np.nan

count_hours = temp_hour.resample('D').count()
temp_day = temp_hour.resample('D').mean()
temp_day[count_hours<24 ] = np.nan

temp_hour.to_csv('data_v1.0/gem_database/2022/zac_a_hour_temperature.csv', index = True, float_format = '%g')
temp_day.to_csv('data_v1.0/gem_database/2022/zac_a_day_temperature.csv', index = True, float_format = '%g')

#+END_SRC

#+RESULTS:
:results:
# Out [205]: 
:end:

***** Radiation

#+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

rad_hour = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

# Deliting bad data: Out of bounds
maximum = 1000
variable = 'dsr'
rad_hour[variable][rad_hour[variable]>maximum] = np.nan
rad_hour[variable][rad_hour[variable]<0] = np.nan

variable = 'usr'
albedo = rad_hour['usr']/rad_hour['dsr']
rad_hour[variable][albedo>1] = np.nan

variable = 'ulr'
rad_hour[variable][rad_hour[variable]<150] = np.nan

variable = 'dlr'
rad_hour[variable][rad_hour[variable]<120] = np.nan

# Deleting bad data manually
rad_hour['2015-01-01':'2015-05-01'] = np.nan

# Then calculate daily averages again
count_hours = rad_hour.resample('D').count()
rad_day = rad_hour.resample('D').mean()
rad_day[count_hours<24 ] = np.nan

rad_hour.to_csv('data_v1.0/gem_database/2022/zac_a_hour_radiation.csv', index = True, float_format = '%g')
rad_day.to_csv('data_v1.0/gem_database/2022/zac_a_day_radiation.csv', index = True, float_format = '%g')

#+END_SRC

#+RESULTS:
:results:
# Out [13]: 
:end:

***** Relative humidity
#+BEGIN_SRC ipython
<<load_libs>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rad_hour = pd.read_csv(datapath+'preQC/zac_a_hour_relative_humidity.csv', parse_dates = True, index_col=0)


# Outliers
rad_hour = rad_hour.where(rad_hour['rh']<= 100., np.nan)
rad_hour = rad_hour.where(rad_hour['rh']>= 0., np.nan)


count_hours = rad_hour.resample('D').count()
rad_day = rad_hour.resample('D').mean()
rad_day[count_hours<24 ] = np.nan

rad_hour.to_csv('data_v1.0/gem_database/2022/zac_a_hour_relative_humidity.csv', index = True, float_format = '%g')
rad_day.to_csv('data_v1.0/gem_database/2022/zac_a_day_relative_humidity.csv', index = True, float_format = '%g')
#+END_SRC

#+RESULTS:
:results:
# Out [19]: 
:end:


***** Wind speed
#+BEGIN_SRC ipython
<<load_libs>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
wspd_hour = pd.read_csv(datapath+'preQC/zac_a_hour_wind_speed.csv', parse_dates = True, index_col=0)


# Bad data
startday = datetime.datetime(2020,8,15)
endday = datetime.datetime(2022,4,21)
wspd_hour['2020-August-15':'2021-July-21'] = np.nan

count_hours = wspd_hour.resample('D').count()
wspd_day = wspd_hour.resample('D').mean()
wspd_day[count_hours<24 ] = np.nan

wspd_hour.to_csv('data_v1.0/gem_database/2022/zac_a_hour_wind_speed.csv', index = True, float_format = '%g')
wspd_day.to_csv('data_v1.0/gem_database/2022/zac_a_day_wind_speed.csv', index = True, float_format = '%g')
#+END_SRC

#+RESULTS:
:results:
# Out [75]: 
:end:


***** Pressure
#+BEGIN_SRC ipython
<<load_libs>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
p_hour = pd.read_csv(datapath+'preQC/zac_a_hour_pressure.csv', parse_dates = True, index_col=0)

#Outliers

p_hour = p_hour.where(p_hour['p']> 800., np.nan)


count_hours = p_hour.resample('D').count()
p_day = p_hour.resample('D').mean()
p_day[count_hours<24 ] = np.nan

p_hour.to_csv('data_v1.0/gem_database/2022/zac_a_hour_pressure.csv', index = True, float_format = '%g')
p_day.to_csv('data_v1.0/gem_database/2022/zac_a_day_pressure.csv', index = True, float_format = '%g')
#+END_SRC

#+RESULTS:
:results:
# Out [94]: 
:end:

*** zac_l

**** Pre QC
***** Temperature
#+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_l_path) as ds:
    df = ds[['t_1']].to_dataframe()

count10min = df.resample('H').count()
temp_hour = df.resample('H').mean()
temp_hour[count10min<6] = np.nan
count_hours = temp_hour.resample('D').count()
count_hours.plot()
count10min.plot()
temp_day = temp_hour.resample('D').mean()
temp_day[count_hours<24 ] = np.nan

temp_day.plot()
temp_hour.plot()

temp_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_temperature.csv', index = True, float_format = '%g')
temp_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_temperature.csv', index = True, float_format = '%g')

#+END_SRC

#+RESULTS:
:results:
# Out [180]: 
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/a2af43761d6603a432521a5fa8930961dd6fa826.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/72abb9b3f5c7d6055ef0c15fd216e3f22182da95.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/3e388564fb64bfe5f80ab6922b98a4649a36b9e8.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/40786f3480905c749fb162c4ee974012f7adeab0.png]]
:end:



***** Radiation
  #+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>
    
fig, ax = plt.subplots(1,1,figsize = (10,5))
with xr.open_dataset(zac_l_path) as ds:
    ds = ds[['dsr','usr','dlr','ulr']].to_dataframe()
    ds['dsr'] = ds['dsr'].where(ds['dsr'] != -9999.)
    ds['usr'] = ds['usr'].where(ds['usr'] != -9999.)
    ds['dlr'] = ds['dlr'].where(ds['dlr'] != -9999.)
    ds['ulr'] = ds['ulr'].where(ds['ulr'] != -9999.)#.to_dataframe()
    ds['ulr'] = ds['ulr'].where(ds['ulr'] < 10000.)#.to_dataframe()
    ds['dlr'] = ds['dlr'].where(ds['dlr'] < 10000.)#.to_dataframe()

count10min = ds.resample('H').count()
rad_hour = ds.resample('H').mean()
rad_hour[count10min<6] = np.nan
count_hours = rad_hour.resample('D').count()
count_hours.plot()
rad_day = rad_hour.resample('D').mean()
rad_day[count_hours<24 ] = np.nan

rad_day.plot(ax=ax)

rad_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_radiation.csv', index = True, float_format = '%g')
rad_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_radiation.csv', index = True, float_format = '%g')


#+END_SRC

#+RESULTS:
:results:
# Out [162]: 
# text/plain
: <Figure size 720x360 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/056320497f8d3e37f776cd594f0781f1f550a581.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/507612017dcb768fadfff99146485e2b459aaf67.png]]
:end:


***** Relative humidity

#+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_l_path) as ds:
    df = ds[['rh']].to_dataframe()

count10min = df.resample('H').count()
rh_hour = df.resample('H').mean()
rh_hour[count10min<6] = np.nan
count_hours = rh_hour.resample('D').count()
count_hours.plot()
count10min.plot()
rh_day = rh_hour.resample('D').mean()
rh_day[count_hours<24 ] = np.nan

rh_day.plot()
rh_hour.plot()

rh_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_relative_humidity.csv', index = True, float_format = '%g')
rh_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_relative_humidity.csv', index = True, float_format = '%g')
    
#+END_SRC

#+RESULTS:
:results:
# Out [2]: 
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/49204a121b210b62cd1f7bd53c971d5d683ae966.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/9f0eabee8d045cc1ac0979a999023a869f5bcfab.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/7fd78f6c43cb99b9f86244f1e127bffdb6fc4499.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/dfb592e40191de5216dab0ce1b199b8d88725c86.png]]
:end:


***** Windspeed

#+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_l_path) as ds:
    df = ds[['wspd']].to_dataframe()

count10min = df.resample('H').count()
wspd_hour = df.resample('H').mean()
wspd_hour[count10min<6] = np.nan
count_hours = wspd_hour.resample('D').count()
count_hours.plot()
count10min.plot()
wspd_day = wspd_hour.resample('D').mean()
wspd_day[count_hours<24 ] = np.nan

wspd_day.plot()
wspd_hour.plot()

wspd_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_wind_speed.csv', index = True, float_format = '%g')
wspd_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_wind_speed.csv', index = True, float_format = '%g')
    
#+END_SRC

#+RESULTS:
:results:
# Out [57]: 
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/1d52117124a1ba01c32c9ef2a79a2be5db4680e4.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/2aea217ee3f0c6fde9a968c08f1b63a9ad99beff.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/cbd85bcaeed30463a6ef53fb85df00f4271fd33a.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/7e67105463dbd73b8b4c46123a496a4e063ae38b.png]]
:end:

***** pressure

#+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_l_path) as ds:
    df = ds[['p']].to_dataframe()

count10min = df.resample('H').count()
p_hour = df.resample('H').mean()
p_hour[count10min<6] = np.nan
count_hours = p_hour.resample('D').count()
count_hours.plot()
count10min.plot()
p_day = p_hour.resample('D').mean()
p_day[count_hours<24 ] = np.nan

p_day.plot()
p_hour.plot()

p_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_pressure.csv', index = True, float_format = '%g')
p_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_pressure.csv', index = True, float_format = '%g')
    
#+END_SRC

#+RESULTS:
:results:
# Out [83]: 
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/e2d596eaf7af0fb76ce9a5c11393be8d5833b46e.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/ce2ae00073d30f3fc9d365e7fdb4cae990ca2787.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/569f68c817905a6316b135816769c917e3312071.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/336a6b6d1ed5518d60e95c612a5697314dbad5ae.png]]
:end:


**** Post QC
***** Temperature
#+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

temp_hour = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
#Deling bad data

count_hours = temp_hour.resample('D').count()
temp_day = temp_hour.resample('D').mean()
temp_day[count_hours<24 ] = np.nan


temp_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_temperature.csv', index = True, float_format = '%g')
temp_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_temperature.csv', index = True, float_format = '%g')

#+END_SRC

#+RESULTS:
:results:
# Out [193]: 
:end:


***** Radiation

#+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

rad_hour = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)


# Deliting bad data: Out of bounds
maximum = 1000
variable = 'dsr'
rad_hour[variable][rad_hour[variable]>maximum] = np.nan
rad_hour[variable][rad_hour[variable]<0] = np.nan

variable = 'dlr'
rad_hour[variable][rad_hour[variable]<120] = np.nan

variable = 'ulr'
rad_hour[variable][rad_hour[variable]<150] = np.nan

variable = 'usr'
albedo = rad_hour['usr']/rad_hour['dsr']
rad_hour[variable][albedo>1] = np.nan



# Deleting bad data manually
rad_hour['usr']['2021-01-01':'2021-07-21'] = np.nan

# Then calculate daily averages again
count_hours = rad_hour.resample('D').count()
rad_day = rad_hour.resample('D').mean()
rad_day[count_hours<24 ] = np.nan

rad_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_radiation.csv', index = True, float_format = '%g')
rad_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_radiation.csv', index = True, float_format = '%g')

#+END_SRC

#+RESULTS:
:results:
# Out [12]: 
:end:


***** Relative humidity
#+BEGIN_SRC ipython
<<load_libs>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rad_hour = pd.read_csv(datapath+'preQC/zac_l_hour_relative_humidity.csv', parse_dates = True, index_col=0)


# Outliers
rad_hour = rad_hour.where(rad_hour['rh']<= 100., np.nan)
rad_hour = rad_hour.where(rad_hour['rh']>= 0., np.nan)


count_hours = rad_hour.resample('D').count()
rad_day = rad_hour.resample('D').mean()
rad_day[count_hours<24 ] = np.nan

rad_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_relative_humidity.csv', index = True, float_format = '%g')
rad_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_relative_humidity.csv', index = True, float_format = '%g')
#+END_SRC

#+RESULTS:
:results:
# Out [16]: 
:end:


***** Wind speed
#+BEGIN_SRC ipython
<<load_libs>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
wspd_hour = pd.read_csv(datapath+'preQC/zac_l_hour_wind_speed.csv', parse_dates = True, index_col=0)



count_hours = wspd_hour.resample('D').count()
wspd_day = wspd_hour.resample('D').mean()
wspd_day[count_hours<24 ] = np.nan

wspd_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_wind_speed.csv', index = True, float_format = '%g')
wspd_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_wind_speed.csv', index = True, float_format = '%g')
#+END_SRC

#+RESULTS:
:results:
# Out [156]: 
:end:


***** Pressure
#+BEGIN_SRC ipython
<<load_libs>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
p_hour = pd.read_csv(datapath+'preQC/zac_l_hour_pressure.csv', parse_dates = True, index_col=0)

#Outliers

p_hour = p_hour.where(p_hour['p']> 870., np.nan)


p_hour['2016-February-26':'2016-March-1'] = np.nan
p_hour['2017-January-5':'2017-February-22'] = np.nan
p_hour['2018-February-21':'2018-February-28'] = np.nan


count_hours = p_hour.resample('D').count()
p_day = p_hour.resample('D').mean()
p_day[count_hours<24 ] = np.nan

p_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_pressure.csv', index = True, float_format = '%g')
p_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_pressure.csv', index = True, float_format = '%g')
#+END_SRC

#+RESULTS:
:results:
# Out [153]: 
:end:


*** zac_u


**** Pre QC



***** Temperature
#+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_u_path) as ds:
    df = ds[['t_1']].to_dataframe()

count10min = df.resample('H').count()
temp_hour = df.resample('H').mean()
temp_hour[count10min<6] = np.nan
count_hours = temp_hour.resample('D').count()
count_hours.plot()
count10min.plot()
temp_day = temp_hour.resample('D').mean()
temp_day[count_hours<24 ] = np.nan

temp_day.plot()
temp_hour.plot()

temp_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_temperature.csv', index = True, float_format = '%g')
temp_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_temperature.csv', index = True, float_format = '%g')

#+END_SRC

#+RESULTS:
:results:
# Out [181]: 
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/15d0d4bc32351b1b5f34e022f1a0686e8bec6b64.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/6f0ba422f2c3a65ae4ff3332ba9589ebd86384f6.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/06b1e63607a8e431df97fdad78093e664aa1f192.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/d7762ac5cd9a6509a2bf79429e3d1987c00712ee.png]]
:end:


***** Radiation

  #+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>
    
fig, ax = plt.subplots(1,1,figsize = (10,5))
with xr.open_dataset(zac_u_path) as ds:
    ds = ds[['dsr','usr','dlr','ulr']].to_dataframe()
    ds['dsr'] = ds['dsr'].where(ds['dsr'] != -9999.)
    ds['usr'] = ds['usr'].where(ds['usr'] != -9999.)
    ds['dlr'] = ds['dlr'].where(ds['dlr'] != -9999.)
    ds['ulr'] = ds['ulr'].where(ds['ulr'] != -9999.)#.to_dataframe()
    ds['ulr'] = ds['ulr'].where(ds['ulr'] < 10000.)#.to_dataframe()
    ds['dlr'] = ds['dlr'].where(ds['dlr'] < 10000.)#.to_dataframe()


count10min = ds.resample('H').count()
rad_hour = ds.resample('H').mean()
rad_hour[count10min<6] = np.nan
count_hours = rad_hour.resample('D').count()
rad_day = rad_hour.resample('D').mean()
rad_day[count_hours<24 ] = np.nan
#count_hours['April-2014'].plot()
#count10min['April-2014'].plot()
#rad_hour['April-14-2014':'April-15-2014'].plot(ax=ax)

rad_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_radiation.csv', index = True, float_format = '%g')
rad_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_radiation.csv', index = True, float_format = '%g')

#+END_SRC

#+RESULTS:
:results:
# Out [175]: 
# text/plain
: <Figure size 720x360 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/c41910a4eec14e9dfc309a177dc192c9bede8e77.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/3d1a147b194ad7569bf0d7ac50bf0b560388be43.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/600be8e7537f13212cf55fc01caa345b1907b7e7.png]]
:end:


***** Relative humidity

#+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_u_path) as ds:
    df = ds[['rh']].to_dataframe()

count10min = df.resample('H').count()
rh_hour = df.resample('H').mean()
rh_hour[count10min<6] = np.nan
count_hours = rh_hour.resample('D').count()
count_hours.plot()
count10min.plot()
rh_day = rh_hour.resample('D').mean()
rh_day[count_hours<24 ] = np.nan

rh_day.plot()
rh_hour.plot()

rh_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_relative_humidity.csv', index = True, float_format = '%g')
rh_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_relative_humidity.csv', index = True, float_format = '%g')
    
#+END_SRC

#+RESULTS:
:results:
# Out [1]: 
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/7964a7cc51edff2816693689d04e991193425782.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/711835bb17fb7065aebc81f20b68de6f78048817.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/00fad85c3fab36ce3889b0088d32f28f445d65f6.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/5503b83a5bf405cd2d05baa336f186eba11df3ec.png]]
:end:


***** Windspeed

#+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_u_path) as ds:
    df = ds[['wspd']].to_dataframe()

count10min = df.resample('H').count()
wspd_hour = df.resample('H').mean()
wspd_hour[count10min<6] = np.nan
count_hours = wspd_hour.resample('D').count()
count_hours.plot()
count10min.plot()
wspd_day = wspd_hour.resample('D').mean()
wspd_day[count_hours<24 ] = np.nan

wspd_day.plot()
wspd_hour.plot()

wspd_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_wind_speed.csv', index = True, float_format = '%g')
wspd_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_wind_speed.csv', index = True, float_format = '%g')
    
#+END_SRC

#+RESULTS:
:results:
# Out [53]: 
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/9d6dabe7ef19d6227c59914d59508d7744f9837d.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/dcf9b6733fd4f7334aebb57310b45d9de0f9ad13.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/554abf1af32cea2e7fdcc9b88dd8f027e2393721.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/51337ff3da6a485de6fff929c0c3a5b901ce824e.png]]
:end:


***** pressure

#+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>

with xr.open_dataset(zac_u_path) as ds:
    df = ds[['p']].to_dataframe()

count10min = df.resample('H').count()
p_hour = df.resample('H').mean()
p_hour[count10min<6] = np.nan
count_hours = p_hour.resample('D').count()
count_hours.plot()
count10min.plot()
p_day = p_hour.resample('D').mean()
p_day[count_hours<24 ] = np.nan

p_day.plot()
p_hour.plot()

p_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_pressure.csv', index = True, float_format = '%g')
p_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_pressure.csv', index = True, float_format = '%g')
    
#+END_SRC

#+RESULTS:
:results:
# Out [82]: 
# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/d04d48f06de022214a4bbab07c570f35e254d1e0.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/3766e8b007329965e5be46a859688501de571d78.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/9b24e13bf33c41a2d03f3e904c5a64b923e360d3.png]]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/f5df4be4d5af534e536311d5f33a382ac3727f21.png]]
:end:


**** Post QC
***** Temperature

#+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>

datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
temp_hour = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)

# Bad data deleted
temp_hour['2020-09-15':'2021-08-01'] = np.nan
temp_hour['2014-10-30':'2015-12-31'] = np.nan


count_hours = temp_hour.resample('D').count()
temp_day = temp_hour.resample('D').mean()
temp_day[count_hours<24 ] = np.nan

temp_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_temperature.csv', index = True, float_format = '%g')
temp_day.to_csv('data_v1.0/gem_database/2022/zac_u_day_temperature.csv', index = True, float_format = '%g')

#+END_SRC

#+RESULTS:
:results:
# Out [237]: 
:end:

***** Radiation
#+BEGIN_SRC ipython
<<load_libs>>
<<data_file_paths>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

rad_hour = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)


# Deliting bad data: Out of bounds
maximum = 1000
variable = 'dsr'
rad_hour[variable][rad_hour[variable]>maximum] = np.nan
rad_hour[variable][rad_hour[variable]<0] = np.nan

variable = 'usr'
albedo = rad_hour['usr']/rad_hour['dsr']
rad_hour[variable][albedo>1] = np.nan

variable = 'ulr'
rad_hour[variable][rad_hour[variable]<150] = np.nan

variable = 'dlr'
rad_hour[variable][rad_hour[variable]<120] = np.nan

# Deleting bad data manually
rad_hour['2020-08-15':'2021-08-01'] = np.nan # Station was tilted
rad_hour['2015-01-01':'2015-12-31'] = np.nan

rad_hour= rad_hour['2012-05-05':]


# Then calculate daily averages again
count_hours = rad_hour.resample('D').count()
rad_day = rad_hour.resample('D').mean()
rad_day[count_hours<24 ] = np.nan

rad_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_radiation.csv', index = True, float_format = '%g')
rad_day.to_csv('data_v1.0/gem_database/2022/zac_u_day_radiation.csv', index = True, float_format = '%g')

#+END_SRC

#+RESULTS:
:results:
# Out [11]: 
:end:


***** Relative humidity
#+BEGIN_SRC ipython
<<load_libs>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rh_hour = pd.read_csv(datapath+'preQC/zac_u_hour_relative_humidity.csv', parse_dates = True, index_col=0)


# Outliers
rh_hour = rh_hour.where(rh_hour['rh']<= 100., np.nan)
rh_hour = rh_hour.where(rh_hour['rh']>= 0., np.nan)


# Bad data
startday = datetime.datetime(2020,8,15)
endday = datetime.datetime(2021,7,21)
rh_hour['2020-August-15':'2021-July-21'] = np.nan

count_hours = rh_hour.resample('D').count()
rh_day = rh_hour.resample('D').mean()
rh_day[count_hours<24 ] = np.nan

rh_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_relative_humidity.csv', index = True, float_format = '%g')
rh_day.to_csv('data_v1.0/gem_database/2022/zac_u_day_relative_humidity.csv', index = True, float_format = '%g')
#+END_SRC

#+RESULTS:
:results:
# Out [72]: 
:end:




***** Wind speed
#+BEGIN_SRC ipython
<<load_libs>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
wspd_hour = pd.read_csv(datapath+'preQC/zac_u_hour_wind_speed.csv', parse_dates = True, index_col=0)


# Bad data
startday = datetime.datetime(2020,8,15)
endday = datetime.datetime(2022,4,21)
wspd_hour['2020-August-15':'2022-July-21'] = np.nan

count_hours = wspd_hour.resample('D').count()
wspd_day = wspd_hour.resample('D').mean()
wspd_day[count_hours<24 ] = np.nan

wspd_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_wind_speed.csv', index = True, float_format = '%g')
wspd_day.to_csv('data_v1.0/gem_database/2022/zac_u_day_wind_speed.csv', index = True, float_format = '%g')
#+END_SRC

#+RESULTS:
:results:
# Out [157]: 
:end:


***** Pressure
#+BEGIN_SRC ipython
<<load_libs>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
p_hour = pd.read_csv(datapath+'preQC/zac_u_hour_pressure.csv', parse_dates = True, index_col=0)

#Outliers

p_hour = p_hour.where(p_hour['p']> 850., np.nan)

# Bad data
p_hour['2016-April-4':'2016-April-18'] = np.nan
p_hour['2017-January-23':'2017-March-2'] = np.nan


count_hours = p_hour.resample('D').count()
p_day = p_hour.resample('D').mean()
p_day[count_hours<24 ] = np.nan

p_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_pressure.csv', index = True, float_format = '%g')
p_day.to_csv('data_v1.0/gem_database/2022/zac_u_day_pressure.csv', index = True, float_format = '%g')
#+END_SRC

#+RESULTS:
:results:
# Out [143]: 
:end:

** QC


*** temperature
**** Utilities
#+NAME: plot_gradients_full_period
#+BEGIN_SRC ipython
fig,ax = plt.subplots(4,1,figsize = (10,10), sharex=True)
temp_l[variable].plot(ax = ax[0], label = 'zac_l')
temp_u[variable].plot(ax = ax[0], label = 'zac_u')
temp_a[variable].plot(ax = ax[0], label = 'zac_a')
ax[0].legend()

d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)

d_l_u[variable].plot(ax=ax[1], label = 'zac_l minus zac_u')
d_u_a[variable].plot(ax=ax[2], label = 'zac_u minus zac_a')
d_l_a[variable].plot(ax=ax[3], label = 'zac_l minus zac_a')
ax[1].legend()
ax[2].legend()
ax[3].legend()

#+END_SRC

#+NAME: plot_the_timeperiod
#+BEGIN_SRC ipython
# The year with the problem
fig,ax = plt.subplots(4,2,figsize = (12,10))
temp_l[variable].plot(ax = ax[0,1], label = 'zac_l')
temp_u[variable].plot(ax = ax[0,1], label = 'zac_u')
temp_a[variable].plot(ax = ax[0,1], label = 'zac_a')
ax[0,1].legend()
ax[0,1].set_title(timeperiod1_title)
ax[0,1].set_xlim(timeperiod1)

d_l_u[variable].plot(ax=ax[1,1], label = 'zac_l minus zac_u')
d_u_a[variable].plot(ax=ax[2,1], label = 'zac_u minus zac_a')
d_l_a[variable].plot(ax=ax[3,1], label = 'zac_l minus zac_a')
ymin,ymax = ax[0,1].get_ylim()
ax[0,1].vlines(date_of_interest1,ymin,ymax, color = 'gray', linestyle = '--')
ax[0,1].vlines(date_of_interest2,ymin,ymax, color = 'gray', linestyle = '--')
ymin,ymax = ax[1,1].get_ylim()
ax[1,1].vlines(date_of_interest1,ymin,ymax, color = 'gray', linestyle = '--')
ax[1,1].vlines(date_of_interest2,ymin,ymax, color = 'gray', linestyle = '--')
ymin,ymax = ax[2,1].get_ylim()
ax[2,1].vlines(date_of_interest1,ymin,ymax, color = 'gray', linestyle = '--')
ax[2,1].vlines(date_of_interest2,ymin,ymax, color = 'gray', linestyle = '--')

ymin,ymax = ax[3,1].get_ylim()
ax[3,1].vlines(date_of_interest1,ymin,ymax, color = 'gray', linestyle = '--')
ax[3,1].vlines(date_of_interest2,ymin,ymax, color = 'gray', linestyle = '--')
ax[1,1].legend()
ax[2,1].legend()
ax[3,1].legend()
ax[1,1].set_xlim(timeperiod1)
ax[2,1].set_xlim(timeperiod1)
ax[3,1].set_xlim(timeperiod1)
ax[0,1].set_ylim(-40,20)
#ax[1,1].set_ylim(-0.05,0.05)
#ax[2,1].set_ylim(-0.05,0.05)
#ax[3,1].set_ylim(-0.05,0.05)


# The previous year for reference
temp_l[variable].plot(ax = ax[0,0], label = 'zac_l')
temp_u[variable].plot(ax = ax[0,0], label = 'zac_u')
temp_a[variable].plot(ax = ax[0,0], label = 'zac_a')
ax[0,0].legend()
ax[0,0].set_title(timeperiod2_title)

ax[0,0].set_xlim(timeperiod2)



d_l_u[variable].plot(ax=ax[1,0], label = 'zac_l minus zac_u')
d_u_a[variable].plot(ax=ax[2,0], label = 'zac_u minus zac_a')
d_l_a[variable].plot(ax=ax[3,0], label = 'zac_l minus zac_a')
ax[1,0].legend()
ax[2,0].legend()
ax[3,0].legend()
ax[1,0].set_xlim(timeperiod2)
ax[2,0].set_xlim(timeperiod2)
ax[3,0].set_xlim(timeperiod2)

ax[0,0].set_ylim(-40,20)
#ax[1,0].set_ylim(-0.05,0.05)
#ax[2,0].set_ylim(-0.05,0.05)
#ax[3,0].set_ylim(-0.05,0.05)




#+END_SRC



**** post QC check
Gradients

#+BEGIN_SRC ipython
<<import_libraries>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

temp_l = pd.read_csv(datapath+'zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'zac_a_hour_temperature.csv', parse_dates = True, index_col=0)

<<plot_gradients_full_period>>

temp_l = pd.read_csv(datapath+'zac_l_day_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'zac_u_day_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'zac_a_day_temperature.csv', parse_dates = True, index_col=0)

<<plot_gradients_full_period>>


#+END_SRC

#+RESULTS:
:results:
# Out [220]: 


# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/b4925957e1f50717159c756b730b9a43173a04df.png]]

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/7dd8cd19221673d809fae0188fc1676f9bbc8260.png]]
:end:


**** QC
Gradients

#+BEGIN_SRC ipython
<<import_libraries>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
variable = 't_1'
temp_l = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)

<<plot_gradients_full_period>>

temp_l = pd.read_csv(datapath+'preQC/zac_l_day_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_day_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_day_temperature.csv', parse_dates = True, index_col=0)

<<plot_gradients_full_period>>

#+END_SRC

#+RESULTS:
:results:
# Out [186]: 


# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/1888e82c18c583a9ca4794893cf44b28feac7802.png]]

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/76b53e9854be00a5f6d1087ab34f7546b6d328e3.png]]
:end:




There are issues with zac_u in winter 2014/2015 and likely with zac_l in winter 2020/2021








**** The issue at zac_u in 2020/2021

I will remove both data from zac_u between 2010-10-01 and 2021-07-01

#+BEGIN_SRC ipython
<<import_libraries>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

temp_l = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)


timeperiod1_title = '2020-01-01 to 2021-12-31'
timeperiod1 = (datetime.datetime(2020,1,1), datetime.datetime(2021,12,31))
timeperiod2_title = '2019-01-01 to 2020-12-31'
timeperiod2 = (datetime.datetime(2019,1,1), datetime.datetime(2020,12,31))
date_of_interest1=datetime.datetime(2020,9,15)
date_of_interest2=datetime.datetime(2021,8,1)

<<plot_the_timeperiod>>

temp_l = pd.read_csv(datapath+'preQC/zac_l_day_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_day_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_day_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)


<<plot_the_timeperiod>>


#+END_SRC

#+RESULTS:
:results:
# Out [217]: 
# text/plain
: (-40.0, 20.0)

# text/plain
: <Figure size 864x720 with 8 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/f0efb8e1eac0183e6579cc4f965279a8a203ffe8.png]]

# text/plain
: <Figure size 864x720 with 8 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/cf4d5635c652e282d85448d4756162a2c0c874f3.png]]
:end:




**** The issue at zac_a in 2015

The data from zac_a between 2015-01-05 to 2015-05-01 looks strange and will be removed

#+BEGIN_SRC ipython
<<import_libraries>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

# meta data
variable = 't_1'
zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

temp_l = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)

timeperiod1_title = '2014-11-1 to 2015-10-15'
timeperiod1 = (datetime.datetime(2014,11,1),datetime.datetime(2015,10,15))
timeperiod2_title = '2013-11-1 to 2014-10-15'
timeperiod2 = (datetime.datetime(2013,11,1),datetime.datetime(2014,10,15))
date_of_interest1=datetime.datetime(2015,1,5)
date_of_interest2=datetime.datetime(2015,5,1)

<<plot_the_timeperiod>>

temp_l = pd.read_csv(datapath+'preQC/zac_l_day_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_day_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_day_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)


<<plot_the_timeperiod>>

#+END_SRC

#+RESULTS:
:results:
# Out [197]: 
# text/plain
: (-40.0, 20.0)

# text/plain
: <Figure size 864x720 with 8 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/f815913459035854589ede4a846461cc22813b42.png]]

# text/plain
: <Figure size 864x720 with 8 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/6258f58b61585fe481916e0c2d6a901680037bdc.png]]
:end:


#+NAME: plot_the_2015_issue_at_zac_l
#+BEGIN_SRC ipython
# The year with the problem
fig,ax = plt.subplots(4,2,figsize = (12,10))
temp_l['Air temperature, C'].plot(ax = ax[0,1], label = 'zac_l')
temp_u['Air temperature, C'].plot(ax = ax[0,1], label = 'zac_u')
temp_a['Air temperature, C'].plot(ax = ax[0,1], label = 'zac_a')
ax[0,1].legend()
ax[0,1].set_title('2015-1-1 to 2015-5-15')
ax[0,1].set_xlim(datetime.datetime(2015,1,1),datetime.datetime(2015,5,15))

d_l_u['Air temperature, C'].plot(ax=ax[1,1], label = 'zac_l minus zac_u')
d_u_a['Air temperature, C'].plot(ax=ax[2,1], label = 'zac_u minus zac_a')
d_l_a['Air temperature, C'].plot(ax=ax[3,1], label = 'zac_l minus zac_a')
ymin,ymax = ax[3,1].get_ylim()
ax[3,1].vlines(datetime.datetime(2015,1,12),ymin,ymax)
ax[1,1].legend()
ax[2,1].legend()
ax[3,1].legend()
ax[1,1].set_xlim(datetime.datetime(2015,1,1),datetime.datetime(2015,5,15))
ax[2,1].set_xlim(datetime.datetime(2015,1,1),datetime.datetime(2015,5,15))
ax[3,1].set_xlim(datetime.datetime(2015,1,1),datetime.datetime(2015,5,15))
ax[0,1].set_ylim(-40,20)
#ax[1,1].set_ylim(-0.05,0.05)
#ax[2,1].set_ylim(-0.05,0.05)
#ax[3,1].set_ylim(-0.05,0.05)


# The previous year for reference
temp_l['Air temperature, C'].plot(ax = ax[0,0], label = 'zac_l')
temp_u['Air temperature, C'].plot(ax = ax[0,0], label = 'zac_u')
temp_a['Air temperature, C'].plot(ax = ax[0,0], label = 'zac_a')
ax[0,0].legend()
ax[0,0].set_title('2013-1-1 to 2013-5-12')
timeperiod = (datetime.datetime(2013,1,1),datetime.datetime(2013,5,12))
ax[0,0].set_xlim(timeperiod)



d_l_u['Air temperature, C'].plot(ax=ax[1,0], label = 'zac_l minus zac_u')
d_u_a['Air temperature, C'].plot(ax=ax[2,0], label = 'zac_u minus zac_a')
d_l_a['Air temperature, C'].plot(ax=ax[3,0], label = 'zac_l minus zac_a')
ax[1,0].legend()
ax[2,0].legend()
ax[3,0].legend()
ax[1,0].set_xlim(timeperiod)
ax[2,0].set_xlim(timeperiod)
ax[3,0].set_xlim(timeperiod)

ax[0,0].set_ylim(-40,20)
#ax[1,0].set_ylim(-0.05,0.05)
#ax[2,0].set_ylim(-0.05,0.05)
#ax[3,0].set_ylim(-0.05,0.05)




#+END_SRC
**** The issue at zac_a beginning of record
We remove data from before 2009-08-08 21:00
#+BEGIN_SRC ipython
<<import_libraries>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

# meta data
variable = 't_1'
zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

temp_l = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)

timeperiod1_title = '2009-8-1 to 2009-9-1'
timeperiod1 = (datetime.datetime(2009,8,5),datetime.datetime(2009,8,6))
timeperiod2_title = '2010-8-1 to 2010-9-1'
timeperiod2 = (datetime.datetime(2010,8,5),datetime.datetime(2010,8,6))
date_of_interest1=datetime.datetime(2015,1,5)
date_of_interest2=datetime.datetime(2015,5,1)

<<plot_the_timeperiod>>

#+END_SRC

#+RESULTS:
:results:
# Out [204]: 
# text/plain
: (-40.0, 20.0)

# text/plain
: <Figure size 864x720 with 8 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/52fcca926023e03aeafeff972491057f21667832.png]]
:end:

**** The issue at zac_u in 2015

The conclusion from the below investigation is that we will discard the zac_u data from 2015, the fan must have stopped running
The excat period that can be discarded at zac_u: 2014-10-30 to 2015-12-31
Hourly
#+BEGIN_SRC ipython
<<import_libraries>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
# meta data
variable = 't_1'
zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

temp_l = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)

<<plot_the_2015_issue_at_zac_u>>

temp_l = pd.read_csv(datapath+'preQC/zac_l_day_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_day_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_day_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)

<<plot_the_2015_issue_at_zac_u>>


#+END_SRC

#+RESULTS:
:results:
# Out [210]: 
# text/plain
: (-40.0, 20.0)

# text/plain
: <Figure size 864x720 with 8 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/bdf68acbc5ed059722c0bc40d9f930c298347c5d.png]]

# text/plain
: <Figure size 864x720 with 8 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/7a308c6d85ebcf64b1e75fd9fd3a9b2b0280ab30.png]]
:end:

#+NAME: plot_the_2015_issue_at_zac_u
#+BEGIN_SRC ipython
# The year with the problem
fig,ax = plt.subplots(4,2,figsize = (12,10))
temp_l[variable].plot(ax = ax[0,1], label = 'zac_l')
temp_u[variable].plot(ax = ax[0,1], label = 'zac_u')
temp_a[variable].plot(ax = ax[0,1], label = 'zac_a')
ax[0,1].legend()
ax[0,1].set_title('2014-10-30 to 2015-12-31')
ax[0,1].set_xlim(datetime.datetime(2014,10,30),datetime.datetime(2015,12,31))



d_l_u[variable].plot(ax=ax[1,1], label = 'zac_l minus zac_u')
d_u_a[variable].plot(ax=ax[2,1], label = 'zac_u minus zac_a')
d_l_a[variable].plot(ax=ax[3,1], label = 'zac_l minus zac_a')
ax[1,1].legend()
ax[2,1].legend()
ax[3,1].legend()
ax[1,1].set_xlim(datetime.datetime(2014,10,30),datetime.datetime(2015,12,31))
ax[2,1].set_xlim(datetime.datetime(2014,10,30),datetime.datetime(2015,12,31))
ax[3,1].set_xlim(datetime.datetime(2014,10,30),datetime.datetime(2015,12,31))
ax[0,1].set_ylim(-40,20)
#ax[1,1].set_ylim(-0.05,0.05)
#ax[2,1].set_ylim(-0.05,0.05)
#ax[3,1].set_ylim(-0.05,0.05)


# The previous year for reference
temp_l[variable].plot(ax = ax[0,0], label = 'zac_l')
temp_u[variable].plot(ax = ax[0,0], label = 'zac_u')
temp_a[variable].plot(ax = ax[0,0], label = 'zac_a')
ax[0,0].legend()
ax[0,0].set_title('2013-10-30 to 2014-12-31')
ax[0,0].set_xlim(datetime.datetime(2013,10,30),datetime.datetime(2014,12,31))



d_l_u[variable].plot(ax=ax[1,0], label = 'zac_l minus zac_u')
d_u_a[variable].plot(ax=ax[2,0], label = 'zac_u minus zac_a')
d_l_a[variable].plot(ax=ax[3,0], label = 'zac_l minus zac_a')
ax[1,0].legend()
ax[2,0].legend()
ax[3,0].legend()
ax[1,0].set_xlim(datetime.datetime(2013,10,30),datetime.datetime(2014,12,31))
ax[2,0].set_xlim(datetime.datetime(2013,10,30),datetime.datetime(2014,12,31))
ax[3,0].set_xlim(datetime.datetime(2013,10,30),datetime.datetime(2014,12,31))

ax[0,0].set_ylim(-40,20)
#ax[1,0].set_ylim(-0.05,0.05)
#ax[2,0].set_ylim(-0.05,0.05)
#ax[3,0].set_ylim(-0.05,0.05)




#+END_SRC

#+RESULTS: plot_the_2015_issue_at_zac_u
:results:
# Out [208]: 
# output
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2888             try:
-> 2889                 return self._engine.get_loc(casted_key)
   2890             except KeyError as err:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 'Air temperature, C'

The above exception was the direct cause of the following exception:

KeyError                                  Traceback (most recent call last)
/tmp/ipykernel_8873/1461341412.py in <module>
     26 
     27 # The previous year for reference
---> 28 temp_l['Air temperature, C'].plot(ax = ax[0,0], label = 'zac_l')
     29 temp_u['Air temperature, C'].plot(ax = ax[0,0], label = 'zac_u')
     30 temp_a['Air temperature, C'].plot(ax = ax[0,0], label = 'zac_a')

~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/core/frame.py in __getitem__(self, key)
   2897             if self.columns.nlevels > 1:
   2898                 return self._getitem_multilevel(key)
-> 2899             indexer = self.columns.get_loc(key)
   2900             if is_integer(indexer):
   2901                 indexer = [indexer]

~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2889                 return self._engine.get_loc(casted_key)
   2890             except KeyError as err:
-> 2891                 raise KeyError(key) from err
   2892 
   2893         if tolerance is not None:

KeyError: 'Air temperature, C'
# text/plain
: <Figure size 864x720 with 8 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/022b1522d44b039d915d6f19c1c15a943c40a4e1.png]]
:end:

#+RESULTS:
:results:
# Out [74]: 
# text/plain
: (-0.05, 0.05)

# text/plain
: <Figure size 864x720 with 8 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/3de00a68d27b0ae967820a612e0cd6baf7ee3997.png]]
:end:








#+BEGIN_SRC ipython
<<import_libraries>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
temp_l = pd.read_csv(datapath+'preQC/zac_l_day_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_day_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_day_temperature.csv', parse_dates = True, index_col=0)


fig,ax = plt.subplots(4,1,figsize = (10,10), sharex=True)
temp_l['Air temperature, C'].plot(ax = ax[0], label = 'zac_l')
temp_u['Air temperature, C'].plot(ax = ax[0], label = 'zac_u')
temp_a['Air temperature, C'].plot(ax = ax[0], label = 'zac_a')
ax[0].legend()

d_l_u = temp_l-temp_u
d_u_a = temp_u-temp_a
d_l_a = temp_l-temp_a

d_l_u['Air temperature, C'].plot(ax=ax[1], label = 'zac_l minus zac_u')
d_u_a['Air temperature, C'].plot(ax=ax[2], label = 'zac_u minus zac_a')
d_l_a['Air temperature, C'].plot(ax=ax[3], label = 'zac_l minus zac_a')
ax[1].legend()
ax[2].legend()
ax[3].legend()


#+END_SRC

#+RESULTS:
:results:
# Out [23]: 
# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/8fba46d9acf993b9adae626bcb97605b84e58093/eda6a101171a844b87facea93bd9b6a9cbc6467b.png]]
:end:


*** Radiation
**** Utilities
#+NAME: read_in_hourly_preQC_radiation
#+BEGIN_SRC ipython
rad_l = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

hourmax = 1000
maximum = hourmax

rad_l[variable][rad_l[variable]>maximum] = np.nan
rad_l[variable][rad_l[variable]<0] = np.nan

rad_u[variable][rad_u[variable]>maximum] = np.nan
rad_u[variable][rad_u[variable]<0] = np.nan

rad_a[variable][rad_a[variable]>maximum] = np.nan
rad_a[variable][rad_a[variable]<0] = np.nan
#+END_SRC

#+NAME: read_in_hourly_radiation_postQC
#+BEGIN_SRC ipython
rad_l = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

#+END_SRC

#+NAME: read_in_daily_preQC_radiation
#+BEGIN_SRC ipython
rad_l = pd.read_csv(datapath+'preQC/zac_l_day_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'preQC/zac_u_day_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'preQC/zac_a_day_radiation.csv', parse_dates = True, index_col=0)
daymax = 500
maximum = daymax


rad_l[variable][rad_l[variable]>maximum] = np.nan
rad_l[variable][rad_l[variable]<0] = np.nan

rad_u[variable][rad_u[variable]>maximum] = np.nan
rad_u[variable][rad_u[variable]<0] = np.nan

rad_a[variable][rad_a[variable]>maximum] = np.nan
rad_a[variable][rad_a[variable]<0] = np.nan
#+END_SRC


#+NAME: plot_rad_gradients_full_period
#+BEGIN_SRC ipython
fig,ax = plt.subplots(4,1,figsize = (10,10), sharex=True)
rad_l[variable].plot(ax = ax[0], label = 'zac_l')
rad_u[variable].plot(ax = ax[0], label = 'zac_u')
rad_a[variable].plot(ax = ax[0], label = 'zac_a')
ax[0].legend()

d_l_u = (rad_l-rad_u)
d_u_a = (rad_u-rad_a)
d_l_a = (rad_l-rad_a)

d_l_u[variable].plot(ax=ax[1], label = 'zac_l minus zac_u')
d_u_a[variable].plot(ax=ax[2], label = 'zac_u minus zac_a')
d_l_a[variable].plot(ax=ax[3], label = 'zac_l minus zac_a')
ax[1].legend()
ax[2].legend()
ax[3].legend()

#+END_SRC

#+RESULTS: plot_rad_gradients_full_period
:results:
# Out [88]: 


# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/253f13095695a2517675fd67e52fe2676781336a.png]]
:end:

#+NAME: plot_rad_gradients_selected_period
#+BEGIN_SRC ipython
<<plot_rad_gradients_full_period>>
ax[3].set_xlim(startdate,enddate)
#+END_SRC

**** Post QC check
#+BEGIN_SRC ipython
<<load_libs>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
variable = 'ulr'
rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

<<plot_rad_gradients_full_period>>
#+END_SRC

#+RESULTS:
:results:
# Out [3]: 


# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/0e956d142e5f005a49e0b4559b6548495c7f39f3.png]]
:end:


**** QC
***** Outliers

#+BEGIN_SRC ipython
<<load_libs>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)


fig,ax = plt.subplots(2,1,figsize = (10,5))
rad_l[['dsr','usr']].plot(ax = ax[0])
rad_l[['dlr','ulr']].plot(ax = ax[1])

fig,ax = plt.subplots(2,1,figsize = (10,5))
rad_u[['dsr','usr']].plot(ax = ax[0])
rad_u[['dlr','ulr']].plot(ax = ax[1])

fig,ax = plt.subplots(2,1,figsize = (10,5))
rad_a[['dsr','usr']].plot(ax = ax[0])
rad_a[['dlr','ulr']].plot(ax = ax[1])

#+END_SRC

#+RESULTS:
:results:
# Out [14]: 
# text/plain
: <AxesSubplot:xlabel='time'>

# text/plain
: <Figure size 720x360 with 2 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/9d969ffbf1f5c6509dd7f18378084c471333bdd2.png]]

# text/plain
: <Figure size 720x360 with 2 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/fcb76ad8ec27fde21da54c85520f25798ec4f496.png]]

# text/plain
: <Figure size 720x360 with 2 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/776cd80655eaef77886b24fc476be8b6b8ede3f0.png]]
:end:


***** Annual mean
#+BEGIN_SRC ipython
<<import_libraries>>

#+END_SRC


I have a suspicion that the radiometers calibration is not really good enough
But it looks like that dsr was just higher during 2016-2019 or something like that?

#+BEGIN_SRC ipython
<<load_libs>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)
mast_dsr = pd.read_csv('/home/shl/data/gem/climatebasis/climate_mast/View_ClimateBasis_Zackenberg_Data_Radiation_Short_wave_incoming_radiation_@_200_cm__5min_average_W_m2270920221347180965.csv', sep = '\t', index_col = 0, parse_dates = {'date':['Date','Time']}, na_values = -9999)

variable = 'dsr'
fig, ax = plt.subplots(1,1, figsize = (7,7))
rad_l[variable].resample('Y').mean().plot(ax = ax)
rad_a[variable].resample('Y').mean().plot(ax = ax)
rad_u[variable].resample('Y').mean().plot(ax = ax)
mast_dsr.resample('Y').mean().plot(ax=ax)
#ax1 = ax[0].twinx()
#rad_l[variable].resample('Y').count().plot(ax=ax1, linestyle = '--')

#+END_SRC

#+RESULTS:
:results:
# Out [9]: 
# text/plain
: <AxesSubplot:xlabel='date'>

# text/plain
: <Figure size 504x504 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/00120a04d0569520b9c3d55e72c35207f3d2768a.png]]
:end:



***** Gradients
     #+BEGIN_SRC ipython
<<import_libraries>>
# meta data

#zac_l_elev = 644
#zac_u_elev = 877
#zac_a_elev = 1477

datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
variable = 'usr'

datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

#<<read_in_hourly_preQC_radiation>>
#<<plot_rad_gradients_full_period>>
startdate=datetime.datetime(2008,1,1)
enddate= datetime.datetime(2008,12,31)
# We will remove 2011-03-15 13:00 to 2011-03-15 23:00

d_l_u = (rad_l-rad_u)
d_u_a = (rad_u-rad_a)
d_l_a = (rad_l-rad_a)


for year in range(2008, 2022+1):
    fig,ax = plt.subplots(4,1,figsize = (10,10), sharex=True)
    if year >= 2009 and year <2020:
        rad_a[variable][str(year)].plot(ax = ax[0], label = 'zac_a')
    ax[0].legend()
    ax[0].set_ylim(0,600)
    rad_u[variable][str(year)].plot(ax = ax[0], label = 'zac_u')
    rad_l[variable][str(year)].plot(ax = ax[0], label = 'zac_l')
    
    


    d_l_u[variable][str(year)].plot(ax=ax[1], label = 'zac_l minus zac_u')
    d_u_a[variable][str(year)].plot(ax=ax[2], label = 'zac_u minus zac_a')
    d_l_a[variable][str(year)].plot(ax=ax[3], label = 'zac_l minus zac_a')
    ax[1].legend()
    ax[2].legend()
    ax[3].legend()
    fig.savefig('QCfigs/'+variable+'_'+str(year)+'.png')


#+END_SRC

#+RESULTS:
:results:
# Out [14]: 
# output
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/7658ded01529ef3eaed96ff44c9111b9a796c959.png]]

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/6f44ad2f166b0bfc70b53f07c17c375759b3ddd3.png]]

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/63c9749417148f6cda6ec1ed20f9c901212f0889.png]]

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/9d46b48f863147bcfa6613ac4460376dfb9527ec.png]]

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/866e3a6984ea107111e2f19a49752d07b0028ed6.png]]

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/85083f8268296f46a192ec68f586dc547be29601.png]]

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/e939a6a5efed22f559086a53d3847bc6165428fb.png]]

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/35076af8da6b03eaacde2cfb55bc4c3d88671bf3.png]]

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/d381935388acaed0080f21b7aff78e46c9570508.png]]

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/b8e3ef7a3336ffdb91b7b0070446a8b8edfd2597.png]]

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/35f29d9e3d2859d7de42c31ba8ea8c60f50b9e3c.png]]

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/886126b6f07739d97ebabcc34d99a8effc919446.png]]

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/f83ad677b1666b1181e062bffc6f77d31d574027.png]]

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/ec5f5ddf4d1c81f4b9831ecda42c38dd22885d17.png]]

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/627b60d20663d3263f89ae834a77b19429e1df64.png]]
:end:

**** The issue from setting up zac_a

#+BEGIN_SRC ipython
<<read_in_hourly_radiation>>
startdate=datetime.datetime(2009,8,5)
enddate= datetime.datetime(2009,8,6)
# Startdate of record: 2009-08-06 21:00
<<plot_rad_gradients_selected_period>>
#+END_SRC

#+RESULTS:
:results:
# Out [102]: 
# text/plain
: (347064.0, 347088.0)

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/b890a52128f9867bd50d60a9eaa93448c2c3da7c.png]]
:end:



**** The visit in 2010
#+BEGIN_SRC ipython
<<read_in_hourly_radiation>>
startdate=datetime.datetime(2010,5,10)
enddate= datetime.datetime(2010,5,17)
<<plot_rad_gradients_selected_period>>
#+END_SRC

#+RESULTS:
:results:
# Out [155]: 
# text/plain
: (353736.0, 353904.0)

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/9c1de3d847d37cc37f9ac270a1f1f025cd5dee96.png]]
:end:


**** The several issues at zac_a in early 2011
#+BEGIN_SRC ipython
<<read_in_hourly_radiation>>
startdate=datetime.datetime(2011,3,14)
enddate= datetime.datetime(2011,3,17)
# We will remove 2011-03-15 13:00 to 2011-03-15 23:00
<<plot_rad_gradients_selected_period>>
#+END_SRC

#+RESULTS:
:results:
# Out [118]: 
# text/plain
: (361128.0, 361200.0)

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/0f573b6bb5939ca316b05244438a76481c0b79cb.png]]
:end:

#+BEGIN_SRC ipython
<<read_in_hourly_radiation>>
#<<read_in_hourly_radiation_postQC>>
startdate=datetime.datetime(2011,5,4)
enddate= datetime.datetime(2011,5,6)
# We will remove 2011-05-04 12:00 to 2011-05-05 17:00
<<plot_rad_gradients_selected_period>>
#+END_SRC

#+RESULTS:
:results:
# Out [130]: 
# text/plain
: (362352.0, 362400.0)

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/8e563233c1094ed78915f3288c4dd30548c86d6b.png]]
:end:

#+BEGIN_SRC ipython
<<read_in_hourly_radiation>>
#<<read_in_hourly_radiation_postQC>>
startdate=datetime.datetime(2011,1,1)
enddate= datetime.datetime(2011,11,1)
# We will remove 2011-05-04 12:00 to 2011-05-05 17:00
<<plot_rad_gradients_selected_period>>
#+END_SRC

#+RESULTS:
:results:
# Out [134]: 
# text/plain
: (359400.0, 366696.0)

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/1e6c9133963dd7d9acd5dc10bc6b43a394f544d6.png]]
:end:


**** Zac l in 2021 before the visit

the dsr values look suspecious, maybe the station has been too tilted or the instrument has problems. I will in any case remove the data up until the visit i July
Have a look at the annual figures in QCfigs


*** Relative humidity

Load in the data
#+BEGIN_SRC ipython
<<load_libs>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rh_l = pd.read_csv(datapath+'zac_l_hour_relative_humidity.csv', parse_dates = True, index_col=0)
rh_u = pd.read_csv(datapath+'zac_u_hour_relative_humidity.csv', parse_dates = True, index_col=0)
rh_a = pd.read_csv(datapath+'zac_a_hour_relative_humidity.csv', parse_dates = True, index_col=0)

#+END_SRC

#+RESULTS:
:results:
# Out [58]: 
:end:


**** Checking for outliers
#+BEGIN_SRC ipython
fig, ax = plt.subplots(3,1,figsize=(10,10))
rh_l.plot(ax= ax[0])
rh_u.plot(ax= ax[1])
rh_a.plot(ax= ax[2])


#+END_SRC

#+RESULTS:
:results:
# Out [24]: 
# text/plain
: <AxesSubplot:xlabel='time'>

# text/plain
: <Figure size 720x720 with 3 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/aac3e258e7140322be07d180636f64957abc7531.png]]
:end:

**** Gradients
#+BEGIN_SRC ipython
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (rh_l-rh_u)
d_u_a = (rh_u-rh_a)
d_l_a = (rh_l-rh_a)

rh_l['rh'].plot(ax= ax[0], label = 'zac_l')
rh_u['rh'].plot(ax= ax[0], label = 'zac_u')
rh_a['rh'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])


#+END_SRC

#+RESULTS:
:results:
# Out [45]: 
# text/plain
: <AxesSubplot:xlabel='time'>

# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/d00952effe542dc8ef228fc974256a0725e632b9.png]]
:end:

***** The issue at zac_u in 2020-2021

#+BEGIN_SRC ipython
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (rh_l-rh_u)
d_u_a = (rh_u-rh_a)
d_l_a = (rh_l-rh_a)

rh_l['rh'].plot(ax= ax[0], label = 'zac_l')
rh_u['rh'].plot(ax= ax[0], label = 'zac_u')
rh_a['rh'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2020,1,1),datetime.datetime(2021,9,1))
startday = datetime.datetime(2020,8,15)
endday = datetime.datetime(2021,7,21)
ymin,ymax = 0, 100
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC

#+RESULTS:
:results:
# Out [42]: 


# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/41832a329cabf6a88232d57333b9d28a69a9566b.png]]
:end:


***** zac_a data appears to be a bit more unstable - but maybe its real

We will take a look at 2013-2014
#+BEGIN_SRC ipython
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (rh_l-rh_u)
d_u_a = (rh_u-rh_a)
d_l_a = (rh_l-rh_a)

rh_l['rh'].plot(ax= ax[0], label = 'zac_l')
rh_u['rh'].plot(ax= ax[0], label = 'zac_u')
rh_a['rh'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2013,1,1),datetime.datetime(2014,12,1))
startday = datetime.datetime(2013,1,1)
endday = datetime.datetime(2014,12,1)
ymin,ymax = 0, 100
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC

#+RESULTS:
:results:
# Out [47]: 


# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/7321fb67ebcbb97aa41988457b4399c6c263b516.png]]
:end:


*** Windspeed



#+BEGIN_SRC ipython
<<load_libs>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
wspd_l = pd.read_csv(datapath+'zac_l_hour_wind_speed.csv', parse_dates = True, index_col=0)
wspd_u = pd.read_csv(datapath+'zac_u_hour_wind_speed.csv', parse_dates = True, index_col=0)
wspd_a = pd.read_csv(datapath+'zac_a_hour_wind_speed.csv', parse_dates = True, index_col=0)

#+END_SRC

#+RESULTS:
:results:
# Out [79]: 
:end:


**** Checking for outliers

It seems like windspeed is pretty solid
#+BEGIN_SRC ipython
fig, ax = plt.subplots(3,1,figsize=(10,10))
wspd_l.plot(ax= ax[0])
wspd_u.plot(ax= ax[1])
wspd_a.plot(ax= ax[2])


#+END_SRC

#+RESULTS:
:results:
# Out [80]: 
# text/plain
: <AxesSubplot:xlabel='time'>

# text/plain
: <Figure size 720x720 with 3 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/e75519e218cd7236a01cda12921e29f0908219ff.png]]
:end:

**** Gradients
#+BEGIN_SRC ipython
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (wspd_l-wspd_u)
d_u_a = (wspd_u-wspd_a)
d_l_a = (wspd_l-wspd_a)

wspd_l['wspd'].plot(ax= ax[0], label = 'zac_l')
wspd_u['wspd'].plot(ax= ax[0], label = 'zac_u')
wspd_a['wspd'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])


#+END_SRC

#+RESULTS:
:results:
# Out [81]: 
# text/plain
: <AxesSubplot:xlabel='time'>

# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/974a9d31e0f76cd890c2907d729c3e1707ed39ee.png]]
:end:

***** The issue at zac_u in 2020 (station tilting)

#+BEGIN_SRC ipython
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (wspd_l-wspd_u)
d_u_a = (wspd_u-wspd_a)
d_l_a = (wspd_l-wspd_a)

wspd_l['wspd'].plot(ax= ax[0], label = 'zac_l')
wspd_u['wspd'].plot(ax= ax[0], label = 'zac_u')
wspd_a['wspd'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2020,1,1),datetime.datetime(2022,4,25))
startday = datetime.datetime(2020,8,15)
endday = datetime.datetime(2022,4,21)
ymin,ymax = ax[0].get_ylim()
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC

#+RESULTS:
:results:
# Out [68]: 


# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/a39c5117508c93e4bf5c2165c10d9c9dcff8def8.png]]
:end:



*** Pressure



#+BEGIN_SRC ipython
<<load_libs>>
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
p_l = pd.read_csv(datapath+'zac_l_hour_pressure.csv', parse_dates = True, index_col=0)
p_u = pd.read_csv(datapath+'zac_u_hour_pressure.csv', parse_dates = True, index_col=0)
p_a = pd.read_csv(datapath+'zac_a_hour_pressure.csv', parse_dates = True, index_col=0)

#+END_SRC

#+RESULTS:
:results:
# Out [154]: 
:end:


**** Checking for outliers

It seems like windspeed is pretty solid
#+BEGIN_SRC ipython
fig, ax = plt.subplots(3,1,figsize=(10,10))
p_l.plot(ax= ax[0])
p_u.plot(ax= ax[1])
p_a.plot(ax= ax[2])


#+END_SRC

#+RESULTS:
:results:
# Out [100]: 
# text/plain
: <AxesSubplot:xlabel='time'>

# text/plain
: <Figure size 720x720 with 3 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/10c09556fcc02dcbb9723a5fbc90d4f86f5fffec.png]]
:end:

**** Gradients
#+BEGIN_SRC ipython
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (p_l-p_u)
d_u_a = (p_u-p_a)
d_l_a = (p_l-p_a)

p_l['p'].plot(ax= ax[0], label = 'zac_l')
p_u['p'].plot(ax= ax[0], label = 'zac_u')
p_a['p'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])


#+END_SRC

#+RESULTS:
:results:
# Out [155]: 
# text/plain
: <AxesSubplot:xlabel='time'>

# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/4b8aea1ce833ce11c0350d163ab28386dd6d5d75.png]]
:end:

***** The issue in 2016 at zac_l
#+BEGIN_SRC ipython
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (p_l-p_u)
d_u_a = (p_u-p_a)
d_l_a = (p_l-p_a)

p_l['p'].plot(ax= ax[0], label = 'zac_l')
p_u['p'].plot(ax= ax[0], label = 'zac_u')
p_a['p'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2016,1,1),datetime.datetime(2016,5,1))
startday = datetime.datetime(2016,2,26)
endday = datetime.datetime(2016,3,1)
ymin,ymax = ax[0].get_ylim()
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[2].get_ylim()
ax[2].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[3].get_ylim()
ax[3].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC

#+RESULTS:
:results:
# Out [109]: 


# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/7c0aa8ea276757a4ca76744be0a8311a47019db2.png]]
:end:





***** The issue in 2016 at zac_u
#+BEGIN_SRC ipython
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (p_l-p_u)
d_u_a = (p_u-p_a)
d_l_a = (p_l-p_a)

p_l['p'].plot(ax= ax[0], label = 'zac_l')
p_u['p'].plot(ax= ax[0], label = 'zac_u')
p_a['p'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2016,1,1),datetime.datetime(2016,5,1))
startday = datetime.datetime(2016,4,5)
endday = datetime.datetime(2016,4,18)
ymin,ymax = ax[0].get_ylim()
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[2].get_ylim()
ax[2].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[3].get_ylim()
ax[3].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC

#+RESULTS:
:results:
# Out [118]: 


# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/d3937292b5aea274f7b8ca43e4737dda4266deff.png]]
:end:






***** The issue at zac_l in 2016-2017
#+BEGIN_SRC ipython
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (p_l-p_u)
d_u_a = (p_u-p_a)
d_l_a = (p_l-p_a)

p_l['p'].plot(ax= ax[0], label = 'zac_l')
p_u['p'].plot(ax= ax[0], label = 'zac_u')
p_a['p'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2016,10,1),datetime.datetime(2017,4,1))
startday = datetime.datetime(2017,1,5)
endday = datetime.datetime(2017,2,22)
ymin,ymax = ax[0].get_ylim()
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[2].get_ylim()
ax[2].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[3].get_ylim()
ax[3].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC

#+RESULTS:
:results:
# Out [127]: 


# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/49166008e08a68fc1c133f8022711194d5d87bea.png]]
:end:

***** The issue at zac_u in 2016-2017
#+BEGIN_SRC ipython
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (p_l-p_u)
d_u_a = (p_u-p_a)
d_l_a = (p_l-p_a)

p_l['p'].plot(ax= ax[0], label = 'zac_l')
p_u['p'].plot(ax= ax[0], label = 'zac_u')
p_a['p'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2016,10,1),datetime.datetime(2017,4,1))
startday = datetime.datetime(2017,1,23)
endday = datetime.datetime(2017,3,2)
ymin,ymax = ax[0].get_ylim()
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[2].get_ylim()
ax[2].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[3].get_ylim()
ax[3].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC

#+RESULTS:
:results:
# Out [142]: 


# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/46e76368edde8796fa2cc114dee3e7daec7b4c86.png]]
:end:


***** The issue at zac_l in 2018
#+BEGIN_SRC ipython
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (p_l-p_u)
d_u_a = (p_u-p_a)
d_l_a = (p_l-p_a)

p_l['p'].plot(ax= ax[0], label = 'zac_l')
p_u['p'].plot(ax= ax[0], label = 'zac_u')
p_a['p'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2018,1,1),datetime.datetime(2018,6,1))
startday = datetime.datetime(2018,2,21)
endday = datetime.datetime(2018,2,28)
ymin,ymax = ax[0].get_ylim()
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[2].get_ylim()
ax[2].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[3].get_ylim()
ax[3].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC

#+RESULTS:
:results:
# Out [152]: 


# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/ffbfb9760db97528ddbf21a1f29c810d4cd4c0ed.png]]
:end:


** Utilities


#+NAME: data_file_paths
#+BEGIN_SRC ipython
station = 'zac_l'
filename = 'zac_l-2008-2022.nc'
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/'+station+'/'
zac_l_path = datapath+filename

station = 'zac_u'
filename = 'zac_u-2008-2022.nc'
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/'+station+'/'
zac_u_path = datapath+filename

station = 'zac_a'
filename = 'zac_a-2009-2020.nc'
datapath = '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/'+station+'/'
zac_a_path = datapath+filename
#+END_SRC

#+NAME: Load_transmitted_data
#+BEGIN_SRC ipython
datadir = '/home/shl/OneDrive/projects/glaciobasis/data/aws_transmitted/'
station = 'zac_a'
filename = 'AWS_300234061218540.txt'
diagnostic = 'AWS_300234061218540-D.txt'
transmitted = pd.read_csv(datadir+station+'/'+filename,header=0,skiprows=[1,2,3],sep=',',engine='python')
transmitted.index = pd.to_datetime(transmitted[' timestamp'])
transmitted = transmitted.drop(' timestamp', axis = 1)
diag = pd.read_csv(datadir+station+'/'+diagnostic,header=0,skiprows=[1],sep=',',engine='python')
#print(data.keys().tolist())
#print(diag.keys())



#+END_SRC

#+NAME: load_transmitted_trusted
#+BEGIN_SRC ipython
<<Load_transmitted_data>>
transmitted = transmitted[:'2019-August-30'].astype(float)


# The transmitted data is divided into timestamps so 
timestep = transmitted.index[1:]-transmitted.index[0:-1]

timestep_hour = pd.DataFrame(timestep.components.hours)
timestep_hour.index =  transmitted.index[0:-1]
transmitted_hour = transmitted[0:-1][(timestep_hour==1).values]

timestep_day = timestep.components.days
timestep_day.index =  transmitted.index[0:-1]
transmitted_day = transmitted[0:-1][(timestep_day==1).values]


#+END_SRC

#+RESULTS: load_transmitted_trusted
:results:
# Out [29]: 
# output
                     seconds_since_1990  airpressure  temperature  \
 timestamp                                                          
2019-05-29 17:00:00         927997200.0        849.0       -4.244   
2019-05-29 18:00:00         928000800.0        849.0       -4.488   
2019-05-29 19:00:00         928004400.0        849.0       -4.897   
2019-05-29 20:00:00         928008000.0        849.0       -4.805   
2019-05-29 21:00:00         928011600.0        849.0       -5.417   
...                                 ...          ...          ...   
2019-08-30 19:00:00         936039600.0        838.0       -4.746   
2019-08-30 20:00:00         936043200.0        838.0       -4.402   
2019-08-30 21:00:00         936046800.0        838.0       -4.266   
2019-08-30 22:00:00         936050400.0        839.0       -4.162   
2019-08-30 23:00:00         936054000.0        839.0       -4.151   

                     temperature2  relativehumidity  windspeed_1  windspeed_2  \
 timestamp                                                                      
2019-05-29 17:00:00        -4.209              87.3        5.432        204.7   
2019-05-29 18:00:00        -4.436              90.0        4.688        200.3   
2019-05-29 19:00:00        -4.853              94.6        3.992        211.9   
2019-05-29 20:00:00        -4.852              91.1        5.808        193.0   
2019-05-29 21:00:00        -5.425              95.8        5.895        192.9   
...                           ...               ...          ...          ...   
2019-08-30 19:00:00        -4.967              99.8        6.051        193.7   
2019-08-30 20:00:00        -4.688              99.8        6.582        202.7   
2019-08-30 21:00:00        -4.525              99.8        7.606        205.9   
2019-08-30 22:00:00        -4.409              99.8        7.408        204.1   
2019-08-30 23:00:00        -4.382              99.8        6.989        202.0   

                     shortwaveradiationin  shortwaveradiationout  \
 timestamp                                                         
2019-05-29 17:00:00               445.300                426.300   
2019-05-29 18:00:00               358.600                345.600   
2019-05-29 19:00:00               287.100                273.000   
2019-05-29 20:00:00               408.900                361.500   
2019-05-29 21:00:00               280.700                254.900   
...                                   ...                    ...   
2019-08-30 19:00:00                16.610                 15.440   
2019-08-30 20:00:00                12.710                 12.430   
2019-08-30 21:00:00                 6.910                  5.350   
2019-08-30 22:00:00                 3.009                  1.059   
2019-08-30 23:00:00                 2.173                  0.111   

                     longwaveradiationin  ...  temperature.4  temperature2.3  \
 timestamp                                ...                                  
2019-05-29 17:00:00              -15.400  ...            NaN             NaN   
2019-05-29 18:00:00              -14.520  ...            NaN             NaN   
2019-05-29 19:00:00              -17.320  ...            NaN             NaN   
2019-05-29 20:00:00              -29.970  ...            NaN             NaN   
2019-05-29 21:00:00              -45.900  ...            NaN             NaN   
...                                  ...  ...            ...             ...   
2019-08-30 19:00:00               -1.444  ...            NaN             NaN   
2019-08-30 20:00:00               -2.300  ...            NaN             NaN   
2019-08-30 21:00:00               -1.259  ...            NaN             NaN   
2019-08-30 22:00:00               -0.890  ...            NaN             NaN   
2019-08-30 23:00:00               -0.923  ...            NaN             NaN   

                     relativehumidity.4  windspeed_1.2  windspeed_2.1  \
 timestamp                                                              
2019-05-29 17:00:00                 NaN            NaN            NaN   
2019-05-29 18:00:00                 NaN            NaN            NaN   
2019-05-29 19:00:00                 NaN            NaN            NaN   
2019-05-29 20:00:00                 NaN            NaN            NaN   
2019-05-29 21:00:00                 NaN            NaN            NaN   
...                                 ...            ...            ...   
2019-08-30 19:00:00                 NaN            NaN            NaN   
2019-08-30 20:00:00                 NaN            NaN            NaN   
2019-08-30 21:00:00                 NaN            NaN            NaN   
2019-08-30 22:00:00                 NaN            NaN            NaN   
2019-08-30 23:00:00                 NaN            NaN            NaN   

                     shortwaveradiationin.3  shortwaveradiationout.3  \
 timestamp                                                             
2019-05-29 17:00:00                     NaN                      NaN   
2019-05-29 18:00:00                     NaN                      NaN   
2019-05-29 19:00:00                     NaN                      NaN   
2019-05-29 20:00:00                     NaN                      NaN   
2019-05-29 21:00:00                     NaN                      NaN   
...                                     ...                      ...   
2019-08-30 19:00:00                     NaN                      NaN   
2019-08-30 20:00:00                     NaN                      NaN   
2019-08-30 21:00:00                     NaN                      NaN   
2019-08-30 22:00:00                     NaN                      NaN   
2019-08-30 23:00:00                     NaN                      NaN   

                     longwaveradiationin.3  longwaveradiationout.3  \
 timestamp                                                           
2019-05-29 17:00:00                    NaN                     NaN   
2019-05-29 18:00:00                    NaN                     NaN   
2019-05-29 19:00:00                    NaN                     NaN   
2019-05-29 20:00:00                    NaN                     NaN   
2019-05-29 21:00:00                    NaN                     NaN   
...                                    ...                     ...   
2019-08-30 19:00:00                    NaN                     NaN   
2019-08-30 20:00:00                    NaN                     NaN   
2019-08-30 21:00:00                    NaN                     NaN   
2019-08-30 22:00:00                    NaN                     NaN   
2019-08-30 23:00:00                    NaN                     NaN   

                     temperatureradsensor.2  
 timestamp                                   
2019-05-29 17:00:00                     NaN  
2019-05-29 18:00:00                     NaN  
2019-05-29 19:00:00                     NaN  
2019-05-29 20:00:00                     NaN  
2019-05-29 21:00:00                     NaN  
...                                     ...  
2019-08-30 19:00:00                     NaN  
2019-08-30 20:00:00                     NaN  
2019-08-30 21:00:00                     NaN  
2019-08-30 22:00:00                     NaN  
2019-08-30 23:00:00                     NaN  

[2239 rows x 135 columns]

:end:


#+BEGIN_SRC ipython
fig, ax = plt.subplots(2,1,figsize=(10,5))
ax[0].plot(transmitted.index[1:], timestep_hour )
ax[0].plot(transmitted.index[1:], timestep_day )
ax[1].plot(transmitted_hour['temperature'])
ax[1].plot(transmitted_day['temperature'])


   #+END_SRC

   #+RESULTS:
   :results:
   # Out [4]: 
   # text/plain
   : [<matplotlib.lines.Line2D at 0x7f3b78b61550>]

   # text/plain
   : <Figure size 720x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/4479d71f944b6501c2709a338cb7265a0ea16930.png]]
   :end:


#+END_SRC

#+RESULTS: Load_transmitted_data
:results:
# Out [27]: 
:end:

** Pre 2022

*** Near surface climate

 #+BEGIN_SRC ipython
 import xarray as xr

 destdir = '/home/shl/OneDrive/projects/glaciobasis/gem_database/data/2022/'

 filename = 'data_v1.0/L1/zac_l/zac_l-2008-2021.nc'
 destname = 'GlacioBasis_Zackenberg_Near_surface_weather_AP_Olsen_AWS_Zack_M'
 <<extract_the_near_surface_climate_columns>>
 data_to_file.to_csv(destdir+destname+'.txt',sep = '\t', index = False, float_format = '%g')

 filename = 'data_v1.0/L1/zac_u/zac_u-2008-2021.nc'
 destname = 'GlacioBasis_Zackenberg_Near_surface_weather_AP_Olsen_AWS_Zack_S'
 <<extract_the_near_surface_climate_columns>>
 data_to_file.to_csv(destdir+destname+'.txt',sep = '\t', index = False, float_format = '%g')

 filename = 'data_v1.0/L1/zac_a/zac_a-2009-2020.nc'
 destname = 'GlacioBasis_Zackenberg_Near_surface_weather_AP_Olsen_AWS_Zack_T'
 <<extract_the_near_surface_climate_columns>>
 data_to_file.to_csv(destdir+destname+'.txt',sep = '\t', index = False, float_format = '%g')
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [2]: 
 :end:



 #+NAME: extract_the_near_surface_climate_columns
 #+BEGIN_SRC ipython
 with xr.open_dataset(filename) as ds:
     data_ds = ds[['p','t_1','rh','wspd','wdir','dsr_corr','usr_corr','dlr','ulr','t_rad','tilt_x','tilt_y']]

 data = data_ds.to_dataframe()
 data['datetime'] = data.index
 data.index.name = 'datetime1'
 data["date"] = data["datetime"].dt.date
 data["time"] = data["datetime"].dt.time

 data.rename(columns = {'p':'p_atm','t_1':'T_air','rh':'RH_water','wspd':'ws','wdir':'wd','dsr_corr':'SW_in','usr_corr':'SW_out','dlr':'LW_in','ulr':'LW_out'}, inplace = True)

 data_to_file = data[['date','time','p_atm', 'T_air', 'RH_water', 'ws','wd','SW_in','SW_out','LW_in','LW_out']]

 #+END_SRC


*** Snow sonic ranger

 date	time	h_snow_M	h_snow_M_qual	h_snow_S	h_snow_S_qual	h_snow_T	h_snow_T_qual
 GlacioBasis_Zackenberg_Snow_cover_AP_Olsen_Snow_sonic_ranger_height.txt
 z_boom
 z_boom_q

 #+BEGIN_SRC ipython
 import xarray as xr
 import pandas as pd

 destdir = '/home/shl/OneDrive/projects/glaciobasis/gem_database/data/2022/'

 filename = 'data_v1.0/L1/zac_l/zac_l-2008-2021.nc'
 destname = 'GlacioBasis_Zackenberg_Snow_cover_AP_Olsen_Snow_sonic_ranger_height'
 with xr.open_dataset(filename) as ds:
     data_ds = ds[['z_boom','z_boom_q']]

 data = data_ds.to_dataframe()
 data['datetime'] = pd.to_datetime(data.index)
 data_M = data.rename(columns = {'z_boom':'h_snow_M','z_boom_q':'h_snow_M_qual'})


 filename = 'data_v1.0/L1/zac_u/zac_u-2008-2021.nc'
 with xr.open_dataset(filename) as ds:
     data_ds = ds[['z_boom','z_boom_q']]

 data = data_ds.to_dataframe()
 data['datetime'] = pd.to_datetime(data.index)
 data_S = data.rename(columns = {'z_boom':'h_snow_S','z_boom_q':'h_snow_S_qual'})

 filename = 'data_v1.0/L1/zac_a/zac_a-2009-2020.nc'
 with xr.open_dataset(filename) as ds:
     data_ds = ds[['z_boom','z_boom_q']]

 data = data_ds.to_dataframe()
 data['datetime'] = pd.to_datetime(data.index)
 data_T = data.rename(columns = {'z_boom':'h_snow_T','z_boom_q':'h_snow_T_qual'})

 data_to_file = pd.merge(data_M,data_T, on = 'datetime', how = 'outer')
 data_to_file = pd.merge(data_to_file,data_S,on = 'datetime', how = 'outer')
 data_to_file["date"] = data_to_file["datetime"].dt.date
 data_to_file["time"] = data_to_file["datetime"].dt.time
 data_to_file = data_to_file[['date','time','h_snow_M','h_snow_M_qual','h_snow_S','h_snow_S_qual','h_snow_T','h_snow_T_qual']]
 #data_to_file = data_M.merge(data_S, on = 'datetime', how = 'outer').merge(data_T,on='datetime', how='outer')

 data_to_file.to_csv(destdir+destname+'.txt',sep = '\t', index = False, float_format = '%g')
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [17]: 
 :end:



 #+NAME: extract_the_SR50_snow_height
 #+BEGIN_SRC ipython
 with xr.open_dataset(filename) as ds:
     data_ds = ds[['z_boom','z_boom_q']]

 data = data_ds.to_dataframe()
 data['datetime'] = data.index
 data.index.name = 'datetime1'
 data["date"] = data["datetime"].dt.date
 data["time"] = data["datetime"].dt.time



 #+END_SRC

* Code to create climate data file for cosipy

#+BEGIN_SRC ipython
import xarray as xr

destdir = '/home/shl/OneDrive/projects/cosipy/data/'

filename = 'data_v1.0/L1/zac_l/zac_l-2008-2021.nc'
destname = 'zac_l'

with xr.open_dataset(filename) as ds:
    #print(ds.gps_alt.mean())

    data_ds = ds[['p','t_1','rh_corr','wspd','dsr_corr','dlr','cloud_cov']]

data = data_ds.to_dataframe()

data_h = data.resample('H').mean()
data_h['precip'] = data_h['t_1']*0
data_h['snow_fall'] = data_h['t_1']*0
#data_h['precip'].plot()
data_h_sel = data_h['2010']
data_h_sel.index.name = 'TIMESTAMP'
print(data_h_sel)
data_h_sel.to_csv(destdir+destname+'_2010.csv',sep = ',', index = True)
#+END_SRC

#+RESULTS:
:results:
# Out [3]: 
# output
                              p        t_1    rh_corr      wspd  dsr_corr  \
TIMESTAMP                                                                   
2010-01-01 00:00:00  938.969084  -4.398308  44.094986  5.254667       0.0   
2010-01-01 01:00:00  938.408063  -4.857979  46.513883  5.333833       0.0   
2010-01-01 02:00:00  938.138019  -4.195570  49.170522  5.328167       0.0   
2010-01-01 03:00:00  937.799136  -2.256114  42.660858  2.926167       0.0   
2010-01-01 04:00:00  937.517759  -1.946340  43.939941  3.486667       0.0   
...                         ...        ...        ...       ...       ...   
2010-12-31 19:00:00  946.530840 -17.205402  58.909828  1.017000       0.0   
2010-12-31 20:00:00  946.454890 -16.302662  53.837006  2.437667       0.0   
2010-12-31 21:00:00  946.252707 -15.659983  47.780156  3.996833       0.0   
2010-12-31 22:00:00  946.424671 -15.706187  49.430248  3.059000       0.0   
2010-12-31 23:00:00  945.747349 -17.722428  53.484185  5.858500       0.0   

                            dlr  cloud_cov  precip  snow_fall  
TIMESTAMP                                                      
2010-01-01 00:00:00  226.626823   0.277173    -0.0       -0.0  
2010-01-01 01:00:00  236.384476   0.400313    -0.0       -0.0  
2010-01-01 02:00:00  251.607388   0.528743    -0.0       -0.0  
2010-01-01 03:00:00  252.325813   0.444662    -0.0       -0.0  
2010-01-01 04:00:00  262.387864   0.535241    -0.0       -0.0  
...                         ...        ...     ...        ...  
2010-12-31 19:00:00  169.203901   0.211946    -0.0       -0.0  
2010-12-31 20:00:00  171.102879   0.197535    -0.0       -0.0  
2010-12-31 21:00:00  173.358059   0.196585    -0.0       -0.0  
2010-12-31 22:00:00  171.991429   0.183956    -0.0       -0.0  
2010-12-31 23:00:00  173.850834   0.280863    -0.0       -0.0  

[8760 rows x 9 columns]

:end:

#+BEGIN_SRC ipython
filename = 'data_v1.0/L1/zac_u/zac_u-2008-2021.nc'
destname = 'GlacioBasis_Zackenberg_Near_surface_weather_AP_Olsen_AWS_Zack_S'
<<extract_the_near_surface_climate_columns>>
data_to_file.to_csv(destdir+destname+'.txt',sep = '\t', index = False, float_format = '%g')

filename = 'data_v1.0/L1/zac_a/zac_a-2009-2020.nc'
destname = 'GlacioBasis_Zackenberg_Near_surface_weather_AP_Olsen_AWS_Zack_T'
<<extract_the_near_surface_climate_columns>>
data_to_file.to_csv(destdir+destname+'.txt',sep = '\t', index = False, float_format = '%g')
#+END_SRC

#+RESULTS:
:results:
# Out [2]: 
:end:



#+NAME: extract_colipy_columns
#+BEGIN_SRC ipython

#+END_SRC
