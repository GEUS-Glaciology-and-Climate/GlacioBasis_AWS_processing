#+TITLE: AWS processing
#+AUTHOR: Signe Hillerup Larsen
#+EMAIL: shl@geus.dk
#+DATE: {{{time(%Y-%m-%d)}}}
#+DESCRIPTION: Getting the GlacioBasis raw data into usefull formats
#+KEYWORDS:
#+OPTIONS:   H:4 num:4 toc:nil \n:nil ::t |:t ^:{} -:t f:t *:t <:t
#+EXCLUDE_TAGS: noexport
#+ARCHIVE: ::* Archive
#+PROPERTY: header-args:bash :noweb yes :tangle-mode (identity #o544)

#+PROPERTY: header-args :session *aws_processing_v1.0-shell* :noweb yes :eval yes
#+PROPERTY: header-args:jupyter-python :noweb yes :kernel aws_processing


Kernel:
#+BEGIN_SRC sh
ipython kernel install --name "aws_processing" --user

#+END_SRC

ipython kernel install --name "aws_processing" --user

* Code to process all downloaded data

Overall workflow:

Updates done in the raw data, then everything needs to be done again.

#+BEGIN_SRC jupyter-python
# Raw to L0
<<run_all_L0_zac_l>>
<<run_all_L0_zac_u>>
<<run_all_L0_zac_a>>
#+END_SRC

#+RESULTS:
: Index(['rec', 'p', 't_1', 't_2', 'rh', 'wspd', 'wdir', 'wd_std', 'dsr', 'usr',
:        'dlr', 'ulr', 't_rad', 'z_boom', 'z_boom_q', 'z_stake', 'z_stake_q',
:        'z_pt', 't_i_1', 't_i_2', 't_i_3', 't_i_4', 't_i_5', 't_i_6', 't_i_7',
:        't_i_8', 'tilt_x', 'tilt_y', 'gps_time', 'gps_lat', 'gps_lon',
:        'gps_alt', 'gps_geoid', 'gps_q', 'gps_numsat', 'gps_hdop', 't_log',
:        'fan_dc'],
:       dtype='object')

#+BEGIN_SRC bash :results verbatim
# L0 to L0M
<<make_L0M_zac_l>>
<<make_L0M_zac_u>>
<<make_L0M_zac_a>>
#+END_SRC


TODO print success lines in a better way in make L0M

#+BEGIN_SRC jupyter-python
# L0M to L1
<<convert_to_physical_values_workflow>>

# L1 to QC + databases
<<run_all_QC_filtering_steps_for_meterological_observations>>
#+END_SRC

TODO
- Clear out figures created in the QC process
- update nead

** Create L0 files: extract raw data and unify headers and correct special cases

Reading in raw files and writing them out as L0 files with nice headers and special case issues solved
*** Workflow for generating all L0 files

#+BEGIN_SRC jupyter-python
<<run_all_L0_zac_l>>
<<run_all_L0_zac_u>>
<<run_all_L0_zac_a>>

#+END_SRC

#+RESULTS:
: Index(['rec', 'p', 't_1', 't_2', 'rh', 'wspd', 'wdir', 'wd_std', 'dsr', 'usr',
:        'dlr', 'ulr', 't_rad', 'z_boom', 'z_boom_q', 'z_stake', 'z_stake_q',
:        'z_pt', 't_i_1', 't_i_2', 't_i_3', 't_i_4', 't_i_5', 't_i_6', 't_i_7',
:        't_i_8', 'tilt_x', 'tilt_y', 'gps_time', 'gps_lat', 'gps_lon',
:        'gps_alt', 'gps_geoid', 'gps_q', 'gps_numsat', 'gps_hdop', 't_log',
:        'fan_dc'],
:       dtype='object')

*** Libraries
#+NAME: load_libraries
#+BEGIN_SRC jupyter-python
from glob import glob
import pandas as pd
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
import os as os
#+END_SRC


*** Constants

#+NAME: define_constants
#+BEGIN_SRC jupyter-python
mmHg2hPa = 1.33322368
#+END_SRC

*** working folders

#+NAME: define_working_folders
#+BEGIN_SRC jupyter-python
raw = '/home/shl@geus.dk/OneDrive/projects/glaciobasis/data/aws_raw/'
destination = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0/'
#+END_SRC


*** Define which headers to pass forward
#+NAME: headers_to_pass_forward
#+BEGIN_SRC jupyter-python
new_headers = ['rec', 'p', 't_1', 't_2', 'rh', 'wspd', 'wdir', 'wd_std', 'dsr', 'usr', 'dlr', 'ulr', 't_rad', 'z_boom', 'z_boom_q', 'z_stake', 'z_stake_q', 'z_pt', 't_i_1', 't_i_2', 't_i_3', 't_i_4', 't_i_5', 't_i_6', 't_i_7', 't_i_8', 'tilt_x', 'tilt_y', 'gps_time', 'gps_lat', 'gps_lon', 'gps_alt', 'gps_geoid', 'gps_q', 'gps_numsat', 'gps_hdop', 't_log', 'fan_dc']


#+END_SRC


*** Each station, year by year

**** Take a look at all the table mem files and find the long one

#+BEGIN_SRC jupyter-python
<<load_libraries>>
station_id = 'zac_l'
ystart = '2017'
yend = '2018'

filelist = glob(raw + station_id + '/' + ystart + '-' + yend +'/*TableMem*')
for f in filelist:
    df = pd.read_csv(f, header=1,skiprows=[2,3],sep=',',engine='python')
    df.index = pd.to_datetime(df.TIMESTAMP)
    df = df.drop(['TIMESTAMP'], axis=1)
    df.index.name = 'time'
    df.sort_index(inplace=True)
    df = df.replace('NAN',np.NaN)
    print(f)
    print(df['AS_T_Avg'])
    #df['AS_T_Avg'].plot()
    
#+END_SRC

#+RESULTS:
:results:
# Out [83]: 
# output
/home/shl@geus.dk/OneDrive/projects/glaciobasis/data/aws_raw/zac_l/2017-2018/TOA5_13745.TableMem.dat
time
2017-04-24 16:30:00    -2.714936
2017-04-24 16:40:00    -13.24721
2017-04-24 16:50:00    -13.14341
2017-04-24 17:00:00      -13.461
2017-04-24 17:10:00    -13.51251
                         ...    
2018-04-14 14:00:00    -3.442842
2018-04-14 14:10:00    -2.958845
2018-04-14 14:20:00    -2.451514
2018-04-14 14:30:00    -2.114108
2018-04-14 14:40:00    -2.296419
Name: AS_T_Avg, Length: 35461, dtype: object

:end:


**** zac_l

#+NAME: run_all_L0_zac_l
#+BEGIN_SRC jupyter-python
<<zac_l_2008_2010>>
<<zac_l_2010_2011>>
<<zac_l_2011_2014>>
<<zac_l_2012_2013>>
<<zac_l_2014_2015>>
<<zac_l_2015_2016>>
<<zac_l_2016_2017>>
<<zac_l_2017_2018>>
<<zac_l_2018_2020>>
<<zac_l_2020_2021>>
<<zac_l_2021_2022>>
#+END_SRC

#+RESULTS: run_all_L0_zac_l
:results:
# Out [1]: 
# output
Index(['rec', 'p', 't_1', 't_2', 'rh', 'wspd', 'wdir', 'wd_std', 'dsr', 'usr',
       'dlr', 'ulr', 't_rad', 'z_boom', 'z_boom_q', 'z_stake', 'z_stake_q',
       'z_pt', 't_i_1', 't_i_2', 't_i_3', 't_i_4', 't_i_5', 't_i_6', 't_i_7',
       't_i_8', 'tilt_x', 'tilt_y', 'gps_time', 'gps_lat', 'gps_lon',
       'gps_alt', 'gps_geoid', 'gps_q', 'gps_numsat', 'gps_hdop', 't_log',
       'fan_dc'],
      dtype='object')
Index(['rec', 'p', 't_1', 't_2', 'rh', 'wspd', 'wdir', 'wd_std', 'dsr', 'usr',
       'dlr', 'ulr', 't_rad', 'z_boom', 'z_boom_q', 'z_stake', 'z_stake_q',
       'z_pt', 't_i_1', 't_i_2', 't_i_3', 't_i_4', 't_i_5', 't_i_6', 't_i_7',
       't_i_8', 'tilt_x', 'tilt_y', 'gps_time', 'gps_lat', 'gps_lon',
       'gps_alt', 'gps_geoid', 'gps_q', 'gps_numsat', 'gps_hdop', 't_log',
       'fan_dc'],
      dtype='object')

:end:

***** zac_l_2008_2010


#+NAME: zac_l_2008_2010
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2008'
yend = '2010'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'ZACK M_TableMem.dat.backup'
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<read_named_TableMem_file>>
<<fix_headers_type0_SR50_switched>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 

df['p'] = df['p']*mmHg2hPa
# Radiation from mV to 10^-5 V
#variables = ['dsr','usr','dlr','ulr']
#df[variables] = df[variables].astype(float)
#sel = df.index > '2010-May-12 20:50:00' 
#df.loc[sel,variables] = df.loc[sel,variables]/100

df.to_csv(outfilename,sep=',', index=True)
#+END_SRC


#+RESULTS: zac_l_2008_2010
:results:
# Out [69]: 
:end:

Read in and concatenate the raw files

***** zac_l_2010_2011

#+NAME: zac_l_2010_2011
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2010'
yend = '2011'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13745.TableMem.dat'
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type0_SR50_switched>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
#df['tilt_x'] = df['tilt_x'].astype(float)/10 
#df['tilt_y'] = df['tilt_y'].astype(float)/10 
# Even though the header in the raw data says that the unit is in mV, the data looks like it is in V/100

df['p'] = df['p']*mmHg2hPa
# Radiation from mV to 10^-5 V
variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100

df.to_csv(outfilename,sep=',', index=True)
#+END_SRC

#+RESULTS: zac_l_2010_2011
:results:
# Out [72]: 
:end:

***** zac_l_2011_2014

#+NAME: zac_l_2011_2014
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2011'
yend = '2014'

yend1 = '2012'
ystart2 = '2013'

outfilename1 = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend1 + '.csv'
outfilename2 = destination+station_id + '/' + station_id + '-' + ystart2 +'-' + yend + '.csv'
filename = 'ZACK M_TableMem.dat.7.backup'
<<read_named_TableMem_file>>

#<<read_and_concat_all_TableMem_files_in_raw_folder>>
df_orig = df

df=df_orig[:'17-April-2012'].copy()
<<fix_headers_type0_SR50_switched>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 
df['p'] = df['p']*mmHg2hPa
df.to_csv(outfilename1,sep=',', index=True)


df=df_orig['1-May-2013':].copy()
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 

df['p'] = df['p']*mmHg2hPa
df.to_csv(outfilename2,sep=',', index=True)


#+END_SRC

#+RESULTS: zac_l_2011_2014
:results:
# Out [74]: 
:end:

***** zac_l_2012_2013

#+NAME: zac_l_2012_2013
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2012'
yend = '2013'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13745.TableMem.dat'
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type0_SR50_switched>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 
df['p'] = df['p']*mmHg2hPa

df.to_csv(outfilename,sep=',', index=True)
#+END_SRC

#+RESULTS: zac_l_2012_2013
:results:
# Out [76]: 
:end:

***** zac_l_2014_2015

#+NAME: zac_l_2014_2015
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2014'
yend = '2015'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'ZACK M_TableMem.dat.14.backup'
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

df['p'] = df['p']*mmHg2hPa
# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 

df.to_csv(outfilename,sep=',', index=True)
#+END_SRC

#+RESULTS: zac_l_2014_2015
:results:
# Out [78]: 
:end:

***** zac_l_2015_2016

#+NAME: zac_l_2015_2016
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2015'
yend = '2016'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13745.TableMem.dat'
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

df['p'] = df['p']*mmHg2hPa
# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 
df.to_csv(outfilename,sep=',', index=True)
#+END_SRC

#+RESULTS: zac_l_2015_2016
:results:
# Out [80]: 
:end:

***** zac_l_2016_2017

#+NAME: zac_l_2016_2017
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2016'
yend = '2017'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13745.TableMem.dat'
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

df['p'] = df['p']*mmHg2hPa
# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 
df.to_csv(outfilename,sep=',', index=True)
#+END_SRC

#+RESULTS: zac_l_2016_2017
:results:
# Out [82]: 
:end:

***** zac_l_2017_2018

#+NAME: zac_l_2017_2018
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2017'
yend = '2018'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13745.TableMem.dat'
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)

df['p'] = df['p']*mmHg2hPa
# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 
df.to_csv(outfilename,sep=',', index=True)
#+END_SRC

#+RESULTS: zac_l_2017_2018
:results:
# Out [84]: 
:end:

***** zac_l_2018_2020

#+NAME: zac_l_2018_2020
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2018'
yend = '2020'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13745.TableMem.dat'                                                               
<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df['p'] = df['p']*mmHg2hPa
# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 

df.to_csv(outfilename,sep=',', index=True)
#+END_SRC


***** zac_l_2020_2021

#+NAME: zac_l_2020_2021
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2020'
yend = '2021'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'CR1000_nn_TableMem.dat.backup'
#<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<read_named_TableMem_file>>
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 
df['p'] = df['p']*mmHg2hPa

#print(df.keys())
df.to_csv(outfilename,sep=',', index=True)
#+END_SRC



***** zac_l_2021_2022

#+NAME: zac_l_2021_2022
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_l'
ystart = '2021'
yend = '2022'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'zac-l_TableMem.dat'
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<read_named_TableMem_file>>
<<fix_headers_type0>>
df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
# in the zac_l logger program use the unit mV and in zac_u and zac_a the unit is V/100, the later workflow assumes V/100
df['tilt_x'] = df['tilt_x'].astype(float)/10 
df['tilt_y'] = df['tilt_y'].astype(float)/10 
df['p'] = df['p']*mmHg2hPa

print(df.keys())
df.to_csv(outfilename,sep=',', index=True)
#+END_SRC

#+RESULTS: zac_l_2021_2022
:results:
# Out [3]: 
# output
Index(['rec', 'p', 't_1', 't_2', 'rh', 'wspd', 'wdir', 'wd_std', 'dsr', 'usr',
       'dlr', 'ulr', 't_rad', 'z_boom', 'z_boom_q', 'z_stake', 'z_stake_q',
       'z_pt', 't_i_1', 't_i_2', 't_i_3', 't_i_4', 't_i_5', 't_i_6', 't_i_7',
       't_i_8', 'tilt_x', 'tilt_y', 'gps_time', 'gps_lat', 'gps_lon',
       'gps_alt', 'gps_geoid', 'gps_q', 'gps_numsat', 'gps_hdop', 't_log',
       'fan_dc'],
      dtype='object')

:end:


**** zac_u
#+NAME: run_all_L0_zac_u
#+BEGIN_SRC jupyter-python
<<zac_u_2008_2010>>
<<zac_u_2010_2011>>
<<zac_u_2011_2012>>
<<zac_u_2012_2013>>
<<zac_u_2013_2014>>
<<zac_u_2014_2015>>
<<zac_u_2015_2016>>
<<zac_u_2016_2017>>
<<zac_u_2017_2019>>
<<zac_u_2019_2020>>
<<zac_u_2020_2021>>
<<zac_u_2021_2022>>
#+END_SRC

#+RESULTS: run_all_L0_zac_u
:results:
# Out [2]: 
:end:

***** zac_u_2008_2010
#+NAME: zac_u_2008_2010
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2008'
yend = '2010'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'

<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type0>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data


df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2008_2010
:results:
# Out [153]: 
:end:

***** zac_u_2010_2011
#+NAME: zac_u_2010_2011
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2010'
yend = '2011'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_13744.TableMem.dat'

<<read_named_TableMem_file>>
<<fix_headers_type0>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2010_2011
:results:
# Out [154]: 
:end:

***** zac_u_2011_2012
#+NAME: zac_u_2011_2012
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2011'
yend = '2012'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'ZACK S_TableMem.dat'

<<read_named_TableMem_file>>

df.index = df.index - pd.to_timedelta('1 day')

<<fix_headers_type0>>

df = df[new_headers] # pressure is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

# the unit of mmHg is converted to hPa
df['p'] = df['p']*mmHg2hPa

df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2011_2012
:results:
# Out [107]: 
:end:

***** zac_u_2012_2013
#+NAME: zac_u_2012_2013
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2012'
yend = '2013'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
df.index = df.index - pd.to_timedelta('1 day')
<<fix_headers_type1>>

df = df[new_headers] # radiation and tilt is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2012_2013
:results:
# Out [158]: 
:end:

***** zac_u_2013_2014
#+NAME: zac_u_2013_2014
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2013'
yend = '2014'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
df.index = df.index - pd.to_timedelta('1 day')

<<fix_headers_type1>>

df = df[new_headers] # radiation and tilt is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df['tilt_x'] = df['tilt_x'].astype(float)*10
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2013_2014
:results:
# Out [159]: 
:end:

***** zac_u_2014_2015
#+NAME: zac_u_2014_2015
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2014'
yend = '2015'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type1>>

df = df[new_headers] # radiation and tilt is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data


variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2014_2015
:results:
# Out [160]: 
:end:


***** zac_u_2015_2016
#+NAME: zac_u_2015_2016
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2015'
yend = '2016'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_E2101.TableMem.dat'
#<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<read_named_TableMem_file>>

<<fix_headers_type1>>

df = df[new_headers] # radiation and tilt is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df['tilt_x'] = df['tilt_x'].astype(float)*10
df['tilt_y'] = df['tilt_y'].astype(float)*10
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2015_2016
:results:
# Out [1]: 
:end:

***** zac_u_2016_2017
#+NAME: zac_u_2016_2017
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2016'
yend = '2017'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type1>>

df = df[new_headers] # radiation and tilt is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2016_2017
:results:
# Out [163]: 
:end:


***** zac_u_2017_2019
#+NAME: zac_u_2017_2019
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2017'
yend = '2019'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type1>>

df = df[new_headers] # pressure transducer is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2017_2019
:results:
# Out [164]: 
:end:

***** zac_u_2019_2020
#+NAME: zac_u_2019_2020
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2019'
yend = '2020'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type1>>

df = df[new_headers] # thermistor is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2019_2020
:results:
# Out [165]: 
:end:

***** zac_u_2020_2021
#+NAME: zac_u_2020_2021
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2020'
yend = '2021'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'TOA5_E2101.TableMem.dat'

<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type1>>

df = df[new_headers] # thermistor is added
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2020_2021
:results:
# Out [2]: 
:end:

***** zac_u_2021_2022
#+NAME: zac_u_2021_2022
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_u'
ystart = '2021'
yend = '2022'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'
filename = 'zac-u_TableMem.dat'

<<read_named_TableMem_file>>
#<<read_and_concat_all_TableMem_files_in_raw_folder>>

<<fix_headers_type1>>

df = df[new_headers] # thermistor is added

new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)
df[variables] = df[variables]/100
#df['p'] = df['p']*mmHg2hPa
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_u_2021_2022
:results:
# Out [93]: 
:end:



**** zac_a
#+NAME: run_all_L0_zac_a
#+BEGIN_SRC jupyter-python
<<zac_a_2009_2010>>
<<zac_a_2010_2011>>
<<zac_a_2011_2012>>
<<zac_a_2012_2013>>
<<zac_a_2013_2014>>
<<zac_a_2014_2015>>
<<zac_a_2015_2016>>
<<zac_a_2016_2017>>
<<zac_a_2017_2018>>
<<zac_a_2018_2019>>
#+END_SRC

#+RESULTS: run_all_L0_zac_a
:results:
# Out [3]: 
# output
Index(['rec', 'p', 't_1', 't_2', 'rh', 'wspd', 'wdir', 'wd_std', 'dsr', 'usr',
       'dlr', 'ulr', 't_rad', 'z_boom', 'z_boom_q', 'z_stake', 'z_stake_q',
       'z_pt', 't_i_1', 't_i_2', 't_i_3', 't_i_4', 't_i_5', 't_i_6', 't_i_7',
       't_i_8', 'tilt_x', 'tilt_y', 'gps_time', 'gps_lat', 'gps_lon',
       'gps_alt', 'gps_geoid', 'gps_q', 'gps_numsat', 'gps_hdop', 't_log',
       'fan_dc'],
      dtype='object')

:end:


***** zac_a_2009_2010
#+NAME: zac_a_2009_2010
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2009'
yend = '2010'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 

df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_a_2009_2010
:results:
# Out [13]: 
:end:

***** zac_a_2010_2011
#+NAME: zac_a_2010_2011
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2010'
yend = '2011'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)

df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_a_2010_2011
:results:
# Out [2]: 
:end:

***** zac_a_2011_2012
#+NAME: zac_a_2011_2012
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2011'
yend = '2012'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>
df.index = df.index - pd.to_timedelta('1 day')

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_a_2011_2012
:results:
# Out [3]: 
:end:


***** zac_a_2012_2013
#+NAME: zac_a_2012_2013
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2012'
yend = '2013'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)
#+END_SRC



***** zac_a_2013_2014
#+NAME: zac_a_2013_2014
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2013'
yend = '2014'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_a_2013_2014
:results:
# Out [5]: 
:end:


***** zac_a_2014_2015
#+NAME: zac_a_2014_2015
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2014'
yend = '2015'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_a_2014_2015
:results:
# Out [6]: 
:end:

***** zac_a_2015_2016
#+NAME: zac_a_2015_2016
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2015'
yend = '2016'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_a_2015_2016
:results:
# Out [7]: 
:end:
***** zac_a_2016_2017
#+NAME: zac_a_2016_2017
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2016'
yend = '2017'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_a_2016_2017
:results:
# Out [8]: 
:end:

***** zac_a_2017_2018
#+NAME: zac_a_2017_2018
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2017'
yend = '2018'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1>>

df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
#print(df.keys())
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_a_2017_2018
:results:
# Out [10]: 
# output
Index(['rec', 'p', 't_1', 't_2', 'rh', 'wspd', 'wdir', 'wd_std', 'dsr', 'usr',
       'dlr', 'ulr', 't_rad', 'z_boom', 'z_boom_q', 'z_stake', 'z_stake_q',
       'z_pt', 't_i_1', 't_i_2', 't_i_3', 't_i_4', 't_i_5', 't_i_6', 't_i_7',
       't_i_8', 'tilt_x', 'tilt_y', 'gps_time', 'gps_lat', 'gps_lon',
       'gps_alt', 'gps_geoid', 'gps_q', 'gps_numsat', 'gps_hdop', 't_log',
       'fan_dc'],
      dtype='object')

:end:



***** zac_a_2018_2019
#+NAME: zac_a_2018_2019
#+BEGIN_SRC jupyter-python
<<load_libraries>>
<<define_constants>>
<<define_working_folders>>
<<headers_to_pass_forward>>

station_id = 'zac_a'
ystart = '2018'
yend = '2019'

outfilename = destination+station_id + '/' + station_id + '-' + ystart +'-' + yend + '.csv'


<<read_and_concat_all_TableMem_files_in_raw_folder>>
<<fix_headers_type1a>>


df = df[new_headers]
new_index = pd.date_range(start = df.index[0], end =df.index[-1], freq = '10min')
if len(new_index) < len(df.index):
    df = df.reindex(new_index, fill_value = np.nan)
df = df[endtime:] # removing all values before last file ends
endtime = df.index[-1] # time to be transferred to next file to cut it where this ends in case there is ssame copies of the data

variables = ['dsr','usr','dlr','ulr']
df[variables] = df[variables].astype(float)/100
df['tilt_x'] = df['tilt_x'].astype(float)*10 
df['tilt_y'] = df['tilt_y'].astype(float)*10 
df.to_csv(outfilename,sep=',', index=True)

#+END_SRC

#+RESULTS: zac_a_2018_2019
:results:
# Out [13]: 
:end:





**** Code
#+NAME: read_and_concat_all_TableMem_files_in_raw_folder
#+BEGIN_SRC jupyter-python

filelist = glob(raw + station_id + '/' + ystart + '-' + yend +'/*TableMem*')
df = pd.concat((pd.read_csv(f, header=1,skiprows=[2,3],sep=',',engine='python') for f in filelist))
df.index = pd.to_datetime(df.TIMESTAMP)
df = df.drop(['TIMESTAMP'], axis=1)
df.index.name = 'time'
df.sort_index(inplace=True)
df = df.replace('NAN',np.NaN)


#+END_SRC

#+NAME: read_named_TableMem_file
#+BEGIN_SRC jupyter-python
df = pd.read_csv(raw + station_id + '/' + ystart + '-' + yend +'/'+filename, header=1,skiprows=[2,3],sep=',',engine='python')
df.index = pd.to_datetime(df.TIMESTAMP)
df = df.drop(['TIMESTAMP'], axis=1)
df.index.name = 'time'
df.sort_index(inplace=True)
df = df.replace('NAN',np.NaN)
#+END_SRC

#+RESULTS: read_named_TableMem_file


#+NAME: fix_headers_type0_SR50_switched
#+BEGIN_SRC jupyter-python

df = df.rename(columns = {'TIMESTAMP':'time', 'RECORD':'rec', 'BP_mmHg_Avg':'p','AS_Pt100_Avg':'t_1', 'AS_T_Avg':'t_2', 'AS_RH_Avg':'rh', 'WS_ms_S_WVT':'wspd', 'WindDir_D1_WVT':'wdir', 'WindDir_SD1_WVT':'wd_std', 'CNR1_SWin_Avg':'dsr', 'CNR1_SWout_Avg':'usr', 'CNR1_LWin_Avg':'dlr', 'CNR1_LWout_Avg':'ulr','CNR1_Pt100_Avg':'t_rad', 'SnowHeight':'z_stake', 'SnowHeightQuality':'z_stake_q', 'Ablation':'z_boom', 'AblationQuality':'z_boom_q', 'Ablation_meter_Avg':'z_pt', 'Thermistor_1':'t_i_1', 'Thermistor_2':'t_i_2', 'Thermistor_3':'t_i_3', 'Thermistor_4':'t_i_4', 'Thermistor_5':'t_i_5', 'Thermistor_6':'t_i_6','Thermistor_7':'t_i_7', 'Thermistor_8':'t_i_8', 'Xtilt_Avg':'tilt_x', 'Ytilt_Avg':'tilt_y', 'TIME':'gps_time', 'LAT':'gps_lat', 'LONGI':'gps_lon', 'ALTDE':'gps_alt', 'GIODAL':'gps_geoid', 'QUAL':'gps_q', 'NUMSATS':'gps_numsat', 'HDP':'gps_hdop', 'PTemp_C_Avg':'t_log', 'Fan_current_avg':'fan_dc' })

#df = df.drop(columns = ['HEMINS','HEMIEW','ALTUNIT','GEOUNIT'])
#+END_SRC

#+NAME: fix_headers_type0
#+BEGIN_SRC jupyter-python

df = df.rename(columns = {'TIMESTAMP':'time', 'RECORD':'rec', 'BP_mmHg_Avg':'p','AS_Pt100_Avg':'t_1', 'AS_T_Avg':'t_2', 'AS_RH_Avg':'rh', 'WS_ms_S_WVT':'wspd', 'WindDir_D1_WVT':'wdir', 'WindDir_SD1_WVT':'wd_std', 'CNR1_SWin_Avg':'dsr', 'CNR1_SWout_Avg':'usr', 'CNR1_LWin_Avg':'dlr', 'CNR1_LWout_Avg':'ulr','CNR1_Pt100_Avg':'t_rad', 'SnowHeight':'z_boom', 'SnowHeightQuality':'z_boom_q', 'Ablation':'z_stake', 'AblationQuality':'z_stake_q', 'Ablation_meter_Avg':'z_pt', 'Thermistor_1':'t_i_1', 'Thermistor_2':'t_i_2', 'Thermistor_3':'t_i_3', 'Thermistor_4':'t_i_4', 'Thermistor_5':'t_i_5', 'Thermistor_6':'t_i_6','Thermistor_7':'t_i_7', 'Thermistor_8':'t_i_8', 'Xtilt_Avg':'tilt_x', 'Ytilt_Avg':'tilt_y', 'TIME':'gps_time', 'LAT':'gps_lat', 'LONGI':'gps_lon', 'ALTDE':'gps_alt', 'GIODAL':'gps_geoid', 'QUAL':'gps_q', 'NUMSATS':'gps_numsat', 'HDP':'gps_hdop', 'PTemp_C_Avg':'t_log', 'Fan_current_avg':'fan_dc' })

#df = df.drop(columns = ['HEMINS','HEMIEW','ALTUNIT','GEOUNIT'])
#+END_SRC

#+NAME: fix_headers_type1
#+BEGIN_SRC jupyter-python
df = df.rename(columns = {'RECORD':'rec', 'AirPressure_Avg':'p','Temperature_Avg':'t_1', 'Temperature2_Avg':'t_2', 'RelativeHumidity_Avg':'rh', 'WindSpeed':'wspd', 'WindDirection':'wdir', 'WindDirection_SD':'wd_std', 'ShortwaveRadiationIn_Avg':'dsr', 'ShortwaveRadiationOut_Avg':'usr','LongwaveRadiationIn_Avg':'dlr', 'LongwaveRadiationOut_Avg':'ulr','TemperatureRadSensor_Avg':'t_rad', 'SnowHeight':'z_boom', 'SnowHeightQuality':'z_boom_q', 'SurfaceHeight':'z_stake', 'SurfaceHeightQuality':'z_stake_q', 'IceHeight_Avg':'z_pt', 'TemperatureIce1m_Avg':'t_i_1', 'TemperatureIce2m_Avg':'t_i_2', 'TemperatureIce3m_Avg':'t_i_3', 'TemperatureIce4m_Avg':'t_i_4', 'TemperatureIce5m_Avg':'t_i_5', 'TemperatureIce6m_Avg':'t_i_6','TemperatureIce7m_Avg':'t_i_7', 'TemperatureIce10m_Avg':'t_i_8', 'TiltX_Avg':'tilt_x', 'TiltY_Avg':'tilt_y', 'TimeGPS':'gps_time', 'Latitude':'gps_lat', 'Longitude':'gps_lon', 'Altitude':'gps_alt', 'Giodal':'gps_geoid', 'Quality':'gps_q', 'NumberSatellites':'gps_numsat', 'HDOP':'gps_hdop', 'TemperatureLogger_Avg':'t_log', 'Fan_current_avg':'fan_dc' })
#+END_SRC

#+NAME: fix_headers_type1a
#+BEGIN_SRC jupyter-python
df = df.rename(columns = {'RECORD':'rec', 'AirPressure_Avg':'p','Temperature_Avg':'t_1', 'Temperature2_Avg':'t_2', 'RelativeHumidity_Avg':'rh', 'WindSpeed':'wspd', 'WindDirection':'wdir', 'WindDirection_SD':'wd_std', 'ShortwaveRadiationIn_Avg':'dsr', 'ShortwaveRadiationOut_Avg':'usr','LongwaveRadiationIn_Avg':'dlr', 'LongwaveRadiationOut_Avg':'ulr','TemperatureRadSensor_Avg':'t_rad', 'SnowHeight':'z_boom', 'SnowHeightQuality':'z_boom_q', 'SurfaceHeight':'z_stake', 'SurfaceHeightQuality':'z_stake_q', 'IceHeight_Avg':'z_pt', 'TemperatureIce1m_Avg':'t_i_1', 'TemperatureIce2m_Avg':'t_i_2', 'TemperatureIce3m_Avg':'t_i_3', 'TemperatureIce4m_Avg':'t_i_4', 'TemperatureIce5m_Avg':'t_i_5', 'TemperatureIce6m_Avg':'t_i_6','TemperatureIce7m_Avg':'t_i_7', 'TemperatureIce10m_Avg':'t_i_8', 'TiltX_Avg':'tilt_x', 'TiltY_Avg':'tilt_y', 'TimeGPS':'gps_time', 'Latitude':'gps_lat', 'Longitude':'gps_lon', 'Altitude':'gps_alt', 'Giodal':'gps_geoid', 'Quality':'gps_q', 'NumberSatellites':'gps_numsat', 'HDOP':'gps_hdop', 'TemperatureLogger_Avg':'t_log', 'FanCurrent_Avg':'fan_dc' })
#+END_SRC


*** Take a look at L0 data 


**** zac_l
#+BEGIN_SRC jupyter-python
<<load_libraries>>
folder = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0/'
station_id = 'zac_l'
variables = ['t_1', 'rh', 'usr']
fig, ax = plt.subplots(3,1, figsize = (10,10))
filelist = glob(folder+station_id+'/*')
df = pd.concat((pd.read_csv(f,index_col = 0, parse_dates = True,low_memory=False) for f in filelist))
df.sort_index(inplace=True)
df = df.drop(df.index[df.index < datetime(2007,1,1)])


for index,key in enumerate(variables):
    ax[index].plot(df[key])
    ax[index].set_title(key)
    ax[index].plot(df[key])
    ax[index].set_title(key)
    ax[index].plot(df[key])
    ax[index].set_title(key)


    
    

#+END_SRC

#+RESULTS:
:results:
# Out [6]: 
# text/plain
: <Figure size 720x720 with 3 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/fad2afd5dad1e1c83bd3179e4f2c678aa6ca2331.png]]
:end:



**** zac_u
#+BEGIN_SRC jupyter-python
<<load_libraries>>
folder = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0/'
station_id = 'zac_u'
variables = ['t_1','p', 'rh', 'usr']
fig, ax = plt.subplots(4,1, figsize = (10,10))
filelist = glob(folder+station_id+'/*')
df = pd.concat((pd.read_csv(f,index_col = 0, parse_dates = True,low_memory=False) for f in filelist))
df.sort_index(inplace=True)
#df = df.drop(df.index[df.index < datetime(2007,1,1)])


for index,key in enumerate(variables):
    ax[index].plot(df[key])
    ax[index].set_title(key)
    ax[index].plot(df[key])
    ax[index].set_title(key)
    ax[index].plot(df[key])
    ax[index].set_title(key)


    
    

#+END_SRC

#+RESULTS:
:results:
# Out [7]: 
# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/ea3f8f206569b1f87534f4d70e73774649c4c228.png]]
:end:


** Create L0M files: add nead header
L0 files are converted into files with the nead header - this is in order to ensure the correct meta data follows each file
The nead header template is filled out manually as 01-nead header files

# fields: time, rec, p, t_1, t_2, rh, wspd, wdir, wd_std, dsr, usr,dlr, ulr, t_rad, z_boom, z_boom_q, z_stake, z_stake_q, z_pt, t_i_1, t_i_2, t_i_3, t_i_4, t_i_5, t_i_6, t_i_7,t_i_8, tilt_x, tilt_y, gps_time, gps_lat, gps_lon,gps_alt, gps_geoid, gps_q, gps_numsat, gps_hdop, t_log, fan_dc


*** zac_l

concatenate the data with the nead header file, rename and put in L0M folder
#+NAME: make_L0M_zac_l
#+BEGIN_SRC bash :eval yes
station_id=zac_l
sourcedir=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0/$station_id
destdir=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/$station_id

years=2008-2010
endyear=2010 
<<concat_nead_and_data>>

years=2010-2011
endyear=2011 
<<concat_nead_and_data>>

years=2011-2012
endyear=2012 
<<concat_nead_and_data>>

years=2012-2013
endyear=2013 
<<concat_nead_and_data>>

years=2013-2014
endyear=2014 
<<concat_nead_and_data>>

years=2014-2015
endyear=2015 
<<concat_nead_and_data>>

years=2015-2016
endyear=2016 
<<concat_nead_and_data>>

years=2016-2017
endyear=2017 
<<concat_nead_and_data>>

years=2017-2018
endyear=2018 
<<concat_nead_and_data>>

years=2018-2020
endyear=2020 
<<concat_nead_and_data>>

years=2020-2021
endyear=2021 
<<concat_nead_and_data>>

years=2021-2022
endyear=2022 
<<concat_nead_and_data>>
echo "ZAC_L data now has nead headers"
#+END_SRC

#+RESULTS: make_L0M_zac_l

#+NAME: concat_nead_and_data
#+BEGIN_SRC bash :eval yes
cp $sourcedir/$station_id-${years}.csv $destdir/$station_id-${years}.csv
sed -i '1d' $destdir/$station_id-${years}.csv
cat $destdir/01_nead_$station_id-$years.csv $destdir/$station_id-${years}.csv > $destdir/$station_id-$endyear.csv
rm $destdir/$station_id-${years}.csv

#+END_SRC

#+RESULTS: concat_nead_and_data


*** zac_u
I start by taking a look at the pngs of the files to figure out the quality of each file and if any of them should be split up.

None of them needs to be split-up.

Then I manually copy the files to the L0M folder 
#+NAME: make_L0M_zac_u
#+BEGIN_SRC bash :eval yes

station_id=zac_u
sourcedir=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0/$station_id
destdir=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/$station_id

years=2008-2010
endyear=2010 
<<concat_nead_and_data>>

years=2010-2011
endyear=2011 
<<concat_nead_and_data>>

years=2011-2012
endyear=2012 
<<concat_nead_and_data>>

years=2012-2013
endyear=2013 
<<concat_nead_and_data>>

years=2013-2014
endyear=2014 
<<concat_nead_and_data>>

years=2014-2015
endyear=2015 
<<concat_nead_and_data>>

years=2015-2016
endyear=2016 
<<concat_nead_and_data>>

years=2016-2017
endyear=2017 
<<concat_nead_and_data>>

years=2017-2019
endyear=2019 
<<concat_nead_and_data>>

years=2019-2020
endyear=2020 
<<concat_nead_and_data>>

years=2020-2021
endyear=2021 
<<concat_nead_and_data>>

years=2021-2022
endyear=2022 
<<concat_nead_and_data>>
echo "ZAC_U data now has nead headers"

#+END_SRC

#+RESULTS: make_L0M_zac_u


#+BEGIN_SRC bash :eval yes
ls $destdir
#+END_SRC

#+RESULTS:
| 01_nead_zac_u-2008-2010.csv | 01_nead_zac_u-2015-2016.csv | zac_u-2010.csv | zac_u-2016.csv |
| 01_nead_zac_u-2010-2011.csv | 01_nead_zac_u-2016-2017.csv | zac_u-2011.csv | zac_u-2017.csv |
| 01_nead_zac_u-2011-2012.csv | 01_nead_zac_u-2017-2019.csv | zac_u-2012.csv | zac_u-2019.csv |
| 01_nead_zac_u-2012-2013.csv | 01_nead_zac_u-2019-2020.csv | zac_u-2013.csv | zac_u-2020.csv |
| 01_nead_zac_u-2013-2014.csv | 01_nead_zac_u-2020-2021.csv | zac_u-2014.csv | zac_u-2021.csv |
| 01_nead_zac_u-2014-2015.csv | 01_nead_zac_u-2021-2022.csv | zac_u-2015.csv | zac_u-2022.csv |




*** zac_a

time,rec,p,t_1,t_2,rh,wspd,wdir,wd_std,dsr,usr,dlr,ulr,t_rad,z_boom,z_boom_q,z_stake,z_stake_q,t_i_1,t_i_2,t_i_3,t_i_4,t_i_5,t_i_6,t_i_7,t_i_8,tilt_x,tilt_y,gps_time,gps_lat,gps_lon,gps_alt,gps_geoid,gps_q,gps_numsat,gps_hdop,t_log,fan_dc
#+NAME: make_L0M_zac_a
#+BEGIN_SRC bash
station_id=zac_a
sourcedir=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0/$station_id
destdir=/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/$station_id

years=2009-2010
endyear=2010 

<<concat_nead_and_data>>

years=2010-2011
endyear=2011 
<<concat_nead_and_data>>

years=2011-2012
endyear=2012 
<<concat_nead_and_data>>

years=2012-2013
endyear=2013 
<<concat_nead_and_data>>


years=2013-2014
endyear=2014 
<<concat_nead_and_data>>


years=2014-2015
endyear=2015 
<<concat_nead_and_data>>

years=2015-2016
endyear=2016 
<<concat_nead_and_data>>

years=2016-2017
endyear=2017 
<<concat_nead_and_data>>

years=2017-2018
endyear=2018 
<<concat_nead_and_data>>

years=2018-2019
endyear=2019 
<<concat_nead_and_data>>

echo "ZAC_A data now has nead headers"

#+END_SRC

#+RESULTS: make_L0M_zac_a




** Get best guess for PTA calibration coefficient for 2015-2022

#+BEGIN_SRC jupyter-python
import nead
import pandas as pd
import numpy as np
import os
import glob
import re
from datetime import datetime

workingdir ='/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/'  
station = 'zac_l'
#<<convert_to_physical_values>>

filenamestart = workingdir+'data_v1.0/L0M/'+station+'/'+station

infile = filenamestart +'-2016.csv'
<<read_infile_from_filelist>>
<<add_variable_metadata>>
ds = add_variable_metadata(ds)
fig, ax = plt.subplots(1,1)
ds['z_pt'].to_pandas().resample('D').mean().loc['1-July-2015':'9-July-2015'].plot(ax = ax)


icemeltstart = datetime(2015,7,5)

pta = -(ds['z_pt']-ds['z_pt'].loc[icemeltstart].mean())
sr50 = ds['z_stake']-ds['z_stake'].loc[icemeltstart].mean()

rho_af = 1092
pt_z_coef = 0.51
pt_z_factor = 2.5
pta_corr = ds['z_pt'] * pt_z_coef * pt_z_factor * 998.0 / rho_af \
        + 100 * (pt_z_coef - ds['p']) / (rho_af * 9.81)

pta_corr = -(pta_corr-pta_corr.loc[icemeltstart].mean())

fig, ax = plt.subplots(1,1)
pta.plot(ax=ax, label = 'pta')
pta_corr.plot(ax=ax, label = 'pta_corr')
sr50.plot(ax=ax, label = 'sr50')
ax.set_ylim(-1,2)
ax.set_xlim(datetime(2015,7,1),datetime(2015,9,1))
ax.legend()

#+END_SRC

#+RESULTS:
:RESULTS:
: /tmp/ipykernel_17954/3764014050.py:390: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
: Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
:   if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
: <matplotlib.legend.Legend at 0x7f5288253f40>
[[file:./.ob-jupyter/b53fcc194c67a174a1c544785a2c61cc077547af.png]]
[[file:./.ob-jupyter/37a55ccc8ba3ca07fff1d6cb88d82a36e441de98.png]]
:END:

#+BEGIN_SRC jupyter-python

infile = filenamestart +'-2017.csv'
<<read_infile_from_filelist>>
<<add_variable_metadata>>
ds = add_variable_metadata(ds)
fig, ax = plt.subplots(1,1)
ds['z_pt'].to_pandas().resample('D').mean().loc['1-June-2016':'31-July-2016'].plot(ax = ax)


icemeltstart = datetime(2016,7,5)

pta = -(ds['z_pt']-ds['z_pt'].loc[icemeltstart].mean())
sr50 = ds['z_stake']-ds['z_stake'].loc[icemeltstart].mean()

rho_af = 1092
pt_z_coef = 0.51
pt_z_factor = 2.5
pta_corr = ds['z_pt'] * pt_z_coef * pt_z_factor * 998.0 / rho_af \
        + 100 * (pt_z_coef - ds['p']) / (rho_af * 9.81)

pta_corr = -(pta_corr-pta_corr.loc[icemeltstart].mean())

fig, ax = plt.subplots(1,1)
pta.plot(ax=ax, label = 'pta')
pta_corr.plot(ax=ax, label = 'pta_corr')
sr50.plot(ax=ax, label = 'sr50')
ax.set_ylim(-1,2)
ax.set_xlim(datetime(2016,7,1),datetime(2016,9,1))
ax.legend()

#ds['z_stake'].plot(ax = ax)
#+END_SRC

#+RESULTS:
:RESULTS:
: /tmp/ipykernel_17954/3207614599.py:27: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
: Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
:   if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
: <matplotlib.legend.Legend at 0x7f52b6670fa0>
[[file:./.ob-jupyter/a064ae78c03452973e2a323c412c9d2347db672b.png]]
[[file:./.ob-jupyter/ecee1a9cce7626c13bda9c1727a3c74632ef7a14.png]]
:END:

#+BEGIN_SRC jupyter-python
print(ds.time)
#+END_SRC

#+RESULTS:
: <xarray.DataArray 'time' (time: 50865)>
: array(['2015-05-04T13:00:00.000000000', '2015-05-04T13:10:00.000000000',
:        '2015-05-04T13:20:00.000000000', ..., '2016-04-21T18:00:00.000000000',
:        '2016-04-21T18:10:00.000000000', '2016-04-21T18:20:00.000000000'],
:       dtype='datetime64[ns]')
: Coordinates:
:   * time     (time) datetime64[ns] 2015-05-04T13:00:00 ... 2016-04-21T18:20:00


** Create L1 files: convert to physical values 


*** Debugging: check raw tilt values to make sure the unit is correct  

#+BEGIN_SRC jupyter-python :tangle convert_to_physical_values.py
import nead
import pandas as pd
import numpy as np
import os
import glob
import re
workingdir ='/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/'  

station = 'zac_l'
filelist = glob.glob(workingdir+'data_v1.0/L0M/'+station+'/'+station+'**.csv')
fig, ax = plt.subplots(3,1, figsize = (10,15))
for infile in filelist:
    print(str(infile))
    <<read_infile_from_filelist>>
    <<add_variable_metadata>>
    ds = add_variable_metadata(ds)
    ds_raw = ds.copy()

    ds_raw[['tilt_x','tilt_y']].to_dataframe().plot()
#+END_SRC


#+BEGIN_SRC jupyter-python :tangle convert_to_physical_values.py
station = 'zac_u'
filelist = glob.glob(workingdir+'data_v1.0/L0M/'+station+'/'+station+'**.csv')

for infile in filelist:
    print(str(infile))
    <<read_infile_from_filelist>>
    <<add_variable_metadata>>
    ds = add_variable_metadata(ds)
    ds_raw = ds.copy()
ds_raw[['tilt_x','tilt_y']].to_dataframe().plot(ax = ax[1])

station = 'zac_a'
filelist = glob.glob(workingdir+'data_v1.0/L0M/'+station+'/'+station+'**.csv')

for infile in filelist:
    print(str(infile))
    <<read_infile_from_filelist>>
    <<add_variable_metadata>>
    ds = add_variable_metadata(ds)
    ds_raw = ds.copy()
ds_raw[['tilt_x','tilt_y']].to_dataframe().plot(ax = ax[2])

#+END_SRC

#+RESULTS:
:RESULTS:
#+begin_example
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2017.csv
  /tmp/ipykernel_34786/2429569575.py:39: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2020.csv
  /tmp/ipykernel_34786/2429569575.py:39: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2015.csv
  /tmp/ipykernel_34786/2429569575.py:39: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2016.csv
  /tmp/ipykernel_34786/2429569575.py:39: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2012.csv
  /tmp/ipykernel_34786/2429569575.py:39: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2014.csv
  /tmp/ipykernel_34786/2429569575.py:39: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2018.csv
  /tmp/ipykernel_34786/2429569575.py:39: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2021.csv
  /tmp/ipykernel_34786/2429569575.py:39: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2013.csv
  /tmp/ipykernel_34786/2429569575.py:39: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2022.csv
  /tmp/ipykernel_34786/2429569575.py:39: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2011.csv
  /tmp/ipykernel_34786/2429569575.py:39: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_l/zac_l-2010.csv
  /tmp/ipykernel_34786/2429569575.py:39: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2017.csv
  /tmp/ipykernel_34786/2429569575.py:78: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /tmp/ipykernel_34786/2429569575.py:78: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2016.csv
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2019.csv
  /tmp/ipykernel_34786/2429569575.py:78: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2020.csv
  /tmp/ipykernel_34786/2429569575.py:78: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2010.csv
  /tmp/ipykernel_34786/2429569575.py:78: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2021.csv
  /tmp/ipykernel_34786/2429569575.py:78: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2013.csv
  /tmp/ipykernel_34786/2429569575.py:78: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2012.csv
  /tmp/ipykernel_34786/2429569575.py:78: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2011.csv
  /tmp/ipykernel_34786/2429569575.py:78: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2014.csv
  /tmp/ipykernel_34786/2429569575.py:78: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2015.csv
  /tmp/ipykernel_34786/2429569575.py:78: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_u/zac_u-2022.csv
  /tmp/ipykernel_34786/2429569575.py:78: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2011.csv
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2016.csv
  /tmp/ipykernel_34786/2429569575.py:116: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /tmp/ipykernel_34786/2429569575.py:116: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2012.csv
  /tmp/ipykernel_34786/2429569575.py:116: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2018.csv
  /tmp/ipykernel_34786/2429569575.py:116: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2014.csv
  /tmp/ipykernel_34786/2429569575.py:116: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2017.csv
  /tmp/ipykernel_34786/2429569575.py:116: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2013.csv
  /tmp/ipykernel_34786/2429569575.py:116: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2015.csv
  /tmp/ipykernel_34786/2429569575.py:116: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2010.csv
  /tmp/ipykernel_34786/2429569575.py:116: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
  /home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L0M/zac_a/zac_a-2019.csv
  /tmp/ipykernel_34786/2429569575.py:116: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
#+end_example
: <AxesSubplot: xlabel='time'>
[[file:./.ob-jupyter/1a8e951a303c73b82f0a680c1c1ee96b06728cd8.png]]
:END:


*** Workflow

NB I use the NEAD data format for L0 data. Program installed via:
pip install --upgrade git+https://github.com/GEUS-PROMICE/pyNEAD.git


#+NAME: convert_to_physical_values_workflow
#+BEGIN_SRC jupyter-python :tangle convert_to_physical_values.py
import nead
import pandas as pd
import numpy as np
import os
import glob
import re

workingdir ='/home/shl/OneDrive/projects/aws_processing_v1.0/'  
station = 'zac_l'
<<convert_to_physical_values>>

station = 'zac_u'
<<convert_to_physical_values>>

station = 'zac_a'
<<convert_to_physical_values>>


<<Make_a_single_netcdf_per_station>>
#+END_SRC


#+NAME: convert_to_physical_values
#+BEGIN_SRC jupyter-python
filelist = glob.glob(workingdir+'data_v1.0/L0M/'+station+'/'+station+'**.csv')

for infile in filelist:
    print(str(infile))
    <<read_infile_from_filelist>>
    <<add_variable_metadata>>
    ds = add_variable_metadata(ds)
    #ds_raw = ds.copy()
    <<raw_to_phys>> 
    #ds_phys = ds.copy()
    <<correct_rh_and_rad>>
    <<write_out_L1_nc>>
    
   
#+END_SRC


*** code
**** Debugging

#+NAME: Debugging
#+BEGIN_SRC jupyter-python
import nead
import pandas as pd
import numpy as np
import os
import glob
import matplotlib.pyplot as plt


station = 'zac_l'

workingdir ='/home/shl@geus.dk/Dropbox/GEUS/projects/glaciobasis/aws_processing/'  

#filelist = glob.glob(workingdir+'data/L0M/'+station+'/'+station+'**.csv')

infile = workingdir+'data/L0M/'+station+'/'+station+'-2011.csv'
print(str(infile))
<<read_infile_from_filelist>>
<<add_variable_metadata>>
ds = add_variable_metadata(ds)

<<raw_to_phys>>

#+END_SRC

#+RESULTS: Debugging
:results:
# Out [8]: 
# output
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
/tmp/ipykernel_13278/2143753574.py in <module>
      7 
      8 
----> 9 ds = nead.read(infile)

~/miniconda3/envs/py38/lib/python3.8/site-packages/nead/nead.py in read(neadfile, MKS, multi_index, index_col)
     54             key_eq_val = line.split("#")[1].strip()
     55             if key_eq_val == '' or key_eq_val == None: continue  # Line is just "#" or "# " or "#   #"...
---> 56             assert("=" in key_eq_val), print(line, key_eq_val)
     57             key = key_eq_val.split("=")[0].strip()
     58             val = key_eq_val.split("=")[1].strip()

AssertionError: None
:end:

#+BEGIN_SRC jupyter-python

<<plot_dsr_usr>>

<<correct_rh_and_rad>>
<<plot_dsr_usr>>

<<write_out_L1_nc>>
    
   
#+END_SRC

#+NAME: plot_dsr_usr
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(1,2)
ds.usr.plot(ax = ax[0])
ds.dsr.plot(ax = ax[1])
#+END_SRC

#+BEGIN_SRC jupyter-python
workingdir ='/home/shl@geus.dk/Dropbox/GEUS/projects/glaciobasis/aws_processing/'  
station = 'zac_u'
infile = workingdir+'data/L0M/'+station+'/'+station+'-2013.csv'

ds = nead.read(infile)
liste = list(ds.keys())
print(liste)
#+END_SRC

#+RESULTS:
:results:
# Out [33]: 
# output
['time', 'rec', 'p', 't_1', 't_2', 'rh', 'wspd', 'wdir', 'wd_std', 'dsr', 'usr', 'dlr', 'ulr', 't_rad', 'z_boom', 'z_boom_q', 'tilt_x', 'tilt_y', 't_log', 'fan_dc']

:end:

#+NAME: plot_ds_all_variab
#+BEGIN_SRC jupyter-python
liste = list(ds.keys())
data = {}
for variab in liste:
    data[variab] = ds[variab]


data_df = pd.DataFrame(data)    
data_df.index = pd.to_datetime(data_df['time'])


#data_df.loc[data_df.index > '2014-April-25' ,'t_i_1'].plot()
data_df.plot(subplots=True, layout=(6,7), figsize=(30,30))
#print(ds['z_pt'].dtype)
#ds['z_pt_corr'].plot()
#+END_SRC

#+RESULTS:
:results:
# Out [34]: 
# text/plain
: array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fe08bbd4bb0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0ab0e1e50>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a81c80d0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a7225d30>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a71d9310>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a72038b0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a720f850>],
:        [<matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a71bae20>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a718d9a0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a7139f40>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a70ee520>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a7099ac0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a70cf0a0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a7079640>],
:        [<matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a7024be0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a6fdc1c0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a7004760>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a6f2fd00>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a6ee52e0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a6d9c760>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a6dc6040>],
:        [<matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a6d6e790>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a6cd7f10>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a6d0c6d0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a079be50>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a038f610>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a033ad90>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a036d550>],
:        [<matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a0317cd0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a02cc490>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a02f6c10>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a02ac3d0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a0253b50>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a020a310>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a0231a90>],
:        [<matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a01e7250>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a01909d0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a0147190>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a016e910>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a01260d0>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a00ce850>,
:         <matplotlib.axes._subplots.AxesSubplot object at 0x7fe0a0046040>]],
:       dtype=object)

# text/plain
: <Figure size 2160x2160 with 42 Axes>

# image/png
[[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/e83682bc5d7086478aec9ea5649bfef18eb458c9.png]]
:end:



**** read files and add metadata
#+NAME: read_infile_from_filelist
#+BEGIN_SRC jupyter-python
#infile = filelist[0]
ds = nead.read(infile)
ds = ds.set_coords(['time'])
ds = ds.set_index({'index':'time'})
ds = ds.rename({'index':'time'})
ds['time'] = pd.to_datetime(ds.time.values)
ds['n'] = (('time'), np.arange(ds.time.size)+1)

# Remove duplicate dates
_, index_dublicates = np.unique(ds['time'], return_index=True)
ds = ds.isel(time=index_dublicates)

# Remove inf
for column in ds.keys():
    ds[column][ds[column]==np.inf] = np.nan


#+END_SRC


#+NAME: add_variable_metadata
#+BEGIN_SRC jupyter-python
def add_variable_metadata(ds):
    """Uses the variable DB (variables.csv) to add metadata to the xarray dataset."""
    df = pd.read_csv("./variables.csv", index_col=0, comment="#")

    for v in df.index:
        if v == 'time': continue # coordinate variable, not normal var
        if v not in list(ds.variables): continue
        for c in ['standard_name', 'long_name', 'units']:
            if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
            ds[v].attrs[c] = df[c][v]
            
    return ds
#+END_SRC


**** Dataprocessing code

***** raw_to_phys
#+NAME: raw_to_phys
#+BEGIN_SRC jupyter-python
T_0 = 273.15

# Calculate pressure transducer fluid density

if 'z_pt' in ds:
    if ds.attrs['pt_antifreeze'] == 50:
        rho_af = 1092
    elif ds.attrs['pt_antifreeze'] == 100:
        rho_af = 1145
    else:
        rho_af = np.nan
        print("ERROR: Incorrect metadata: 'pt_antifreeze =' ", ds.attrs['pt_antifreeze'])
        print("Antifreeze mix only supported at 50 % or 100%")
        # assert(False)
    
for v in ['gps_geounit','min_y']:
    if v in list(ds.variables): ds = ds.drop_vars(v)


# convert radiation from engineering to physical units
if 'dsr' in ds:
        
    ds['dsr'] = (ds['dsr']*10) / ds.attrs['dsr_eng_coef'] * 100
    ds['usr'] = (ds['usr']*10) / ds.attrs['usr_eng_coef'] * 100
    ds['dlr'] = ((ds['dlr']*1000) / ds.attrs['dlr_eng_coef']) + 5.67*10**(-8)*(ds['t_rad'] + T_0)**4 
    ds['ulr'] = ((ds['ulr']*1000) / ds.attrs['ulr_eng_coef']) + 5.67*10**(-8)*(ds['t_rad'] + T_0)**4
    
    ds['tilt_x'] = ds['tilt_x'].astype(float) / 100
    ds['tilt_y'] = ds['tilt_y'].astype(float) / 100

# Adjust sonic ranger readings for sensitivity to air temperature
if 'z_boom' in ds:
    ds['z_boom'] = ds['z_boom'] * ((ds['t_1'] + T_0)/T_0)**0.5 
if 'z_stake' in ds:
    ds['z_stake'] = ds['z_stake'] * ((ds['t_1'] + T_0)/T_0)**0.5

# Adjust pressure transducer due to fluid properties
if 'z_pt' in ds:
    #print('z_pt_corr is produced in' + str(infile) )
    #ds['z_pt'] = ds['z_pt'] * ds.attrs['pt_z_coef'] * ds.attrs['pt_z_factor'] * 998.0 / rho_af

    # Calculate pressure transducer depth
    ds['z_pt_corr'] = ds['z_pt'] * np.nan # new 'z_pt_corr' copied from 'z_pt'
    ds['z_pt_corr'].attrs['long_name'] = ds['z_pt'].long_name + " corrected"
    ds['z_pt_corr'] = ds['z_pt'] * ds.attrs['pt_z_coef'] * ds.attrs['pt_z_factor'] * 998.0 / rho_af \
        + 100 * (ds.attrs['pt_z_p_coef'] - ds['p']) / (rho_af * 9.81)


# Decode GPS
if 'gps_lat' in ds:
    if ds['gps_lat'].dtype.kind == 'O': # not a float. Probably has "NH"
        #assert('NH' in ds['gps_lat'].dropna(dim='time').values[0])
        for v in ['gps_lat','gps_lon','gps_time']:
            a = ds[v].attrs # store
            str2nums = [re.findall(r"[-+]?\d*\.\d+|\d+", _) if isinstance(_, str) else [np.nan] for _ in ds[v].values]
            ds[v][:] = pd.DataFrame(str2nums).astype(float).T.values[0]
            ds[v] = ds[v].astype(float)
            ds[v].attrs = a # restore

    if np.any((ds['gps_lat'] <= 90) & (ds['gps_lat'] > 0)):  # Some stations only recorded minutes, not degrees
        xyz = np.array(re.findall("[-+]?[\d]*[.][\d]+", ds.attrs['geometry'])).astype(float)
        x=xyz[0]; y=xyz[1]; z=xyz[2] if len(xyz) == 3 else 0
        p = shapely.geometry.Point(x,y,z)
        # from IPython import embed; embed()
        # assert(False) # should p be ints rather than floats here?
        # ds['gps_lat'] = ds['gps_lat'].where(
        ds['gps_lat'] = ds['gps_lat'] + 100*p.y
    if np.any((ds['gps_lon'] <= 90) & (ds['gps_lon'] > 0)):
        ds['gps_lon'] = ds['gps_lon'] + 100*p.x

    for v in ['gps_lat','gps_lon']:
        a = ds[v].attrs # store
        ds[v] = np.floor(ds[v] / 100) + (ds[v] / 100 - np.floor(ds[v] / 100)) * 100 / 60
        ds[v].attrs = a # restore


# Correct winddir due to boom_azimuth

# ds['ws'].

# tilt-o-meter voltage to degrees
# if transmitted ne 'yes' then begin
#    tiltX = smooth(tiltX,7,/EDGE_MIRROR,MISSING=-999) & tiltY = smooth(tiltY,7,/EDGE_MIRROR, MISSING=-999)
# endif

# Should just be
# if ds.attrs['PROMICE_format'] != 'TX': dstxy = dstxy.rolling(time=7, win_type='boxcar', center=True).mean()
# but the /EDGE_MIRROR makes it a bit more complicated...

if 'tilt_x' in ds:
    win_size=7
    s = np.int(win_size/2)
    tdf = ds['tilt_x'].to_dataframe()
    ds['tilt_x'] = (('time'), tdf.iloc[:s][::-1].append(tdf).append(tdf.iloc[-s:][::-1]).rolling(win_size, win_type='boxcar',     center=True).mean()[s:-s].values.flatten())
    tdf = ds['tilt_y'].to_dataframe()
    ds['tilt_y'] = (('time'), tdf.iloc[:s][::-1].append(tdf).append(tdf.iloc[-s:][::-1]).rolling(win_size, win_type='boxcar',    center=True).mean()[s:-s].values.flatten())

    # # notOKtiltX = where(tiltX lt -100, complement=OKtiltX) & notOKtiltY = where(tiltY lt -100, complement=OKtiltY)
    notOKtiltX = (ds['tilt_x'] < -100)
    OKtiltX = (ds['tilt_x'] >= -100)
    notOKtiltY = (ds['tilt_y'] < -100)
    OKtiltY = (ds['tilt_y'] >= -100)

    # tiltX = tiltX/10.
    #ds['tilt_x'] = ds['tilt_x'] / 10
    #ds['tilt_y'] = ds['tilt_y'] / 10

    # tiltnonzero = where(tiltX ne 0 and tiltX gt -40 and tiltX lt 40)
    # if n_elements(tiltnonzero) ne 1 then tiltX[tiltnonzero] = tiltX[tiltnonzero]/abs(tiltX[tiltnonzero])*(-0.49*(abs(tiltX[tiltnonzero]))^4 +   3.6*(abs(tiltX[tiltnonzero]))^3 - 10.4*(abs(tiltX[tiltnonzero]))^2 +21.1*(abs(tiltX[tiltnonzero])))

    # tiltY = tiltY/10.
    # tiltnonzero = where(tiltY ne 0 and tiltY gt -40 and tiltY lt 40)
    # if n_elements(tiltnonzero) ne 1 then tiltY[tiltnonzero] = tiltY[tiltnonzero]/abs(tiltY[tiltnonzero])*(-0.49*(abs(tiltY[tiltnonzero]))^4 + 3.6*(abs(tiltY[tiltnonzero]))^3 - 10.4*(abs(tiltY[tiltnonzero]))^2 +21.1*(abs(tiltY[tiltnonzero])))

    dstx = ds['tilt_x']
    nz = (dstx != 0) & (np.abs(dstx) < 40)
    dstx = dstx.where(~nz, other = dstx / np.abs(dstx) * (-0.49 * (np.abs(dstx))**4 + 3.6 * (np.abs(dstx))**3 - 10.4 * (np.abs(dstx))**2 + 21.1 * (np.abs(dstx))))
    ds['tilt_x'] = dstx

    dsty = ds['tilt_y']
    nz = (dsty != 0) & (np.abs(dsty) < 40)
    dsty = dsty.where(~nz, other = dsty / np.abs(dsty) * (-0.49 * (np.abs(dsty))**4 + 3.6 * (np.abs(dsty))**3 - 10.4 * (np.abs(dsty))**2 + 21.1 * (np.abs(dsty))))
    ds['tilt_y'] = dsty

    # if n_elements(OKtiltX) gt 1 then tiltX[notOKtiltX] = interpol(tiltX[OKtiltX],OKtiltX,notOKtiltX) ; Interpolate over gaps for radiation correction; set to -999 again below.
    # if n_elements(OKtiltY) gt 1 then tiltY[notOKtiltY] = interpol(tiltY[OKtiltY],OKtiltY,notOKtiltY) ; Interpolate over gaps for radiation correction; set to -999 again below.

    ds['tilt_x'] = ds['tilt_x'].where(~notOKtiltX)
    ds['tilt_y'] = ds['tilt_y'].where(~notOKtiltY)
    ds['tilt_x'] = ds['tilt_x'].interpolate_na(dim='time')
    ds['tilt_y'] = ds['tilt_y'].interpolate_na(dim='time')

# ds['tilt_x'] = ds['tilt_x'].ffill(dim='time')
# ds['tilt_y'] = ds['tilt_y'].ffill(dim='time')


deg2rad = np.pi / 180
ds['wdir'] = ds['wdir'].where(ds['wspd'] != 0)
ds['wspd_x'] = ds['wspd'] * np.sin(ds['wdir'] * deg2rad)
ds['wspd_y'] = ds['wspd'] * np.cos(ds['wdir'] * deg2rad)
    
 #+END_SRC




#+BEGIN_SRC jupyter-python
import matplotlib.pyplot as plt
#sel = (dates.index > pd.to_datetime('2009-05-01', format = '%Y-%m-%d')) & (dates.index < pd.to_datetime('2009-05-04', format = '%Y-%m-%d'))
sel = (dates.index.year==2009) & (dayofyear > 200) & (dayofyear < 203)
#plt.plot(dates.index.values[sel],theta_sensor_rad[sel])
plt.plot(dates.index.values[sel],ds['tilt_x'][sel])

#+END_SRC

#+RESULTS:
:results:
# Out [53]: 
# text/plain
: [<matplotlib.lines.Line2D at 0x7f1021f8e6d0>]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/9331103c48eebae828ac3eb2a391dcbe0fb7c304.png]]
:end:


***** Correct rh and rad

 #+NAME: correct_rh_and_rad
 #+BEGIN_SRC jupyter-python
<<correct_RH>>
<<calc_t_surf>>
if 'dsr' in ds:
    <<correct_short_wave_radiation_for_tilt>>
    

 #+END_SRC

 #+RESULTS: correct_rh_and_rad
 :results:
 # Out [2]: 
 :end:


****** Correct RH water to be relative to ice when T_air is freezing :noexport:
  Relative humidity is recorded as over water - it needs to be corrected as it is over ice when the surface is freezing

#+NAME: correct_RH
#+BEGIN_SRC jupyter-python :tangle correct_RH.py
below = ds['t_1'].values < 0
T = ds['t_1'].values + 273.15

ew = 10**(-7.90298*(373.16/T-1)+5.02808*np.log10(373.16/T)-(1.3816*10**(-7))*(10**(11.344*(1-T/373.16))-1)+(8.1328*10**(-3))*(10**(-349149*(373.16/T-1))-1)+np.log10(1013.246))

#ew = 10**(-7.90298*(373.16/T-1)+5.02808*np.log10(373.16/T)-(1.3816*10**(-7))*(10**(11.344*(1-T/373.16))-1)+(8.1328*10**(-3))*(10**(-349149*(373.16/T-1))-1)+np.log10(1013.246))

ei = 10**(-9.09718*(273.16/T-1)-3.56654*np.log10(273.16/T)+0.876793*(1-T/273.16)+np.log10(6.1071))

rh_ice = ds['rh'].values*ew/ei

rh = ds['rh'].copy()
rh[below] = rh_ice[below]
#rh[rh>100] = 100
rh[rh<0] = 0
ds['rh_corr'] = rh.copy()
ds['rh_corr'].attrs['long_name'] = ds['rh'].long_name + " corrected"          

  #+END_SRC

  #+RESULTS: correct_RH
  :results:
  # Out [35]: 
  :end:

 #+BEGIN_SRC jupyter-python
ds['rh'].plot()
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [67]: 
 # text/plain
 : [<matplotlib.lines.Line2D at 0x7f5bac7e1f70>]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/bdf2167b88409bfb17ca4dde11cfeee2cdca87b5.png]]
 :end:

 #+BEGIN_SRC jupyter-python
ds['rh_corr'].plot()
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [47]: 
 # text/plain
 : [<matplotlib.lines.Line2D at 0x7f5bac66e850>]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/3dfff018b48d608bcaaf993d9a28cbb5df998eff.png]]
 :end:


****** calc_t_surf                                                 :noexport:


#+NAME: calc_t_surf
#+BEGIN_SRC jupyter-python :tagle calculate_t_surface.py
T_0 = 273.15
epsilon = 0.97
sigma = 5.67*10**(-8)
Tsurf = ((ds['ulr']-(1-epsilon)*ds['dlr'])/(epsilon*sigma))**0.25 -T_0
ds = ds.assign({'t_surf':Tsurf})

 #+END_SRC

 #+RESULTS: calc_t_surf
 :RESULTS:
 | <matplotlib.lines.Line2D | at | 0x7f2fa0b49d00> |
 [[file:./.ob-jupyter/49dbfd12fda62324324525f008cc43de8c3b4c28.png]]
 :END:


****** Correct short wave radiation for tilt                       :noexport:
 Correcting Short wave radiation for tilt according to MacWhorter and Weller 1990
 This code is adapted from the promice processing idl code version 2012.

 #+NAME: correct_short_wave_radiation_for_tilt
 #+BEGIN_SRC jupyter-python :tagle correct_shortwave_radiation_for_tilt.py
<<cloud_cover_tsurf>>
<<tilt_angle_rotate>>
<<zenith_and_hour_angle>>
<<correction_factor_for_direct_beam_radiation>>
<<corr_srin_for_tilt>>
<<ok_albedos>>
<<corr_sr_diffuse_radiation>>
<<corr_large_zenith_angles>>
<<corr_srin_upper_sensor_not_in_sight>>
<<removing_spikes>> #The spike removal can disquise problems
<<adding_columns>>

 #+END_SRC



******* Code                                                       :noexport:
 Calculate cloud cover for SRin correction and surface temperature
 #+NAME: cloud_cover_tsurf
 #+BEGIN_SRC jupyter-python :tangle correct_sw_for_tilt.py
# Calculate cloud cover
T_0 = 273.15
eps_overcast = 1
eps_clear = 9.36508e-6
LR_overcast = eps_overcast*5.67*10**(-8)*(ds['t_1']+T_0)**4 # assumption
LR_clear = eps_clear*5.67*10**(-8)*(ds['t_1']+T_0)**6 # Swinbank (1963)
CloudCov = (ds['dlr'].values-LR_clear)/(LR_overcast-LR_clear)

overcast = CloudCov > 1
Clear = CloudCov < 0
CloudCov[overcast] = 1
CloudCov[Clear] = 0
DifFrac = 0.2+0.8*CloudCov


 #+END_SRC

 #+RESULTS: cloud_cover_tsurf
 :results:
 # Out [32]: 
 :end:



#+BEGIN_SRC jupyter-python
plt.plot(CloudCov)
#+END_SRC

#+RESULTS:
:results:
# Out [34]: 
# text/plain
: [<matplotlib.lines.Line2D at 0x7f160554b310>]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/91f647bcc404b549585361d9618b3381c35ea7ea.png]]
:end:

 Calculating the tilt angle and direction of senson and rotating to a north-south aligned coordinate system

#+NAME: tilt_angle_rotate
#+BEGIN_SRC jupyter-python :tangle correct_sw_for_tilt.py
# Calculating the tilt angle and direction of senson and rotating to a north-south aligned coordinate system
deg2rad = np.pi / 180
tiltX_rad = ds['tilt_x'].values*deg2rad
tiltY_rad = ds['tilt_y'].values*deg2rad

X = np.sin(tiltX_rad)*np.cos(tiltX_rad)*(np.sin(tiltY_rad))**2 + np.sin(tiltX_rad)*(np.cos(tiltY_rad))**2 # Cartesian coordinate
Y = np.sin(tiltY_rad)*np.cos(tiltY_rad)*(np.sin(tiltX_rad))**2 + np.sin(tiltY_rad)*(np.cos(tiltX_rad))**2 # Cartesian coordinate
Z = np.cos(tiltX_rad)*np.cos(tiltY_rad) + (np.sin(tiltX_rad))**2*(np.sin(tiltY_rad))**2 # Cartesian coordinate
phi_sensor_rad = -np.pi/2-np.arctan(Y/X) # spherical coordinate

phi_sensor_rad[X > 0] = phi_sensor_rad[X > 0]+np.pi
phi_sensor_rad[(X == 0) & (Y < 0)] = np.pi
phi_sensor_rad[(X == 0) & (Y >= 0)] = 0
phi_sensor_rad[phi_sensor_rad < 0] = phi_sensor_rad[phi_sensor_rad < 0]+2*np.pi

phi_sensor_deg = phi_sensor_rad*180/np.pi # radians to degrees
theta_sensor_rad = np.arccos(Z/(X**2+Y**2+Z**2)**0.5) # spherical coordinate (or actually total tilt of the sensor, i.e. 0 when horizontal)
theta_sensor_deg = theta_sensor_rad*180/np.pi # radians to degrees



 #+END_SRC

 #+RESULTS: tilt_angle_rotate
 :results:
 # Out [35]: 
 # output
 <ipython-input-35-807541dbfffd>:8: RuntimeWarning: divide by zero encountered in true_divide
   phi_sensor_rad = -np.pi/2-np.arctan(Y/X) # spherical coordinate

 :end:
#+BEGIN_SRC jupyter-python

plt.plot(theta_sensor_deg)
#plt.plot(X)
#ds['dlr_corr'].plot()
#+END_SRC

#+RESULTS:
:results:
# Out [50]: 
# text/plain
: [<matplotlib.lines.Line2D at 0x7f16053c5790>]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/db35d548c9db588cbf02bf7929edf49622f02527.png]]
:end:



 #+BEGIN_SRC jupyter-python 
import matplotlib.pyplot as plt
#sel = (dates.index > pd.to_datetime('2009-05-01', format = '%Y-%m-%d')) & (dates.index < pd.to_datetime('2009-05-04', format = '%Y-%m-%d'))
sel = (dates.index.year==2009) & (dayofyear > 200) & (dayofyear < 203)
#plt.plot(dates.index.values[sel],theta_sensor_rad[sel])
plt.plot(dates.index.values,ds['tilt_x'])

 #+END_SRC

 #+RESULTS:
 :results:
 # Out [38]: 
 # text/plain
 : [<matplotlib.lines.Line2D at 0x7f1023055d60>]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/c9fb39dc3f980d2cb9cb3f629ba7328bb89b7cf1.png]]
 :end:

 Calculating zenith and hour angle of the sun
 #+NAME: zenith_and_hour_angle
 #+BEGIN_SRC jupyter-python :tangle correct_sw_for_tilt.py
# Calculating zenith and hour angle of the sun
lat = float(ds.geometry[13:19]) #ds['gps_lat'].median().values
lon = float(ds.geometry[6:12]) #ds['gps_lon'].median().values
dates = ds.time.to_dataframe()
dates.index = pd.to_datetime(dates['time'])
dayofyear =dates.index.dayofyear.values
hour = dates.index.hour.values
minute = dates.index.minute.values

d0_rad = 2*np.pi*(dayofyear+(hour+minute/60)/24-1)/365
Declination_rad = np.arcsin(0.006918-0.399912*np.cos(d0_rad)+0.070257*np.sin(d0_rad)-0.006758*np.cos(2*d0_rad)+0.000907*np.sin(2*d0_rad)-0.002697*np.cos(3*d0_rad)+0.00148*np.sin(3*d0_rad))

HourAngle_rad = 2*np.pi*(((hour+minute/60.)/24-0.5))# - lon/360) #- 15.*timezone/360.) ; NB: Make sure time is in UTC and longitude is positive when west! Hour angle should be 0 at noon.
DirectionSun_deg = HourAngle_rad*180/np.pi-180 # This is 180 deg at noon (NH), as opposed to HourAngle.
DirectionSun_deg[DirectionSun_deg < 0] = DirectionSun_deg[DirectionSun_deg < 0]+360
DirectionSun_deg[DirectionSun_deg < 0] = DirectionSun_deg[DirectionSun_deg < 0]+360

ZenithAngle_rad = np.arccos(np.cos(lat*np.pi/180)*np.cos(Declination_rad)*np.cos(HourAngle_rad) + np.sin(lat*np.pi/180)*np.sin(Declination_rad))
ZenithAngle_deg = ZenithAngle_rad*180/np.pi
sundown = ZenithAngle_deg >= 90
SRtoa = 1372*np.cos(ZenithAngle_rad) # SRin at the top of the atmosphere
SRtoa[sundown] = 0




 #+END_SRC

 #+RESULTS: zenith_and_hour_angle
 :results:
 # Out [51]: 
 :end:


 #+BEGIN_SRC jupyter-python
#import matplotlib.pyplot as plt
#sel = (dates.index > pd.to_datetime('2009-05-01', format = '%Y-%m-%d')) & (dates.index < pd.to_datetime('2009-05-04', format = '%Y-%m-%d'))
plt.plot(ZenithAngle_rad)

 #+END_SRC

 #+RESULTS:
 :RESULTS:
 : (51397,)
 : (51397,)
 [[file:./.ob-jupyter/e1b3ff0c7f3f9db16b7ee26e11fdaab46c6c37a9.png]]
 :END:



 http://solardat.uoregon.edu/SolarRadiationBasics.html


% Calculating the correction factor for direct beam radiation (http://solardat.uoregon.edu/SolarRadiationBasics.html)
CorFac = sin(Declination_rad).*sin(lat*pi/180.).*cos(theta_sensor_rad)...
        -sin(Declination_rad).*cos(lat*pi/180.).*sin(theta_sensor_rad).*cos(phi_sensor_rad+pi) ...
        +cos(Declination_rad).*cos(lat*pi/180.).*cos(theta_sensor_rad).*cos(HourAngle_rad) ...
        +cos(Declination_rad).*sin(lat*pi/180.).*sin(theta_sensor_rad).*cos(phi_sensor_rad+pi).*cos(HourAngle_rad) ...
        +cos(Declination_rad).*sin(theta_sensor_rad).*sin(phi_sensor_rad+pi).*sin(HourAngle_rad);





 #+NAME: correction_factor_for_direct_beam_radiation
 #+BEGIN_SRC jupyter-python :tangle correct_sw_for_tilt.py
# correction factor for direct beam radiation
CorFac = np.sin(Declination_rad) * np.sin(lat*np.pi/180.) * np.cos(theta_sensor_rad) \
         -np.sin(Declination_rad) * np.cos(lat*np.pi/180.) * np.sin(theta_sensor_rad) * np.cos(phi_sensor_rad+np.pi) \
        +np.cos(Declination_rad) * np.cos(lat*np.pi/180.) * np.cos(theta_sensor_rad) * np.cos(HourAngle_rad) \
        +np.cos(Declination_rad) * np.sin(lat*np.pi/180.) * np.sin(theta_sensor_rad) * np.cos(phi_sensor_rad+np.pi) * np.cos(HourAngle_rad) \
        +np.cos(Declination_rad) * np.sin(theta_sensor_rad)*np.sin(phi_sensor_rad+np.pi)*np.sin(HourAngle_rad)

CorFac = np.cos(ZenithAngle_rad)/CorFac
no_correction = (CorFac <= 0) | ( ZenithAngle_deg > 90) # sun out of field of view upper sensor
CorFac[no_correction] = 1
  #+END_SRC

 #+RESULTS: correction_factor_for_direct_beam_radiation
 :results:
 # Out [33]: 
 :end:

 #+BEGIN_SRC jupyter-python
plt.plot(CorFac)
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [57]: 
 # text/plain
 : [<matplotlib.lines.Line2D at 0x7f16052278e0>]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/01b726a9adda83cfb8e06d570738da3b2988f3b1.png]]
 :end:


 Calculating SRin over a horizontal surface corrected for station/sensor tilt
 #+NAME: corr_srin_for_tilt
 #+BEGIN_SRC jupyter-python :tangle correct_sw_for_tilt.py
 # Calculating SRin over a horizontal surface corrected for station/sensor tilt
CorFac_all = CorFac/(1-DifFrac+CorFac*DifFrac)
SRin_cor = ds['dsr']*CorFac_all
#srin_tilt_cor = SRin_cor.copy() # Debuggin
 #+END_SRC

 #+RESULTS: corr_srin_for_tilt
 :results:
 # Out [58]: 
 :end:

 #+BEGIN_SRC jupyter-python
plt.plot(SRin_cor)
#plt.ylim([0,1000])
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [59]: 
 # text/plain
 : [<matplotlib.lines.Line2D at 0x7f160517eaf0>]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/c289450a32d53aef4d0a34439312cae89f91754c.png]]
 :end:



 Calculating albedo based on albedo values when sun is in sight of the upper sensor
 #+NAME: ok_albedos
 #+BEGIN_SRC jupyter-python :tangle correct_sw_for_tilt.py
# Calculating albedo based on albedo values when sun is in sight of the upper sensor
AngleDif_deg = 180/np.pi*np.arccos(np.sin(ZenithAngle_rad)*np.cos(HourAngle_rad+np.pi)*np.sin(theta_sensor_rad)*np.cos(phi_sensor_rad) \
                             +np.sin(ZenithAngle_rad)*np.sin(HourAngle_rad+np.pi)*np.sin(theta_sensor_rad)*np.sin(phi_sensor_rad) \
                             +np.cos(ZenithAngle_rad)*np.cos(theta_sensor_rad)) # angle between sun and sensor


albedo = ds['usr']/SRin_cor


OKalbedos = (AngleDif_deg < 70) & (ZenithAngle_deg < 70) & (albedo < 1) & (albedo > 0)

notOKalbedos = (AngleDif_deg >= 70) | (ZenithAngle_deg >= 70) | (albedo >= 1) | (albedo <= 0)

albedo[notOKalbedos] = np.nan
albedo = albedo.ffill('time')
#albedo = interp1(datenumber,albedo,datenumber,'pchip') # interpolate over gaps - gives problems for discontinuous data sets, but is not the end of the world

 #+END_SRC

 #+RESULTS: ok_albedos

#+BEGIN_SRC jupyter-python
print(OKalbedos)
#+END_SRC

#+RESULTS:
:results:
# Out [62]: 
# output
<xarray.DataArray (time: 52852)>
array([False, False, False, ...,  True, False,  True])
Coordinates:
  * time     (time) datetime64[ns] 2016-04-21T17:00:00 ... 2017-04-26T15:40:00

:end:


 #+BEGIN_SRC jupyter-python
plt.plot(OKalbedos)
#plt.ylim([0,1000])
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [63]: 
 # text/plain
 : [<matplotlib.lines.Line2D at 0x7f1605127340>]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/0db119809caeda1c79694d1444edce09c974fbf3.png]]
 :end:

 Correcting SR using SRin when sun is in field of view of lower sensor assuming sensor measures only diffuse radiation

 #+NAME: corr_sr_diffuse_radiation
 #+BEGIN_SRC jupyter-python :tangle correct_sw_for_tilt.py
# Correcting SR using SRin when sun is in field of view of lower sensor assuming sensor measures only diffuse radiation
sunonlowerdome = (AngleDif_deg >= 90) & (ZenithAngle_deg <= 90)
SRin_cor[sunonlowerdome] = ds['dsr'][sunonlowerdome].values/DifFrac[sunonlowerdome]


SRout_cor = ds['usr']
SRout_cor[sunonlowerdome] = albedo[sunonlowerdome]*ds['dsr'][sunonlowerdome].values/DifFrac[sunonlowerdome]
#srin_cor_dome = SRin_cor.copy() # debugging
 #+END_SRC

 #+RESULTS: corr_sr_diffuse_radiation
 :results:
 # Out [64]: 
 :end:

#+BEGIN_SRC jupyter-python
plt.plot(SRout_cor)
#+END_SRC

#+RESULTS:
:results:
# Out [67]: 
# text/plain
: [<matplotlib.lines.Line2D at 0x7f1605648220>]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/426e37f59fff783f18cd28f42d2c4dfc023f4b7a.png]]
:end:

 Setting SRin and SRout to zero for solar zenith angles larger than 95 deg or either SRin or SRout are (less than) zero

 #+NAME: corr_large_zenith_angles
 #+BEGIN_SRC jupyter-python :tangle correct_sw_for_tilt.py
# Setting SRin and SRout to zero for solar zenith angles larger than 95 deg or either SRin or SRout are (less than) zero
no_SR = (ZenithAngle_deg > 95) | (SRin_cor <= 0) | (SRout_cor <= 0)

SRin_cor[no_SR] = 0
SRout_cor[no_SR] = 0

 #+END_SRC

 #+RESULTS: corr_large_zenith_angles
 :results:
 # Out [68]: 
 :end:
 #+BEGIN_SRC jupyter-python
plt.plot(SRin_cor)
#+END_SRC

#+RESULTS:
:results:
# Out [69]: 
# text/plain
: [<matplotlib.lines.Line2D at 0x7f160511a820>]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/b9b8276bdd74614b4c886175a5fe2e1bdfd58f05.png]]
:end:


 % Correcting SRin using more reliable SRout when sun not in sight of upper sensor



 This gives problems for some  of the years
 #+NAME: corr_srin_upper_sensor_not_in_sight
 #+BEGIN_SRC jupyter-python :tangle correct_sw_for_tilt.py
# Correcting SRin using more reliable SRout when sun not in sight of upper sensor

#SRin_cor[no_correction] = SRout_cor[no_correction]/albedo[no_correction]
SRin_cor[~notOKalbedos] = SRout_cor[~notOKalbedos]/albedo[~notOKalbedos]
#SRin_cor_alb = SRin_cor.copy() # Debugging
#SRin_cor = SRout_cor/albedo # What is done in the IDL code
#albedo[notOKalbedos] = -999
 #+END_SRC

 #+RESULTS: corr_srin_upper_sensor_not_in_sight
 :results:
 # Out [70]: 
 :end:

#+BEGIN_SRC jupyter-python
plt.plot(SRin_cor)
#+END_SRC

#+RESULTS:
:results:
# Out [71]: 
# text/plain
: [<matplotlib.lines.Line2D at 0x7f1604fa7a60>]

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/b9b8276bdd74614b4c886175a5fe2e1bdfd58f05.png]]
:end:


 % Removing spikes by interpolation based on a simple top-of-the-atmosphere limitation

 #+NAME: removing_spikes
 #+BEGIN_SRC jupyter-python :tangle correct_sw_for_tilt.py
# Removing spikes by interpolation based on a simple top-of-the-atmosphere limitation

SRin_cor_mark = SRin_cor.copy()
TOA_crit_nopass = SRin_cor > 0.9*SRtoa+10

SRin_cor[TOA_crit_nopass] = np.nan
SRout_cor[TOA_crit_nopass] = np.nan


SRin_cor_final = SRin_cor.copy()

 #+END_SRC

 #+RESULTS: removing_spikes
 :results:
 # Out [72]: 
 :end:

 #+BEGIN_SRC jupyter-python
plt.plot(SRin_cor)
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [76]: 
 # text/plain
 : [<matplotlib.lines.Line2D at 0x7f1604dfc760>]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/74b0a6dcc827939bc64aedc2433f1db30154c8fb/8947f12e7cf13df68082ba45b0e87225e9807a99.png]]
 :end:


 #+NAME: adding_columns
 #+BEGIN_SRC jupyter-python :tangle correct_sw_for_tilt.py
# Assign columns to ds file

ds = ds.assign({'albedo':albedo, 'dsr_corr':SRin_cor, 'usr_corr':SRout_cor, 'cloud_cov':CloudCov})
ds['I'] = ('time', SRtoa)
ds['solar_zenith_angle'] = ('time', ZenithAngle_deg )



#ds_test = ds.assign({'albedo':albedo,'srin_tilt_cor':srin_tilt_cor,'srin_cor_dome':srin_cor_dome, 'SRin_cor_alb':SRin_cor_alb ,'SRin_cor_final':SRin_cor_final,'SRin_cor_mark':SRin_cor_mark , 'dsr_corr':SRin_cor, 'usr_corr':SRout_cor, 'cloud_cov':CloudCov}) # debugging
#ds['dsr_corr']=SRin_cor
ds['dsr_corr'].attrs['long_name'] = ds['dsr'].long_name + " corrected"   
#ds['usr_corr']=SRout_cor.copy()
ds['usr_corr'].attrs['long_name'] = ds['usr'].long_name + " corrected"   

#ds['cloud_cover']=CloudCov.copy()
 #+END_SRC

 #+RESULTS: adding_columns
 #+begin_example
   <xarray.Dataset>
   Dimensions:             (time: 51397, solar_zenith_angle: 51397, I: 51397,
			    Inew: 51397)
   Coordinates:
     ,* time                (time) datetime64[ns] 2018-04-21T14:00:00 ... 2019-04...
     ,* solar_zenith_angle  (solar_zenith_angle) float64 65.0 65.36 ... 65.69 65.67
     ,* I                   (I) float64 575.3 567.3 558.8 ... 559.2 560.2 560.6
     ,* Inew                (Inew) float64 575.3 567.3 558.8 ... 559.2 560.2 560.6
   Data variables: (12/48)
       rec                 (time) float64 31.0 32.0 33.0 ... 5.139e+04 5.139e+04
       p                   (time) float64 840.6 841.0 840.5 ... 843.2 843.5 843.5
       t_1                 (time) float64 -6.473 -7.911 -8.084 ... -1.182 -1.074
       t_2                 (time) float64 -6.394 -7.808 -8.084 ... -0.9944 -0.9998
       rh                  (time) float64 37.28 43.3 45.01 ... 56.74 57.52 57.49
       wspd                (time) float64 0.0 2.974 2.396 ... 5.427 5.403 5.885
       ...                  ...
       rh_corr             (time) float64 39.68 46.73 48.66 ... 57.49 58.14 58.05
       t_surf              (time) float64 -2.871 -2.712 -2.126 ... -5.391 -5.735
       albedo              (time) float64 0.4947 0.4882 0.4847 ... 0.7878 0.7824
       dsr_corr            (time) float64 656.2 658.3 653.0 ... 553.7 628.1 569.8
       usr_corr            (time) float64 324.6 321.4 316.5 ... 425.6 494.8 445.8
       cloud_cov           (time) float64 0.0 0.0 0.0 nan ... 0.3615 0.3472 0.317
   Attributes: (12/16)
       station_id:       zac_a
       field_delimiter:  ,
       nodata:           -9999
       srid:             EPSG:4326
       geometry:         POINT(-21.65, 74.65)
       timezone:         0
       ...               ...
       ulr_eng_coef:     8.26
       pt_z_coef:        0
       pt_z_p_coef:      0
       pt_z_factor:      0
       pt_antifreeze:    50
       boom_azimuth:     50
 #+end_example


**** Write out L1 files
 #+NAME: write_out_L1_nc
 #+BEGIN_SRC jupyter-python

outpath = 'data_v1.0/L1/'+station+'/'
outfile = infile[-14:-4]
ds = ds.sel(time=ds.time.notnull())
#ds_test = ds_test.sel(time=ds.time.notnull()) # debugging

outpathfile = outpath + outfile + ".nc"
#outpathfile_test = outpath + outfile + "_test.nc" #debug
if os.path.exists(outpathfile): os.remove(outpathfile)
ds.to_netcdf(outpathfile, mode='w', format='NETCDF4', compute=True)
#ds_test.to_netcdf(outpathfile_test, mode='w', format='NETCDF4', compute=True) #debug

 #+END_SRC

 #+RESULTS:
 :results:
 # Out [89]: 
 # output
 ---------------------------------------------------------------------------
 ValueError                                Traceback (most recent call last)
 /tmp/ipykernel_13278/2713352631.py in <module>
       1 new_index = pd.date_range(start = ds.time.data[0], end =ds.time.data[-1], freq = '10min')
 ----> 2 ds.reindex(time=new_index, fill_value=np.nan, inplace = True)
       3 print(ds)
       4 #ds = ds.dropna(dim='time', how = 'all')
       5 

 ~/miniconda3/envs/py38/lib/python3.8/site-packages/xarray/core/dataset.py in reindex(self, indexers, method, tolerance, copy, fill_value, **indexers_kwargs)
    2941 
    2942         """
 -> 2943         return self._reindex(
    2944             indexers,
    2945             method,

 ~/miniconda3/envs/py38/lib/python3.8/site-packages/xarray/core/dataset.py in _reindex(self, indexers, method, tolerance, copy, fill_value, sparse, **indexers_kwargs)
    2968         bad_dims = [d for d in indexers if d not in self.dims]
    2969         if bad_dims:
 -> 2970             raise ValueError(f"invalid reindex dimensions: {bad_dims}")
    2971 
    2972         variables, indexes = alignment.reindex_variables(

 ValueError: invalid reindex dimensions: ['inplace']
 :end:

 #+RESULTS: write_out_L1_nc
 :results:
 # Out [87]: 
 # output
 <xarray.Dataset>
 Dimensions:     (time: 51501)
 Coordinates:
   * time        (time) datetime64[ns] 2010-05-13T03:10:00 ... 2011-05-05T18:4...
 Data variables: (12/47)
     rec         (time) float64 ...
     p           (time) float64 ...
     t_1         (time) float64 ...
     t_2         (time) float64 ...
     rh          (time) float64 ...
     wspd        (time) float64 ...
     ...          ...
     wspd_y      (time) float64 ...
     rh_corr     (time) float64 ...
     albedo      (time) float64 ...
     dsr_corr    (time) float64 ...
     usr_corr    (time) float64 ...
     cloud_cov   (time) float64 ...
 Attributes: (12/16)
     station_id:       zac_u
     field_delimiter:  ,
     nodata:           -9999
     srid:             EPSG:4326
     geometry:         POINT(-21.47, 74.64)
     timezone:         0
     ...               ...
     ulr_eng_coef:     0
     pt_z_coef:        0
     pt_z_p_coef:      0
     pt_z_factor:      0
     pt_antifreeze:    50
     boom_azimuth:     240

 :end:

#+BEGIN_SRC jupyter-python
print(ds.time[0].data)
#+END_SRC

#+RESULTS:
:results:
# Out [78]: 
# output
2010-05-13T03:10:00.000000000

:end:
**** Testing what part of the tilt correction that matters in the 'bad' years like 2016

#+BEGIN_SRC jupyter-python

import xarray as xr
ds = xr.load_dataset('data_v1.0/L1/zac_l/zac_l-2016_test.nc')
ds_test = ds[['dsr','dsr_corr','srin_tilt_cor','srin_cor_dome','SRin_cor_alb','SRin_cor_final', 'SRin_cor_mark' ]].to_pandas().loc['June-2015']

fig,ax = plt.subplots(1,1)
#ds_test['srin_tilt_cor'].resample('D').sum().plot(ax = ax, label = 'tilt')
#ds_test['srin_cor_dome'].resample('D').sum().plot(ax = ax, label = 'dome')
#ds_test['SRin_cor_mark'].resample('D').sum().plot(ax = ax, label = 'mark')

#ds_test['SRin_cor_final'].resample('D').sum().plot(ax = ax, label = 'final')
ds_test['dsr'].resample('D').sum().plot(ax = ax, label = 'dsr')
ds_test['dsr_corr'].resample('D').sum().plot(ax = ax, label = 'dsr_corr')
ax.legend()
#ax.set_ylim(35000,45000)

#+END_SRC

#+BEGIN_SRC jupyter-python
ds = xr.load_dataset('data_v1.0/L1/zac_l/zac_l-2017.nc')
ds_sel = ds[['dsr','dsr_corr']].to_pandas().loc['June-2016']

fig,ax = plt.subplots(1,1)

ds_sel['dsr'].resample('D').sum().plot(ax = ax, label = 'dsr')
ds_sel['dsr_corr'].resample('D').sum().plot(ax = ax, label = 'dsr_corr')
ax.legend()
#ax.set_ylim(35000,45000)



 #+END_SRC

 #+RESULTS:
 :RESULTS:
 : <matplotlib.legend.Legend at 0x7f2b9f734400>
 [[file:./.ob-jupyter/5848d238d5f44f6f3bce22c9be01412bc6f82b4b.png]]
 :END:

**** Make one single netcdf per station

 #+NAME: Make_a_single_netcdf_per_station
 #+BEGIN_SRC jupyter-python

 import pandas as pd
 import xarray as xr
 from glob import glob
 import matplotlib.pyplot as plt
 import os

 datadir = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/'
 outfile = datadir+'zac_l/zac_l-2008-2022.nc'
 with xr.open_mfdataset(datadir+'zac_l/zac_l-20??.nc') as ds:
     ds.to_netcdf(outfile)
     ds.to_dataframe().to_csv(datadir+'zac_l/zac_l-2008-2022.txt')

 outfile = datadir+'zac_u/zac_u-2008-2022.nc'
 with xr.open_mfdataset(datadir+'zac_u/zac_u-20??.nc') as ds:
     ds.to_netcdf(outfile)
     ds.to_dataframe().to_csv(datadir+'zac_u/zac_u-2008-2022.txt')

 outfile = datadir+'zac_a/zac_a-2009-2020.nc'
 with xr.open_mfdataset(datadir+'zac_a/zac_a-20??.nc') as ds:
     ds.to_netcdf(outfile)
     ds.to_dataframe().to_csv(datadir+'zac_a/zac_a-2009-2020.txt')


 #+END_SRC

 #+RESULTS:

 #+BEGIN_SRC jupyter-python

 if os.path.isfile(outfile):
     os.remove(outfile)

 ds.to_netcdf(outfile)


 infile = glob(datadir+'zac_u/zac_u-20??.nc')
 print(infile)
 outfile = datadir+'zac_u/zac_u-2008-2022.nc'

 ds = xr.open_dataset(infile[0]).load().dropna(dim='time', how='all')
 for f in infile[1:]:
     tmp = xr.open_mfdataset(f).load().dropna(dim='time', how='all')
     ds = ds.combine_first(tmp)

 if os.path.isfile(outfile):
     os.remove(outfile)

 ds.to_netcdf(outfile)    



 infile = glob(datadir+'zac_a/zac_a-20??.nc')
 outfile = datadir+'zac_a/zac_a-2009-2020.nc'

 ds = xr.open_mfdataset(infile[0]).load().dropna(dim='time', how='all')
 for f in infile[1:]:
     tmp = xr.open_mfdataset(f).load().dropna(dim='time', how='all')
     ds = ds.combine_first(tmp)

 if os.path.isfile(outfile):
     os.remove(outfile)

 ds.to_netcdf(outfile)    


 #+END_SRC

 #+RESULTS:
 :results:
 # Out [12]: 
 # output
 ['/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2013.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2016.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2011.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2015.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2014.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2017.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2010.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2021.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2022.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2020.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2012.nc', '/home/shl/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/zac_u/zac_u-2019.nc']

 :end:



*** Quality check plots


**** dataseries examples
#+BEGIN_SRC jupyter-python 
import xarray as xr
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from glob import glob

filename = 'data_v1.0/L1/zac_a/zac_a-2009-2020.nc'

ds = xr.open_dataset(filename)
ds.tilt_x.plot()

    
ds_filtered = ds.copy()
ds_filtered['p'] = ds['p'].copy()
ds_filtered['p'] = ds['p'].where(ds['p'] > (ds['p'].median() - 100))
ds_filtered['t_1'] = ds['t_1'].where(ds['t_1'] > -50)
ds_filtered['t_2'] = ds['t_2'].where(ds['t_2'] > -50)


fig, ax = plt.subplots(7,1,figsize=(20,20))
ds['dsr_corr'].plot(ax = ax[0])
ds['usr_corr'].plot(ax = ax[0])
ds['dlr'].plot(ax = ax[1])
ds['ulr'].plot(ax = ax[1])
ax[1].set_ylim([0,400])
ds_filtered['p'].plot(ax=ax[2])
ds_filtered['t_1'].plot(ax=ax[3])
ds_filtered['t_2'].plot(ax=ax[3])
ds['rh_corr'].plot(ax=ax[4])
#+END_SRC

#+RESULTS:
:results:
# Out [11]: 
# text/plain
: [<matplotlib.lines.Line2D at 0x7fdb14a999a0>]

# text/plain
: <Figure size 1440x1440 with 7 Axes>

# image/png
[[file:obipy-resources/9a75e1f66bb62abb9073492c1adee0276ae958cd/cb3c704b3b9fccc4606e0ffb16ce77414f383691.png]]
:end:


#+BEGIN_SRC jupyter-python 
import xarray as xr
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from glob import glob

filename = 'data_v1.0/L1/zac_l/zac_l-2008-2022.nc'

ds = xr.open_dataset(filename)

ds_filtered = ds.copy()
ds_filtered['p'] = ds['p'].copy()
ds_filtered['p'] = ds['p'].where(ds['p'] > (ds['p'].median() - 100))
ds_filtered['t_1'] = ds['t_1'].where((ds['t_1'] > -50) & (ds['t_1'] < 50))
ds_filtered['t_2'] = ds['t_2'].where((ds['t_2'] > -50) & (ds['t_2'] < 50))


fig, ax = plt.subplots(5,1,figsize=(20,10))
ds['dsr_corr'].plot(ax = ax[0])
ds['usr_corr'].plot(ax = ax[0])
ds['dlr'].plot(ax = ax[1])
ds['ulr'].plot(ax = ax[1])
ax[1].set_ylim([0,400])
ds_filtered['p'].plot(ax=ax[2])
ds_filtered['t_1'].plot(ax=ax[3])
#ds_filtered['t_2'].plot(ax=ax[3])
ds['rh_corr'].plot(ax=ax[4])

#+END_SRC

#+RESULTS:
:results:
# Out [7]: 
# text/plain
: [<matplotlib.lines.Line2D at 0x7f0ff9809940>]

# text/plain
: <Figure size 1440x720 with 5 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/c08b04ec46e73a8101c21f2d1fd06752ba5e7fae.png]]
:end:


**** Get positions for all three stations
#+BEGIN_SRC jupyter-python 
import xarray as xr
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from glob import glob

filename = 'data_v1.0/L1/zac_l/zac_l-2008-2020.nc'
with xr.open_dataset(filename) as ds:
    lat_l = ds.gps_lat.mean().values
    lon_l = ds.gps_lon.mean().values
    alt_l = ds.gps_alt.mean().values
    

filename = 'data_v1.0/L1/zac_u/zac_u-2008-2020.nc'
with xr.open_dataset(filename) as ds:
    lat_u = ds.gps_lat.mean().values
    lon_u = ds.gps_lon.mean().values
    alt_u = ds.gps_alt.mean().values
filename = 'data_v1.0/L1/zac_a/zac_a-2009-2020.nc'
with xr.open_dataset(filename) as ds:
    lat_a = ds.gps_lat.mean().values
    lon_a = ds.gps_lon.mean().values
    alt_a = ds.gps_alt.mean().values



stations = ['zac_l','zac_u','zac_a']
lat = [+lat_l,+lat_u,+lat_a]
lon = [-lon_l,-lon_u,-lon_a]
alt = [+alt_l,+alt_u, +alt_a]

positions = pd.DataFrame({'station':stations, 'lat':lat, 'lon':lon, 'elev':alt})
positions.set_index('station', inplace= True)
#positions = pd.to_numeric(positions)
positions.to_csv('GlacioBasis_Zackenberg_station_positions.csv', index=True, float_format = '%.4f')
print(positions)
#+END_SRC

#+RESULTS:
:results:
# Out [30]: 
# output
               lat        lon         elev
station                                   
zac_l    74.624558 -21.375218   644.513575
zac_u    74.644020 -21.469426   877.588662
zac_a    74.647475 -21.651841  1476.043991

:end:

#+BEGIN_SRC jupyter-python

    
ds_filtered = ds.copy()
ds_filtered['p'] = ds['p'].copy()
ds_filtered['p'] = ds['p'].where(ds['p'] > (ds['p'].median() - 100))
ds_filtered['t_1'] = ds['t_1'].where(ds['t_1'] > -50)
ds_filtered['t_2'] = ds['t_2'].where(ds['t_2'] > -50)


fig, ax = plt.subplots(7,1,figsize=(20,20))
ds['dsr_corr'].plot(ax = ax[0])
ds['usr_corr'].plot(ax = ax[0])
ds['dlr'].plot(ax = ax[1])
ds['ulr'].plot(ax = ax[1])
ax[1].set_ylim([0,400])
ds_filtered['p'].plot(ax=ax[2])
ds_filtered['t_1'].plot(ax=ax[3])
ds_filtered['t_2'].plot(ax=ax[3])
ds['rh_corr'].plot(ax=ax[4])
#+END_SRC


**** Gradients
#+BEGIN_SRC jupyter-python 
import xarray as xr
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from glob import glob


zac_l= xr.open_dataset('data_v1.0/L1/zac_l/zac_l-2008-2021.nc')
zac_u = xr.open_dataset('data_v1.0/L1/zac_u/zac_u-2008-2020.nc')
zac_a = xr.open_dataset('data_v1.0/L1/zac_a/zac_a-2009-2020.nc')



fig, ax = plt.subplots(2,1,figsize=(10,10))
zac_l['t_1'].plot(ax = ax[0])
zac_u['t_1'].plot(ax = ax[0])
zac_a['t_1'].plot(ax = ax[0])
zac_l['p'].plot(ax = ax[1])
zac_u['p'].plot(ax = ax[1])
zac_a['p'].plot(ax = ax[1])


#+END_SRC

#+RESULTS:
:results:
# Out [21]: 
# text/plain
: [<matplotlib.lines.Line2D at 0x7f2a2f6224f0>]

# text/plain
: <Figure size 720x720 with 2 Axes>

# image/png
[[file:obipy-resources/a20e1397f28347830bf5429083571472c40cf69b/f5351ace8090705aff1c663bec9fa140aed502f0.png]]
:end:




* QC and database delivery

** Python libs

#+NAME: load_libs
#+BEGIN_SRC jupyter-python
import numpy as np
import pandas as pd
import xarray as xr
import matplotlib.pyplot as plt
import datetime
#+END_SRC

#+RESULTS: load_libs
:results:
# Out [68]: 
:end:


** Loading raw data and filtering data based on QC below

*** Meterological observations
Workflow

#+NAME: run_all_QC_filtering_steps_for_meterological_observations
#+BEGIN_SRC python :tangle QC_filtering_of_meterological_data.py

<<run_all_preQC_for_zac_a>>
<<run_all_postQC_for_zac_a>>

<<run_all_preQC_for_zac_l>>
<<run_all_postQC_for_zac_l>>

<<run_all_preQC_for_zac_u>>
<<run_all_postQC_for_zac_u>>
#+END_SRC

#+RESULTS: run_all_QC_filtering_steps_for_meterological_observations
:RESULTS:
/home/shl@geus.dk/anaconda3/envs/py38/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
# [goto error]
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[1], line 51
     45 transmitted_day = transmitted[0:-1][(timestep_day==1).values]
     50 fig, ax = plt.subplots(1,1,figsize = (10,5))
---> 51 with xr.open_dataset(zac_a_path) as ds:
     52     df = ds[['t_1']].to_dataframe()
     54 count10min = df.resample('H').count()

File ~/anaconda3/envs/py38/lib/python3.8/site-packages/xarray/backends/api.py:523, in open_dataset(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)
    520     kwargs.update(backend_kwargs)
    522 if engine is None:
--> 523     engine = plugins.guess_engine(filename_or_obj)
    525 backend = plugins.get_backend(engine)
    527 decoders = _resolve_decoders_kwargs(
    528     decode_cf,
    529     open_backend_dataset_parameters=backend.open_dataset_parameters,
   (...)
    535     decode_coords=decode_coords,
    536 )

File ~/anaconda3/envs/py38/lib/python3.8/site-packages/xarray/backends/plugins.py:177, in guess_engine(store_spec)
    169 else:
    170     error_msg = (
    171         "found the following matches with the input file in xarray's IO "
    172         f"backends: {compatible_engines}. But their dependencies may not be installed, see:\n"
    173         "https://docs.xarray.dev/en/stable/user-guide/io.html \n"
    174         "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html"
    175     )
--> 177 raise ValueError(error_msg)

ValueError: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'h5netcdf']. But their dependencies may not be installed, see:
https://docs.xarray.dev/en/stable/user-guide/io.html 
https://docs.xarray.dev/en/stable/getting-started-guide/installing.html
[[file:./.ob-jupyter/c1d90525e9d61c26b03e0db236bfea5f563769ff.png]]
:END:



**** zac_a

***** Pre QC
#+NAME: run_all_preQC_for_zac_a
#+BEGIN_SRC jupyter-python
<<zac_a_preQC_temperature>>
<<zac_a_preQC_radiation>>
<<zac_a_preQC_relative_humidity>>
<<zac_a_preQC_wind_speed>>
<<zac_a_preQC_pressure>>
<<zac_a_preQC_boom_height>>
<<zac_a_preQC_stake_height>>
#+END_SRC


 There are so many quality issues with the transmitted data - that I will omit it for now...
****** Transmitted data - what data do we have
 Plotting  all the transmitted data:
   #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<Load_transmitted_data>>

 transmitted_reduced = transmitted.drop(' timestamp', axis = 1)
 transmitted_reduced = transmitted_reduced.dropna(axis = 1, how = 'all')
 transmitted_reduced.astype(float).plot(subplots=True, figsize = (20,30), layout = (25,2))
   #+END_SRC

 Plotting the transmitted data that makes sense:

   #+BEGIN_SRC jupyter-python
 fig, ax = plt.subplots(6,1,figsize=(10,10))
 transmitted['temperature.1'].plot(ax=ax[0], label = 'temperature.1')
 transmitted['temperature2'].plot(ax=ax[0] , label = 'temperature2')
 transmitted['temperature'].plot(ax=ax[0], label = 'temperature')
 ax[0].legend()
 #ax[0].set_ylim(-40,20)
 transmitted['airpressure'].plot(ax=ax[1], label = 'airpressure')
 ax[1].legend()
 transmitted['relativehumidity'].plot(ax=ax[2])
 ax[2].legend()

 transmitted['shortwaveradiationin'].plot(ax=ax[3], label = 'shortwaveradiationin')
 transmitted['shortwaveradiationout'].plot(ax=ax[3], label = 'shortwaveradiationout')
 ax[3].legend()

 transmitted['windspeed_1'].plot(ax=ax[4], label = 'windspeed')
 ax[4].legend()

 transmitted['windspeed_2'].plot(ax=ax[5], label = 'windspeed_2 believed to be direction')
 ax[5].legend()

   #+END_SRC

   #+RESULTS:
   :results:
   # Out [31]: 


   # text/plain
   : <Figure size 720x720 with 6 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/c1d4097c9472cc09bd92af6ce1e0fda68d5b8938.png]]
   :end:


****** Transmitted data what is actually usable (when was the station buried)
  We look at the incoming shortwave radiation to define when the station is buried
  #+BEGIN_SRC jupyter-python
  #transmitted_trusted = transmitted[:'2019-August-30']
  <<load_libs>>
  <<data_file_paths>>
  <<Load_transmitted_data>>

  fig, ax = plt.subplots(1,1,figsize = (10,5))
  with xr.open_dataset(zac_a_path) as ds:
      dsr = ds['dsr'].where(ds['dsr'] != -9999.).to_dataframe()
       #print(t_1)

  new_index = pd.date_range(dsr.index[0],dsr.index[-1], freq = '10min')
  dsr = dsr.reindex(new_index)
  ax.plot(dsr.index.dayofyear, dsr)
  ax.plot(transmitted.index.dayofyear,transmitted['shortwaveradiationin'].astype(float))
  dayofinterest=242 # 30 August 2019
  ax.set_ylim(0,1100)
  ymin,ymax = ax.get_ylim()
  ax.vlines(dayofinterest,ymin,ymax, color = 'black', linestyle = '--' )


  #+END_SRC

  #+RESULTS:
  :results:
  # Out [3]: 


  # text/plain
  : <Figure size 720x360 with 1 Axes>

  # image/png
  [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/998d2f2f707a85173574e02e892be356c447cf86.png]]
  :end:

****** Temperature
#+NAME: zac_a_preQC_temperature
 #+BEGIN_SRC jupyter-python

 <<load_libs>>
 <<data_file_paths>>
 <<load_transmitted_trusted>>
  

 fig, ax = plt.subplots(1,1,figsize = (10,5))
 with xr.open_dataset(zac_a_path) as ds:
     df = ds[['t_1']].to_dataframe()

 count10min = df.resample('H').count()
 temp_hour = df.resample('H').mean()
 temp_hour[count10min<6] = np.nan
 count_hours = temp_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 temp_day = temp_hour.resample('D').mean()
 temp_day[count_hours<24 ] = np.nan

 temp_day.plot()
 temp_hour.plot(ax=ax)

 temp_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_temperature.csv', index = True, float_format = '%g')
 temp_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_temperature.csv', index = True, float_format = '%g')

  #+END_SRC

  #+RESULTS:
  :results:
  # Out [183]: 
  # text/plain
  : <Figure size 720x360 with 1 Axes>

  # image/png
  [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/ed93017186a51340562c5fa8fb287a7c894fd633.png]]

  # text/plain
  : <Figure size 432x288 with 1 Axes>

  # image/png
  [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/752b99ca255333fa6c58b84ef91d21f30588eef7.png]]

  # text/plain
  : <Figure size 432x288 with 1 Axes>

  # image/png
  [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/94573e3b437b1b46c8361a0ac1e4195571d048b4.png]]

  # text/plain
  : <Figure size 432x288 with 1 Axes>

  # image/png
  [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/8d6196d0ef52f6f9afa5370f67570ce4ddc88c9c.png]]
  :end:



******* Adding the transmitted data 


 So we trust that the data for now and we can concatenate the hourly and the daily temperature record

 Merging the data
    #+BEGIN_SRC jupyter-python
 temperature_hour_full = pd.concat((t_1_hour['t_1'],transmitted['temperature']), axis=1)
 temperature_hour_full['merged'] = temperature_hour_full[['t_1','temperature']].sum(axis=1, min_count=1)
 temp_hour = pd.DataFrame(temperature_hour_full['merged'].copy())
 temp_hour.rename(columns={'merged':'Air temperature, C'}, inplace = True)
 temp_hour.index.name = 'Timestamp'
 
 #If we had any daily transmissions
 #temp_day_1 = pd.DataFrame(temp_hour['Air temperature, C'].resample('D').mean())
 #temperature_day_full = pd.concat((temp_day_1['Air temperature, C'],transmitted_day['temperature']), axis=1)
 #temperature_day_full['merged'] = temperature_day_full[['Air temperature, C','temperature']].sum(axis=1, min_count=1)
 #temp_day = pd.DataFrame(temperature_day_full['merged'].copy())
 #temp_day.rename(columns={'merged':'Air temperature, C'}, inplace = True)
 #temp_day.index.name = 'Timestamp'
 temp_day = temp_day.resample('D').mean()
 temp_day.plot()
 
 temp_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_temperature.csv', index = True, float_format = '%g')
 temp_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_temperature.csv', index = True, float_format = '%g')
    #+END_SRC

    #+RESULTS:
    :results:
    # Out [34]: 
    # text/plain
    : <Figure size 432x288 with 1 Axes>

    # image/png
    [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/433a9db40cc02596fd4b6ef66dfc1ae6ff4cfedf.png]]
    :end:


****** Radiation 
 The transmitted radiation data does not look right - and since we do not have the tilt, I think I will declare this data too uncertain to use. 
 So we only use the downloaded data:
#+NAME: zac_a_preQC_radiation
  #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>
    
 fig, ax = plt.subplots(1,1,figsize = (10,5))
 with xr.open_dataset(zac_a_path) as ds:
 #    ds = ds[['dsr','usr','dlr','ulr']].to_dataframe()
 #    ds['dsr'] = ds['dsr'].where(ds['dsr'] != -9999.)
 #    ds['usr'] = ds['usr'].where(ds['usr'] != -9999.)
     ds = ds[['dsr','usr','dsr_corr','usr_corr','dlr','ulr','albedo','cloud_cov', 't_surf', 'I']].to_dataframe()
     ds['dsr_corr'] = ds['dsr_corr'].where(ds['dsr_corr'] != -9999.)
     ds['usr_corr'] = ds['usr_corr'].where(ds['usr_corr'] != -9999.)
     ds['dlr'] = ds['dlr'].where(ds['dlr'] != -9999.)
     ds['ulr'] = ds['ulr'].where(ds['ulr'] != -9999.)
    

 count10min = ds.resample('H').count()
 rad_hour = ds.resample('H').mean()
 rad_hour[count10min<6] = np.nan
 count_hours = rad_hour.resample('D').count()
 rad_day = rad_hour.resample('D').mean()
 rad_day[count_hours<24 ] = np.nan

 rad_day.plot(ax=ax)

 rad_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_radiation.csv', index = True, float_format = '%g')
 rad_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_radiation.csv', index = True, float_format = '%g')


 #+END_SRC

 #+RESULTS:
 [[file:./.ob-jupyter/9a496215b8df3344e37c6157acd2573a5cf43944.png]]

******* Looking into the possibilities of merging with transmitted data
 First load in libraries and data
   #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>
 <<load_transmitted_trusted>>
 # Converting transmitted radiation to physical units
 # From the nead header 
 dsr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
 usr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
 #dlr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
 #ulr_eng_coef       = 8.26   # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)

 dsr_trans = (transmitted['shortwaveradiationin']*10) / dsr_eng_coef #* 100
 usr_trans= (transmitted['shortwaveradiationout']*10) / usr_eng_coef #* 100
 #ds['dlr'] = ((ds['dlr']*1000) / dlr_eng_coef) + 5.67*10**(-8)*(ds['t_rad'] + T_0)**4 
 #    ds['ulr'] = ((ds['ulr']*1000) / ulr_eng_coef) + 5.67*10**(-8)*(ds['t_rad'] + T_0)**4


    
 fig, ax = plt.subplots(1,1,figsize = (10,5))
 with xr.open_dataset(zac_a_path) as ds:

     dsr = ds['dsr'].where(ds['dsr'] != -9999.).to_dataframe()
     usr = ds['usr'].where(ds['usr'] !=  -9999.).to_dataframe()
     dlr = ds['dlr'].where(ds['dlr'] != -9999.).to_dataframe()
     ulr = ds['ulr'].where(ds['ulr'] !=  -9999.).to_dataframe()
     #print(t_1)


    


 new_index = pd.date_range(dsr.index[0],t_1.index[-1], freq = '10min')

 #dsr = dsr.reindex(new_index)
 #usr = usr.reindex(new_index)
 #dlr = dlr.reindex(new_index)
 #ulr = ulr.reindex(new_index)



 #fig,ax = plt.subplots(1,1,figsize = (10,5))
 dsr.plot(ax= ax)
 usr.plot(ax=ax)

 #+END_SRC

 #+RESULTS:
 :results:
 # Out [48]: 
 # text/plain
 : <AxesSubplot:xlabel='time'>

 # text/plain
 : <Figure size 720x360 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/63eed35ae21f8ca75549ff6a8fac58cad7d70027.png]]
 :end:

 #+BEGIN_SRC jupyter-python
 dsr_hour = dsr.resample('H').mean()
 dsr_hour_full = pd.concat((dsr_hour['dsr'], dsr_trans), axis = 1)
 dsr_hour_full['dsr'].plot(ax=ax)
 dsr_hour_full['shortwaveradiationin'].plot(ax=ax)
 #print(dsr_hour_full)
 dsr_hour_full.plot()
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [49]: 
 # text/plain
 : <AxesSubplot:>

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/d6a163229cb70f4dd527f578424b340331c0636d.png]]
 :end:






****** Relative humidity
#+NAME: zac_a_preQC_relative_humidity
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_a_path) as ds:
     df = ds[['rh']].to_dataframe()

 count10min = df.resample('H').count()
 rh_hour = df.resample('H').mean()
 rh_hour[count10min<6] = np.nan
 count_hours = rh_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 rh_day = rh_hour.resample('D').mean()
 rh_day[count_hours<24 ] = np.nan

 rh_day.plot()
 rh_hour.plot()

 rh_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_relative_humidity.csv', index = True, float_format = '%g')
 rh_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_relative_humidity.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 [[file:./.ob-jupyter/a0ca46b66dfd8c0c0ce4f72a45154598dd8c8d19.png]]
 [[file:./.ob-jupyter/60eb7396c1ca1351c7aea3bbe9e64e8d50669b48.png]]
 [[file:./.ob-jupyter/9f1e3291f0cdeef3bca0b5a06cbd4b5935588e78.png]]
 [[file:./.ob-jupyter/17307f12369c8cf3a3aa32d22928c03d0b49df09.png]]
 :END:

****** Windspeed
#+NAME: zac_a_preQC_wind_speed
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_a_path) as ds:
     df = ds[['wspd']].to_dataframe()

 count10min = df.resample('H').count()
 wspd_hour = df.resample('H').mean()
 wspd_hour[count10min<6] = np.nan
 count_hours = wspd_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 wspd_day = wspd_hour.resample('D').mean()
 wspd_day[count_hours<24 ] = np.nan

 wspd_day.plot()
 wspd_hour.plot()

 wspd_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_wind_speed.csv', index = True, float_format = '%g')
 wspd_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_wind_speed.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [56]: 
 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/99f67ea6506aee5ea5a3d0be0b52f58a1f6bbe4c.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/25e685dc49b8d27b42be59ef213586b01caf67d2.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/ac31fe396fea775950e8e83551066c0ebf3b48b1.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/6e9f9bfaf86c2a956b000a488283425f3a6df749.png]]
 :end:

****** pressure
#+NAME: zac_a_preQC_pressure
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_a_path) as ds:
     df = ds[['p']].to_dataframe()

 count10min = df.resample('H').count()
 p_hour = df.resample('H').mean()
 p_hour[count10min<6] = np.nan
 count_hours = p_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 p_day = p_hour.resample('D').mean()
 p_day[count_hours<24 ] = np.nan

 p_day.plot()
 p_hour.plot()

 p_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_pressure.csv', index = True, float_format = '%g')
 p_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_pressure.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [84]: 
 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/d513de89470a2b5df497d438279e6ba9a6319806.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/d172f5826f47d143054a3436fe58db72c59f805c.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/b6c4138501f9e83dacd7dab238ee2db465c759e9.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/0ee61f8980a301a220416da13eb168b15d1fc723.png]]
 :end:


****** boom height
#+NAME: zac_a_preQC_boom_height
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_a_path) as ds:
     df = ds[['z_boom']].to_dataframe()

 count10min = df.resample('H').count()
 z_hour = df.resample('H').median()
 z_hour[count10min<6] = np.nan
 count_hours = z_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 z_day = z_hour.resample('D').median()
 z_day[count_hours<24 ] = np.nan

 z_day.plot()
 z_hour.plot()

 z_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_boom_height.csv', index = True, float_format = '%g')
 z_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_boom_height.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [89]: 
 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/dd13ab107a5a921a7d47d6018db718cd72abe0a1.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/b9a88ddb598c3c176718f4a9ec22ebf515228c52.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/fe937a1889dce97f6172a02ee3b19fb1318959ce.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/c1e6baaec1b6ce7c47ccc80f8311373f1234f595.png]]
 :end:


****** SR50 on stakes
#+NAME: zac_a_preQC_stake_height
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_a_path) as ds:
     df = ds[['z_stake']].to_dataframe()

 count10min = df.resample('H').count()
 z_hour = df.resample('H').median()
 z_hour[count10min<6] = np.nan
 count_hours = z_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 z_day = z_hour.resample('D').median()
 z_day[count_hours<24 ] = np.nan

 z_day.plot()
 z_hour.plot()

 z_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_hour_SR50_stake_height.csv', index = True, float_format = '%g')
 z_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_a_day_SR50_stake_height.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [1]: 
 # text/plain
 : <Figure size 640x480 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/1e60f322c4a1514f9bffafb7146f8b7e4c40c2b2.png]]

 # text/plain
 : <Figure size 640x480 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/709b80d899638c1ecb75d736a9b64c291fbf11e6.png]]

 # text/plain
 : <Figure size 640x480 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/6d7faa045f80cd1898c61af93eb25692783441e0.png]]

 # text/plain
 : <Figure size 640x480 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/f0232b5cc3f3cbe1b141baf7552c7d87760f946f.png]]
 :end:




***** Post QC
#+NAME: run_all_postQC_for_zac_a
#+BEGIN_SRC jupyter-python
<<zac_a_postQC_temperature>>
<<zac_a_postQC_radiation>>
<<zac_a_postQC_relative_humidity>>
<<zac_a_postQC_wind_speed>>
<<zac_a_postQC_pressure>>
<<zac_a_postQC_boom_height>>
<<zac_a_postQC_stake_height>>
#+END_SRC
****** Temperature
#+NAME: zac_a_postQC_temperature
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>
 datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

 temp_hour = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)


 # Bad data deleted
 temp_hour['2015-01-05':'2015-05-01'] = np.nan
 temp_hour[:'2009-08-08 21:00'] = np.nan

 count_hours = temp_hour.resample('D').count()
 temp_day = temp_hour.resample('D').mean()
 temp_day[count_hours<24 ] = np.nan

 temp_hour.to_csv('data_v1.0/gem_database/2022/zac_a_hour_temperature.csv', index = True, float_format = '%g')
 temp_day.to_csv('data_v1.0/gem_database/2022/zac_a_day_temperature.csv', index = True, float_format = '%g')

 #+END_SRC

 #+RESULTS:
 :results:
 # Out [205]: 
 :end:

****** Radiation
#+NAME: zac_a_postQC_radiation
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>
 datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

 rad_hour = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

 # Deliting bad data: Out of bounds
 maximum = 1000
 variable = 'dsr_corr'
 rad_hour[variable][rad_hour[variable]>maximum] = np.nan
 rad_hour[variable][rad_hour[variable]<0] = np.nan

 variable = 'usr_corr'
 albedo = rad_hour['usr_corr']/rad_hour['dsr_corr']
 rad_hour[variable][albedo>1] = np.nan
 variable = 'albedo'
 rad_hour[variable][albedo>1] = np.nan
 
 variable = 'ulr'
 rad_hour[variable][rad_hour['ulr']<150] = np.nan
 variable = 'cloud_cov'
 rad_hour[variable][rad_hour['ulr']<150] = np.nan
 variable = 't_surf'
 rad_hour[variable][rad_hour['ulr']<150] = np.nan

 variable = 'dlr'
 rad_hour[variable][rad_hour['dlr']<120] = np.nan
 variable = 'cloud_cov'
 rad_hour[variable][rad_hour['dlr']<120] = np.nan 
 variable = 't_surf'
 rad_hour[variable][rad_hour['dlr']<120] = np.nan 

 # Deleting bad data manually
 rad_hour['2015-01-01':'2015-05-01'] = np.nan

 # Then calculate daily averages again
 count_hours = rad_hour.resample('D').count()
 rad_day = rad_hour.resample('D').mean()
 rad_day[count_hours<24 ] = np.nan

 rad_hour.to_csv('data_v1.0/gem_database/2022/zac_a_hour_radiation.csv', index = True, float_format = '%g')
 rad_day.to_csv('data_v1.0/gem_database/2022/zac_a_day_radiation.csv', index = True, float_format = '%g')

 #+END_SRC

 #+RESULTS:

****** Relative humidity
#+NAME: zac_a_postQC_relative_humidity
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
 rh_hour = pd.read_csv(datapath+'preQC/zac_a_hour_relative_humidity.csv', parse_dates = True, index_col=0)


 # Outliers
 #rh_hour = rh_hour.where(rh_hour['rh']<= 110., np.nan)
 rh_hour = rh_hour.where(rh_hour['rh']>= 0., np.nan)
 # Deleting bad data manually
 rh_hour['2014-12-01':'2015-05-01'] = np.nan
 rh_hour['2011':'2014'] = np.nan
 rh_hour['2016-04-01':] = np.nan

 count_hours = rh_hour.resample('D').count()
 rh_day = rh_hour.resample('D').mean()
 rh_day[count_hours<24 ] = np.nan

 rh_hour.to_csv('data_v1.0/gem_database/2022/zac_a_hour_relative_humidity.csv', index = True, float_format = '%g')
 rh_day.to_csv('data_v1.0/gem_database/2022/zac_a_day_relative_humidity.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS:

****** Wind speed
#+NAME: zac_a_postQC_wind_speed
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
 wspd_hour = pd.read_csv(datapath+'preQC/zac_a_hour_wind_speed.csv', parse_dates = True, index_col=0)


 # Bad data
 startday = datetime.datetime(2020,8,15)
 endday = datetime.datetime(2022,4,21)
 wspd_hour['2020-August-15':'2021-July-21'] = np.nan
 wspd_hour['2014-12-01':'2015-05-01'] = np.nan

 count_hours = wspd_hour.resample('D').count()
 wspd_day = wspd_hour.resample('D').mean()
 wspd_day[count_hours<24 ] = np.nan

 wspd_hour.to_csv('data_v1.0/gem_database/2022/zac_a_hour_wind_speed.csv', index = True, float_format = '%g')
 wspd_day.to_csv('data_v1.0/gem_database/2022/zac_a_day_wind_speed.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS:

****** Pressure
#+NAME: zac_a_postQC_pressure
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
 p_hour = pd.read_csv(datapath+'preQC/zac_a_hour_pressure.csv', parse_dates = True, index_col=0)

 #Outliers

 p_hour = p_hour.where(p_hour['p']> 800., np.nan)


 count_hours = p_hour.resample('D').count()
 p_day = p_hour.resample('D').mean()
 p_day[count_hours<24 ] = np.nan

 p_hour.to_csv('data_v1.0/gem_database/2022/zac_a_hour_pressure.csv', index = True, float_format = '%g')
 p_day.to_csv('data_v1.0/gem_database/2022/zac_a_day_pressure.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [94]: 
 :end:


****** Boom height
#+NAME: zac_a_postQC_boom_height
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
 boom_hour = pd.read_csv(datapath+'preQC/zac_a_hour_boom_height.csv', parse_dates = True, index_col=0)

 #Outliers

 boom_hour = boom_hour.where(boom_hour['z_boom']> 0.1, np.nan)
 boom_hour = boom_hour.where(boom_hour['z_boom']< 4, np.nan)

 # Bad data
 boom_hour['2012-April-16':'2013-August-28'] = np.nan
 boom_hour['2013-December-20':'2014-April-22'] = np.nan
 boom_hour['2015-January-2':'2016-April-22'] = np.nan
 boom_hour['2018-April-20':'2018-April-24'] = np.nan


 count_hours = boom_hour.resample('D').count()
 boom_day = boom_hour.resample('D').median()
 boom_day[count_hours<24 ] = np.nan

 boom_hour.to_csv('data_v1.0/gem_database/2022/zac_a_hour_boom_height.csv', index = True, float_format = '%g')
 boom_day.to_csv('data_v1.0/gem_database/2022/zac_a_day_boom_height.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [144]: 
 :end:

**** zac_l
***** Pre QC
#+NAME: run_all_preQC_for_zac_l
#+BEGIN_SRC jupyter-python


<<zac_l_preQC_temperature>>
<<zac_l_preQC_radiation>>
<<zac_l_preQC_relative_humidity>>
<<zac_l_preQC_wind_speed>>
<<zac_l_preQC_pressure>>
<<zac_l_preQC_boom_height>>
<<zac_l_preQC_stake_height>>
#+END_SRC

****** Temperature
#+NAME: zac_l_preQC_temperature
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_l_path) as ds:
     df = ds[['t_1']].to_dataframe()

 count10min = df.resample('H').count()
 temp_hour = df.resample('H').mean()
 temp_hour[count10min<6] = np.nan
 count_hours = temp_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 temp_day = temp_hour.resample('D').mean()
 temp_day[count_hours<24 ] = np.nan

 temp_day.plot()
 temp_hour.plot()

 temp_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_temperature.csv', index = True, float_format = '%g')
 temp_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_temperature.csv', index = True, float_format = '%g')

 #+END_SRC

 #+RESULTS:
 :results:
 # Out [180]: 
 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/a2af43761d6603a432521a5fa8930961dd6fa826.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/72abb9b3f5c7d6055ef0c15fd216e3f22182da95.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/3e388564fb64bfe5f80ab6922b98a4649a36b9e8.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/40786f3480905c749fb162c4ee974012f7adeab0.png]]
 :end:



****** Radiation
#+NAME: zac_l_preQC_radiation
   #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>
    
 fig, ax = plt.subplots(1,1,figsize = (10,5))
 with xr.open_dataset(zac_l_path) as ds:
     #print(ds)
     ds = ds[['dsr','usr','dsr_corr','usr_corr','dlr','ulr','albedo','cloud_cov','t_surf', 'I']].to_dataframe()

     ds['dsr_corr'] = ds['dsr_corr'].where(ds['dsr_corr'] != -9999.)
     ds['usr_corr'] = ds['usr_corr'].where(ds['usr_corr'] != -9999.)
     ds['dlr'] = ds['dlr'].where(ds['dlr'] != -9999.)
     ds['ulr'] = ds['ulr'].where(ds['ulr'] != -9999.)#.to_dataframe()
     ds['ulr'] = ds['ulr'].where(ds['ulr'] < 10000.)#.to_dataframe()
     ds['dlr'] = ds['dlr'].where(ds['dlr'] < 10000.)#.to_dataframe()

 count10min = ds.resample('H').count()
 rad_hour = ds.resample('H').mean()
 rad_hour[count10min<6] = np.nan
 count_hours = rad_hour.resample('D').count()
 count_hours.plot()
 rad_day = rad_hour.resample('D').mean()
 rad_day[count_hours<24 ] = np.nan

 rad_day.plot(ax=ax)

 rad_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_radiation.csv', index = True, float_format = '%g')
 rad_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_radiation.csv', index = True, float_format = '%g')


 #+END_SRC

 #+RESULTS: zac_l_preQC_radiation
 :RESULTS:
 # [goto error]
 ---------------------------------------------------------------------------
 ValueError                                Traceback (most recent call last)
 Cell In[5], line 22
      19 zac_a_path = datapath+filename
      21 fig, ax = plt.subplots(1,1,figsize = (10,5))
 ---> 22 with xr.open_dataset(zac_l_path) as ds:
      23     #print(ds)
      24     ds = ds[['dsr','usr','dsr_corr','usr_corr','dlr','ulr','albedo','cloud_cov','t_surf', 'I']].to_dataframe()
      26     ds['dsr_corr'] = ds['dsr_corr'].where(ds['dsr_corr'] != -9999.)

 File ~/anaconda3/envs/py38/lib/python3.8/site-packages/xarray/backends/api.py:523, in open_dataset(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)
     520     kwargs.update(backend_kwargs)
     522 if engine is None:
 --> 523     engine = plugins.guess_engine(filename_or_obj)
     525 backend = plugins.get_backend(engine)
     527 decoders = _resolve_decoders_kwargs(
     528     decode_cf,
     529     open_backend_dataset_parameters=backend.open_dataset_parameters,
    (...)
     535     decode_coords=decode_coords,
     536 )

 File ~/anaconda3/envs/py38/lib/python3.8/site-packages/xarray/backends/plugins.py:177, in guess_engine(store_spec)
     169 else:
     170     error_msg = (
     171         "found the following matches with the input file in xarray's IO "
     172         f"backends: {compatible_engines}. But their dependencies may not be installed, see:\n"
     173         "https://docs.xarray.dev/en/stable/user-guide/io.html \n"
     174         "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html"
     175     )
 --> 177 raise ValueError(error_msg)

 ValueError: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'h5netcdf']. But their dependencies may not be installed, see:
 https://docs.xarray.dev/en/stable/user-guide/io.html 
 https://docs.xarray.dev/en/stable/getting-started-guide/installing.html
 [[file:./.ob-jupyter/c1d90525e9d61c26b03e0db236bfea5f563769ff.png]]
 :END:

 #+RESULTS:
 :RESULTS:
 [[file:./.ob-jupyter/f7f0d34b3ba7808ce192cab19ab55df897b2fc3c.png]]
 [[file:./.ob-jupyter/7c67dc8ae81842090316a2c6561812ff7b325735.png]]
 :END:


****** Relative humidity
#+NAME: zac_l_preQC_relative_humidity
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_l_path) as ds:
     df = ds[['rh']].to_dataframe()

 count10min = df.resample('H').count()
 rh_hour = df.resample('H').mean()
 rh_hour[count10min<6] = np.nan
 count_hours = rh_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 rh_day = rh_hour.resample('D').mean()
 rh_day[count_hours<24 ] = np.nan

 rh_day.plot()
 rh_hour.plot()

 rh_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_relative_humidity.csv', index = True, float_format = '%g')
 rh_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_relative_humidity.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [2]: 
 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/49204a121b210b62cd1f7bd53c971d5d683ae966.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/9f0eabee8d045cc1ac0979a999023a869f5bcfab.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/7fd78f6c43cb99b9f86244f1e127bffdb6fc4499.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/dfb592e40191de5216dab0ce1b199b8d88725c86.png]]
 :end:


****** Windspeed
#+NAME: zac_l_preQC_wind_speed
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_l_path) as ds:
     df = ds[['wspd']].to_dataframe()

 count10min = df.resample('H').count()
 wspd_hour = df.resample('H').mean()
 wspd_hour[count10min<6] = np.nan
 count_hours = wspd_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 wspd_day = wspd_hour.resample('D').mean()
 wspd_day[count_hours<24 ] = np.nan

 wspd_day.plot()
 wspd_hour.plot()

 wspd_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_wind_speed.csv', index = True, float_format = '%g')
 wspd_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_wind_speed.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [57]: 
 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/1d52117124a1ba01c32c9ef2a79a2be5db4680e4.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/2aea217ee3f0c6fde9a968c08f1b63a9ad99beff.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/cbd85bcaeed30463a6ef53fb85df00f4271fd33a.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/7e67105463dbd73b8b4c46123a496a4e063ae38b.png]]
 :end:

****** pressure
#+NAME: zac_l_preQC_pressure
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_l_path) as ds:
     df = ds[['p']].to_dataframe()

 count10min = df.resample('H').count()
 p_hour = df.resample('H').mean()
 p_hour[count10min<6] = np.nan
 count_hours = p_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 p_day = p_hour.resample('D').mean()
 p_day[count_hours<24 ] = np.nan

 p_day.plot()
 p_hour.plot()

 p_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_pressure.csv', index = True, float_format = '%g')
 p_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_pressure.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [83]: 
 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/e2d596eaf7af0fb76ce9a5c11393be8d5833b46e.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/ce2ae00073d30f3fc9d365e7fdb4cae990ca2787.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/569f68c817905a6316b135816769c917e3312071.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/336a6b6d1ed5518d60e95c612a5697314dbad5ae.png]]
 :end:


****** OLD? Surface lowering (z_pt)

#+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_l_path) as ds:
     lowering = ds['z_pt_corr'].to_dataframe()


 #lowering = df['z_pt_corr'].copy()
 #for year in np.arange(2008,2021+1):
 #    lowering[str(year)] = df['z_pt_corr'][str(year)]-df['z_pt_corr'][str(year)+'-5-1':str(year)+'-5-15'].mean()

 #lowering[lowering<-5] = np.nan
 #lowering[lowering>0.5] = np.nan
 #lowering.plot()
 lowering.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_lowering.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [102]: 
 :end:
 


****** boom height
#+NAME: zac_l_preQC_boom_height
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_l_path) as ds:
     df = ds[['z_boom']].to_dataframe()

 count10min = df.resample('H').count()
 z_hour = df.resample('H').median()
 z_hour[count10min<6] = np.nan
 count_hours = z_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 z_day = z_hour.resample('D').median()
 z_day[count_hours<24 ] = np.nan

 z_day.plot()
 z_hour.plot()

 z_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_boom_height.csv', index = True, float_format = '%g')
 z_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_boom_height.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [87]: 
 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/e009940083985fb7c42c826b26a88c9bdc484a9c.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/8861d7850218464f9126c23d0713e58254032fec.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/786f16eff176a587bc3f7e87e979ae11bd49c49f.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/9844b4436943e28f87e63bc907ed8b25b60667a3.png]]
 :end:


****** SR50 on stakes
#+NAME: zac_l_preQC_stake_height
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_l_path) as ds:
     df = ds[['z_stake']].to_dataframe()

 count10min = df.resample('H').count()
 z_hour = df.resample('H').median()
 z_hour[count10min<6] = np.nan
 count_hours = z_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 z_day = z_hour.resample('D').median()
 z_day[count_hours<24 ] = np.nan

 z_day.plot()
 z_hour.plot()

 z_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_SR50_stake_height.csv', index = True, float_format = '%g')
 z_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_SR50_stake_height.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [2]: 
 # text/plain
 : <Figure size 640x480 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/6fc7a35ccb35ca925f247d1599554aa24adbfeaa.png]]

 # text/plain
 : <Figure size 640x480 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/a53258f286b8c99517c0dae8fbbf8dac6437bd9d.png]]

 # text/plain
 : <Figure size 640x480 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/037b3574dfa13a1709cd3f483b68996d51f2a141.png]]

 # text/plain
 : <Figure size 640x480 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/48b0d60f32c3261e7bf38a2cbbe1ef948a849fb3.png]]
 :end:



***** Post QC
#+NAME: run_all_postQC_for_zac_l
#+BEGIN_SRC jupyter-python
<<zac_l_postQC_temperature>>
<<zac_l_postQC_radiation>>
<<zac_l_postQC_relative_humidity>>
<<zac_l_postQC_wind_speed>>
<<zac_l_postQC_pressure>>
<<zac_l_postQC_boom_height>>
<<zac_l_postQC_stake_height>>
#+END_SRC
****** Temperature
#+NAME: zac_l_postQC_temperature
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>
 datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

 temp_hour = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
 #Deling bad data

 count_hours = temp_hour.resample('D').count()
 temp_day = temp_hour.resample('D').mean()
 temp_day[count_hours<24 ] = np.nan


 temp_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_temperature.csv', index = True, float_format = '%g')
 temp_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_temperature.csv', index = True, float_format = '%g')

 #+END_SRC

 #+RESULTS:
 :results:
 # Out [193]: 
 :end:


****** Radiation
#+NAME: zac_l_postQC_radiation
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>
 datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

 rad_hour = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)

 # Deliting bad data: Out of bounds
 maximum = 1000
 
 variable = 'dsr_corr'
 rad_hour[variable][rad_hour[variable]>maximum] = np.nan
 rad_hour[variable][rad_hour[variable]<0] = np.nan

 variable = 'usr_corr'
 albedo = rad_hour['usr_corr']/rad_hour['dsr_corr']
 rad_hour[variable][albedo>1] = np.nan
 variable = 'albedo'
 rad_hour[variable][albedo>1] = np.nan
 
 variable = 'ulr'
 rad_hour[variable][rad_hour['ulr']<150] = np.nan
 variable = 'cloud_cov'
 rad_hour[variable][rad_hour['ulr']<150] = np.nan
 variable = 't_surf'
 rad_hour[variable][rad_hour['ulr']<150] = np.nan

 variable = 'dlr'
 rad_hour[variable][rad_hour['dlr']<120] = np.nan
 variable = 'cloud_cov'
 rad_hour[variable][rad_hour['dlr']<120] = np.nan 
 variable = 't_surf'
 rad_hour[variable][rad_hour['dlr']<120] = np.nan 

 # Deleting bad data manually
 rad_hour['usr_corr']['2021-01-01':'2021-07-21'] = np.nan
 rad_hour['2020-07-01':'2021-07-22'] = np.nan
# rad_hour['dsr_corr']['22-Jan-2020':] = np.nan
# rad_hour['usr_corr']['22-Jan-2020':] = np.nan
# rad_hour['albedo']['22-Jan-2020':] = np.nan

 # Then calculate daily averages again
 count_hours = rad_hour.resample('D').count()
 rad_day = rad_hour.resample('D').mean()
 rad_day[count_hours<24 ] = np.nan

 rad_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_radiation.csv', index = True, float_format = '%g')
 rad_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_radiation.csv', index = True, float_format = '%g')

 #+END_SRC

 #+RESULTS: zac_l_postQC_radiation



****** Relative humidity
#+NAME: zac_l_postQC_relative_humidity
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
 rad_hour = pd.read_csv(datapath+'preQC/zac_l_hour_relative_humidity.csv', parse_dates = True, index_col=0)


 # Outliers
 rad_hour = rad_hour.where(rad_hour['rh']<= 100., np.nan)
 rad_hour = rad_hour.where(rad_hour['rh']>= 0., np.nan)


 count_hours = rad_hour.resample('D').count()
 rad_day = rad_hour.resample('D').mean()
 rad_day[count_hours<24 ] = np.nan

 rad_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_relative_humidity.csv', index = True, float_format = '%g')
 rad_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_relative_humidity.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [16]: 
 :end:


****** Wind speed
#+NAME: zac_l_postQC_wind_speed
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
 wspd_hour = pd.read_csv(datapath+'preQC/zac_l_hour_wind_speed.csv', parse_dates = True, index_col=0)



 count_hours = wspd_hour.resample('D').count()
 wspd_day = wspd_hour.resample('D').mean()
 wspd_day[count_hours<24 ] = np.nan

 wspd_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_wind_speed.csv', index = True, float_format = '%g')
 wspd_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_wind_speed.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [156]: 
 :end:


****** Pressure
#+NAME: zac_l_postQC_pressure
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
 p_hour = pd.read_csv(datapath+'preQC/zac_l_hour_pressure.csv', parse_dates = True, index_col=0)

 #Outliers

 p_hour = p_hour.where(p_hour['p']> 870., np.nan)


 p_hour['2016-February-26':'2016-March-1'] = np.nan
 p_hour['2017-January-5':'2017-February-22'] = np.nan
 p_hour['2018-February-21':'2018-February-28'] = np.nan


 count_hours = p_hour.resample('D').count()
 p_day = p_hour.resample('D').mean()
 p_day[count_hours<24 ] = np.nan

 p_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_pressure.csv', index = True, float_format = '%g')
 p_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_pressure.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [153]: 
 :e


****** Boom height
#+NAME: zac_l_postQC_boom_height
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
 boom_hour = pd.read_csv(datapath+'preQC/zac_l_hour_boom_height.csv', parse_dates = True, index_col=0)

 #Outliers

 boom_hour = boom_hour.where(boom_hour['z_boom']> 0.1, np.nan)
 boom_hour = boom_hour.where(boom_hour['z_boom']< 2.75, np.nan)

 # Bad data
 boom_hour['2011-January-25':'2013-May-3'] = np.nan
 boom_hour['2019':'2020'] = np.nan

 count_hours = boom_hour.resample('D').count()
 boom_day = boom_hour.resample('D').median()
 boom_day[count_hours<24 ] = np.nan

 boom_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_boom_height.csv', index = True, float_format = '%g')
 boom_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_boom_height.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS:

**** zac_u

***** Pre QC
#+NAME: run_all_preQC_for_zac_u
#+BEGIN_SRC jupyter-python
<<zac_u_preQC_temperature>>
<<zac_u_preQC_radiation>>
<<zac_u_preQC_relative_humidity>>
<<zac_u_preQC_wind_speed>>
<<zac_u_preQC_pressure>>
<<zac_u_preQC_boom_height>>
<<zac_u_preQC_stake_height>>
#+END_SRC
****** Temperature
#+NAME: zac_u_preQC_temperature
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_u_path) as ds:
     df = ds[['t_1']].to_dataframe()

 count10min = df.resample('H').count()
 temp_hour = df.resample('H').mean()
 temp_hour[count10min<6] = np.nan
 count_hours = temp_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 temp_day = temp_hour.resample('D').mean()
 temp_day[count_hours<24 ] = np.nan

 temp_day.plot()
 temp_hour.plot()

 temp_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_temperature.csv', index = True, float_format = '%g')
 temp_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_temperature.csv', index = True, float_format = '%g')

 #+END_SRC

 #+RESULTS:
 :results:
 # Out [181]: 
 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/15d0d4bc32351b1b5f34e022f1a0686e8bec6b64.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/6f0ba422f2c3a65ae4ff3332ba9589ebd86384f6.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/06b1e63607a8e431df97fdad78093e664aa1f192.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/d7762ac5cd9a6509a2bf79429e3d1987c00712ee.png]]
 :end:

****** Radiation
#+NAME: zac_u_preQC_radiation
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>
    
 #fig, ax = plt.subplots(1,1,figsize = (10,5))
 with xr.open_dataset(zac_u_path) as ds:
     #ds = ds[['dsr','usr','dlr','ulr']].to_dataframe()
     #ds['dsr'] = ds['dsr'].where(ds['dsr'] != -9999.)
     ds = ds[['dsr','usr','dsr_corr','usr_corr','dlr','ulr','albedo','cloud_cov','t_surf', 'I']].to_dataframe()

     ds['dsr_corr'] = ds['dsr_corr'].where(ds['dsr_corr'] != -9999.)
     ds['usr_corr'] = ds['usr_corr'].where(ds['usr_corr'] != -9999.)
     #ds['usr'] = ds['usr'].where(ds['usr'] != -9999.)
     ds['dlr'] = ds['dlr'].where(ds['dlr'] != -9999.)
     ds['ulr'] = ds['ulr'].where(ds['ulr'] != -9999.)#.to_dataframe()
     ds['ulr'] = ds['ulr'].where(ds['ulr'] < 10000.)#.to_dataframe()
     ds['dlr'] = ds['dlr'].where(ds['dlr'] < 10000.)#.to_dataframe()


 count10min = ds.resample('H').count()
 rad_hour = ds.resample('H').mean()
 rad_hour[count10min<6] = np.nan
 count_hours = rad_hour.resample('D').count()
 rad_day = rad_hour.resample('D').mean()
 rad_day[count_hours<24 ] = np.nan
 #count_hours['April-2014'].plot()
 #count10min['April-2014'].plot()
 #rad_hour['April-14-2014':'April-15-2014'].plot(ax=ax)

 rad_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_radiation.csv', index = True, float_format = '%g')
 rad_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_radiation.csv', index = True, float_format = '%g')

 #+END_SRC

 #+RESULTS:

****** Relative humidity
#+NAME: zac_u_preQC_relative_humidity
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_u_path) as ds:
     df = ds[['rh']].to_dataframe()

 count10min = df.resample('H').count()
 rh_hour = df.resample('H').mean()
 rh_hour[count10min<6] = np.nan
 count_hours = rh_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 rh_day = rh_hour.resample('D').mean()
 rh_day[count_hours<24 ] = np.nan

 rh_day.plot()
 rh_hour.plot()

 rh_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_relative_humidity.csv', index = True, float_format = '%g')
 rh_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_relative_humidity.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [1]: 
 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/7964a7cc51edff2816693689d04e991193425782.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/711835bb17fb7065aebc81f20b68de6f78048817.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/00fad85c3fab36ce3889b0088d32f28f445d65f6.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/5503b83a5bf405cd2d05baa336f186eba11df3ec.png]]
 :end:

****** Windspeed
#+NAME: zac_u_preQC_wind_speed
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_u_path) as ds:
     df = ds[['wspd']].to_dataframe()

 count10min = df.resample('H').count()
 wspd_hour = df.resample('H').mean()
 wspd_hour[count10min<6] = np.nan
 count_hours = wspd_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 wspd_day = wspd_hour.resample('D').mean()
 wspd_day[count_hours<24 ] = np.nan

 wspd_day.plot()
 wspd_hour.plot()

 wspd_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_wind_speed.csv', index = True, float_format = '%g')
 wspd_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_wind_speed.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [53]: 
 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/9d6dabe7ef19d6227c59914d59508d7744f9837d.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/dcf9b6733fd4f7334aebb57310b45d9de0f9ad13.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/554abf1af32cea2e7fdcc9b88dd8f027e2393721.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/51337ff3da6a485de6fff929c0c3a5b901ce824e.png]]
 :end:

****** pressure
#+NAME: zac_u_preQC_pressure
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_u_path) as ds:
     df = ds[['p']].to_dataframe()

 count10min = df.resample('H').count()
 p_hour = df.resample('H').mean()
 p_hour[count10min<6] = np.nan
 count_hours = p_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 p_day = p_hour.resample('D').mean()
 p_day[count_hours<24 ] = np.nan

 p_day.plot()
 p_hour.plot()

 p_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_pressure.csv', index = True, float_format = '%g')
 p_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_pressure.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [82]: 
 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/d04d48f06de022214a4bbab07c570f35e254d1e0.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/3766e8b007329965e5be46a859688501de571d78.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/9b24e13bf33c41a2d03f3e904c5a64b923e360d3.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/f5df4be4d5af534e536311d5f33a382ac3727f21.png]]
 :end:






****** boom height
#+NAME: zac_u_preQC_boom_height
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_u_path) as ds:
     df = ds[['z_boom']].to_dataframe()

 count10min = df.resample('H').count()
 z_hour = df.resample('H').median()
 z_hour[count10min<6] = np.nan
 count_hours = z_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 z_day = z_hour.resample('D').median()
 z_day[count_hours<24 ] = np.nan

 z_day.plot()
 z_hour.plot()

 z_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_boom_height.csv', index = True, float_format = '%g')
 z_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_boom_height.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [88]: 
 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/c18c7d6d725e3147399cf7cc0f39f40f9a2fb449.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/604151156190a65d669403857ef0d61c6a4504c5.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/e84b7325d2ec1714944e9e7d768e8fa8e9870d19.png]]

 # text/plain
 : <Figure size 432x288 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/864922d182e50a0c49fb1d4e0411cec78981a207.png]]
 :end:



****** SR50 on stakes
#+NAME: zac_u_preQC_stake_height
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_u_path) as ds:
     df = ds[['z_stake']].to_dataframe()

 count10min = df.resample('H').count()
 z_hour = df.resample('H').median()
 z_hour[count10min<6] = np.nan
 count_hours = z_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 z_day = z_hour.resample('D').median()
 z_day[count_hours<24 ] = np.nan

 z_day.plot()
 z_hour.plot()

 z_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_SR50_stake_height.csv', index = True, float_format = '%g')
 z_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_SR50_stake_height.csv', index = True, float_format = '%g')
    
 #+END_SRC 

 #+RESULTS:
 :results:
 # Out [3]: 
 # text/plain
 : <Figure size 640x480 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/aa6acbabd6701a6bdca43e9a228fa929312771d0.png]]

 # text/plain
 : <Figure size 640x480 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/97797eacba6a507894c449ff5610433e1a57a2b4.png]]

 # text/plain
 : <Figure size 640x480 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/df66eafe87c2e39a015d44154e71c01ce3fed7ec.png]]

 # text/plain
 : <Figure size 640x480 with 1 Axes>

 # image/png
 [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/1069e89db8322d637a0168fa902d2769a7221cc3.png]]
 :end:


***** Post QC

#+NAME: run_all_postQC_for_zac_u
#+BEGIN_SRC jupyter-python
<<zac_u_postQC_temperature>>
<<zac_u_postQC_radiation>>
<<zac_u_postQC_relative_humidity>>
<<zac_u_postQC_wind_speed>>
<<zac_u_postQC_pressure>>
<<zac_u_postQC_boom_height>>
<<zac_u_postQC_stake_height>>
#+END_SRC

****** Temperature
#+NAME: zac_u_postQC_temperature
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
 temp_hour = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)

 # Bad data deleted
 temp_hour['2020-09-15':'2021-08-01'] = np.nan
 temp_hour['2014-10-30':'2015-12-31'] = np.nan


 count_hours = temp_hour.resample('D').count()
 temp_day = temp_hour.resample('D').mean()
 temp_day[count_hours<24 ] = np.nan

 temp_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_temperature.csv', index = True, float_format = '%g')
 temp_day.to_csv('data_v1.0/gem_database/2022/zac_u_day_temperature.csv', index = True, float_format = '%g')

 #+END_SRC

 #+RESULTS:

****** Radiation
#+NAME: zac_u_postQC_radiation
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>
 datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

 rad_hour = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)

 rad_hour.plot()

 # Deliting bad data: Out of bounds
 maximum = 1000
 variable = 'dsr_corr'
 rad_hour[variable][rad_hour[variable]>maximum] = np.nan
 rad_hour[variable][rad_hour[variable]<0] = np.nan

 variable = 'usr_corr'
 albedo = rad_hour['usr_corr']/rad_hour['dsr_corr']
 rad_hour[variable][albedo>1] = np.nan
 variable = 'albedo'
 rad_hour[variable][albedo>1] = np.nan
 
 variable = 'ulr'
 rad_hour[variable][rad_hour['ulr']<150] = np.nan
 variable = 'cloud_cov'
 rad_hour[variable][rad_hour['ulr']<150] = np.nan
 variable = 't_surf'
 rad_hour[variable][rad_hour['ulr']<150] = np.nan

 variable = 'dlr'
 rad_hour[variable][rad_hour['dlr']<120] = np.nan
 variable = 'cloud_cov'
 rad_hour[variable][rad_hour['dlr']<120] = np.nan 
 variable = 't_surf'
 rad_hour[variable][rad_hour['dlr']<120] = np.nan 


 # Deleting bad data manually
 rad_hour['2020-08-15':'2021-08-01'] = np.nan # Station was tilted
 rad_hour['2015-01-01':'2015-12-31'] = np.nan

 rad_hour= rad_hour['2012-05-05':]

 rad_hour.plot()
 # Then calculate daily averages again
 count_hours = rad_hour.resample('D').count()

 rad_day = rad_hour.resample('D').mean()
 rad_day[count_hours<24 ] = np.nan
 rad_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_radiation.csv', index = True, float_format = '%g')
 rad_day.to_csv('data_v1.0/gem_database/2022/zac_u_day_radiation.csv', index = True, float_format = '%g')

 #+END_SRC

 #+RESULTS:
 :RESULTS:
 [[file:./.ob-jupyter/4b76bfe842885213a5021034660d59c5dc620af7.png]]
 [[file:./.ob-jupyter/9b6efc547224302057f75fd2693dceb0aba2e60e.png]]
 :END:

****** Relative humidity
#+NAME: zac_u_postQC_relative_humidity
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
 rh_hour = pd.read_csv(datapath+'preQC/zac_u_hour_relative_humidity.csv', parse_dates = True, index_col=0)


 # Outliers
 rh_hour = rh_hour.where(rh_hour['rh']<= 100., np.nan)
 rh_hour = rh_hour.where(rh_hour['rh']>= 0., np.nan)


 # Bad data
 rh_hour['2020-August-15':'2021-July-21'] = np.nan
 rh_hour['2014-10-30':'2015-12-31'] = np.nan


 count_hours = rh_hour.resample('D').count()
 rh_day = rh_hour.resample('D').mean()
 rh_day[count_hours<24 ] = np.nan

 rh_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_relative_humidity.csv', index = True, float_format = '%g')
 rh_day.to_csv('data_v1.0/gem_database/2022/zac_u_day_relative_humidity.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS:



****** Wind speed
#+NAME: zac_u_postQC_wind_speed
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
 wspd_hour = pd.read_csv(datapath+'preQC/zac_u_hour_wind_speed.csv', parse_dates = True, index_col=0)


 # Bad data
 startday = datetime.datetime(2020,8,15)
 endday = datetime.datetime(2022,4,21)
 wspd_hour['2020-August-15':'2022-July-21'] = np.nan
 #wspd_hour['2014-10-30':'2015-12-31'] = np.nan

 count_hours = wspd_hour.resample('D').count()
 wspd_day = wspd_hour.resample('D').mean()
 wspd_day[count_hours<24 ] = np.nan

 wspd_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_wind_speed.csv', index = True, float_format = '%g')
 wspd_day.to_csv('data_v1.0/gem_database/2022/zac_u_day_wind_speed.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS:

****** Pressure
#+NAME: zac_u_postQC_pressure
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
 p_hour = pd.read_csv(datapath+'preQC/zac_u_hour_pressure.csv', parse_dates = True, index_col=0)

 #Outliers

 p_hour = p_hour.where(p_hour['p']> 850., np.nan)

 # Bad data
 p_hour['2014-10-30':'2015-12-31'] = np.nan
 p_hour['2016-April-4':'2016-April-18'] = np.nan
 p_hour['2017-January-23':'2017-March-2'] = np.nan


 count_hours = p_hour.resample('D').count()
 p_day = p_hour.resample('D').mean()
 p_day[count_hours<24 ] = np.nan

 p_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_pressure.csv', index = True, float_format = '%g')
 p_day.to_csv('data_v1.0/gem_database/2022/zac_u_day_pressure.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS:







****** Boom height
#+NAME: zac_u_postQC_boom_height
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
 boom_hour = pd.read_csv(datapath+'preQC/zac_u_hour_boom_height.csv', parse_dates = True, index_col=0)

 #Outliers

 boom_hour = boom_hour.where(boom_hour['z_boom']> 0.1, np.nan)
 boom_hour = boom_hour.where(boom_hour['z_boom']< 2.75, np.nan)

 # Bad data
 boom_hour['2012-January-1':'2016-April-20'] = np.nan
 boom_hour['2019-June-20':'2021-July-26'] = np.nan
 #boom_hour['2014-10-30':'2015-12-31'] = np.nan



 count_hours = boom_hour.resample('D').count()
 boom_day = boom_hour.resample('D').median()
 boom_day[count_hours<24 ] = np.nan

 boom_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_boom_height.csv', index = True, float_format = '%g')
 boom_day.to_csv('data_v1.0/gem_database/2022/zac_u_day_boom_height.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS:


*** Glaciological observations


**** PTA

***** raw to check when the pta was changed/redrilled
****** Zac_l
#+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_l_path) as ds:
     df = ds[['z_pt_corr','z_boom', 'albedo']].to_dataframe()

 df['z_pt_corr'].plot()

 #+END_SRC

 #+RESULTS:
 :RESULTS:
 : <AxesSubplot: xlabel='time'>
 [[file:./.ob-jupyter/d020ca4c40f9f8c36dc17c1396dd88064ab01da2.png]]
 :END:

****** Zac_u
#+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_u_path) as ds:
     df = ds[['z_pt_corr','z_boom', 'albedo']].to_dataframe()

 df['z_pt_corr'].plot()

 #+END_SRC

 #+RESULTS:
 :RESULTS:
 : <AxesSubplot: xlabel='time'>
 [[file:./.ob-jupyter/fc8876ca90d6de9df0b9e520bd217a2f6e0d1138.png]]
 :END:


***** raw-(ish)
****** Zac_l
#+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_l_path) as ds:
     df = ds[['z_pt_corr','z_boom', 'albedo']].to_dataframe()


 lowering = df['z_pt_corr'].copy()
 for year in np.arange(2008,2021+1):
     lowering[str(year)] = df['z_pt_corr'][str(year)]-df['z_pt_corr'][str(year)+'-5-15':str(year)+'-5-30'].mean()
 #lowering['2010'] = np.nan
 #lowering['2019'] = np.nan
 lowering[lowering<-5] = np.nan
 lowering[lowering>0.5] = np.nan
 lowering.plot()
 lowering.to_csv('data_v1.0/gem_database/2022/raw/zac_l_pta.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS:
 [[file:./.ob-jupyter/e09a83a6cc57354366fde2ba6cd0c3e02d104ce7.png]]

****** Zac_u
#+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_u_path) as ds:
     df = ds[['z_pt_corr','z_boom', 'albedo']].to_dataframe()

 df['z_pt_corr'][:'2017-04-30'] = np.nan
 lowering = df['z_pt_corr'].copy()
 for year in np.arange(2017,2021+1):
     lowering[str(year)] = df['z_pt_corr'][str(year)]-df['z_pt_corr'][str(year)+'-5-1':str(year)+'-5-15'].mean()
 lowering[lowering<-5] = np.nan
 lowering[lowering>0.5] = np.nan
 lowering.plot()
 lowering.to_csv('data_v1.0/gem_database/2022/raw/zac_u_pta.csv', index = True, float_format = '%g')
 #+END_SRC

 #+RESULTS:
 [[file:./.ob-jupyter/26dd76089a29f5af22f1c94346eda6a348e36cea.png]]


 
****** Old/random?
 #+BEGIN_SRC jupyter-python



 z_pt = df['z_pt_corr']
 z_boom = df['z_boom']
 
 z_pt[:'2017-04-30'] = np.nan
 year = '2019'
 z_pt = z_pt[year+'-May-1':year+'-September-1']
 z_boom  = z_boom [year+'-May-1':year+'-September-1']
 albedo = albedo[year+'-May-1':year+'-September-1']
 albedo_day = albedo.resample('D').mean()
 ax = z_pt[year].plot()
 ax1 = ax.twinx()
 albedo_day[year].plot(ax=ax1, color = 'tab:orange')



#+END_SRC

#+RESULTS:
:RESULTS:
# [goto error]
: [0;31m---------------------------------------------------------------------------[0m
: [0;31mNameError[0m                                 Traceback (most recent call last)
: Cell [0;32mIn[5], line 8[0m
: [1;32m      6[0m z_pt [38;5;241m=[39m z_pt[year[38;5;241m+[39m[38;5;124m'[39m[38;5;124m-May-1[39m[38;5;124m'[39m:year[38;5;241m+[39m[38;5;124m'[39m[38;5;124m-September-1[39m[38;5;124m'[39m]
: [1;32m      7[0m z_boom  [38;5;241m=[39m z_boom [year[38;5;241m+[39m[38;5;124m'[39m[38;5;124m-May-1[39m[38;5;124m'[39m:year[38;5;241m+[39m[38;5;124m'[39m[38;5;124m-September-1[39m[38;5;124m'[39m]
: [0;32m----> 8[0m albedo [38;5;241m=[39m [43malbedo[49m[year[38;5;241m+[39m[38;5;124m'[39m[38;5;124m-May-1[39m[38;5;124m'[39m:year[38;5;241m+[39m[38;5;124m'[39m[38;5;124m-September-1[39m[38;5;124m'[39m]
: [1;32m      9[0m albedo_day [38;5;241m=[39m albedo[38;5;241m.[39mresample([38;5;124m'[39m[38;5;124mD[39m[38;5;124m'[39m)[38;5;241m.[39mmean()
: [1;32m     10[0m ax [38;5;241m=[39m z_pt[year][38;5;241m.[39mplot()
: 
: [0;31mNameError[0m: name 'albedo' is not defined
:END:



#+BEGIN_SRC jupyter-python
<<get_meltrates_from_z_pt>>
meltrates['rate'].plot()

#+END_SRC

#+RESULTS:
:results:
# Out [3]: 
# text/plain
: <AxesSubplot:xlabel='time'>

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/55813a3b8e1f170eaeb85912e452140c3313291d.png]]
:end:

#+BEGIN_SRC jupyter-python

meltrates = meltrates.where(meltrates.year != 2010)
meltrates = meltrates.where(meltrates.year != 2019)
#print(meltrates)
meltrates = meltrates.dropna()
meltrates_annual = pd.pivot(meltrates, index = 'doy', columns = 'year', values = 'rate')
meltrates
#print(meltrates_annual)
#+END_SRC


  #+NAME: get_meltrates_from_z_pt
  #+BEGIN_SRC jupyter-python
  z_pt = -df['z_pt_corr'].resample('d').mean()
  z_pt_diff = z_pt.diff()
  z_pt_diff[np.abs(z_pt_diff) > 0.1] = np.nan
  z_pt_diff[z_pt_diff < 0] = 0
  meltrates = pd.DataFrame(z_pt_diff)
  meltrates.columns = ['rate']
  meltrates['year'] = meltrates.index.year
  meltrates['doy'] = meltrates.index.dayofyear
  meltrates_daymean = meltrates.groupby('doy').rate.mean()
  meltrates_daystd = meltrates.groupby('doy').rate.std()
  meltrates_max = meltrates_daymean+meltrates_daystd
  meltrates_min = meltrates_daymean-meltrates_daystd
  meltrates_min[meltrates_min<0] = 0
  meltrates = meltrates.where(meltrates.doy>150)
  meltrates = meltrates.where(meltrates.doy<250)


  #+END_SRC


***** Removing bad data and prepare preQC data 
****** zac_l
  #+BEGIN_SRC jupyter-python
  <<load_libs>>
  datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
  z_l = pd.read_csv(datapath+'raw/zac_l_pta.csv', parse_dates = True, index_col=0)


  z_l.loc[:'2008-7-1'] = np.nan
  z_l.loc['2008-8-23':'2008-12-31'] = np.nan
  z_l.loc['2008'] = z_l.loc['2008']-z_l.loc['2008-7-2'].mean()

  z_l.loc['2009-1-1':'2009-7-4'] = np.nan
  z_l.loc['2009-8-16':'2009-12-31'] = np.nan
  z_l.loc['2009'] = z_l.loc['2009']-z_l.loc['2009-7-5'].mean()

  z_l.loc['2010-1-1':'2010-6-22'] = np.nan
  z_l.loc['2010-8-2':'2010-12-31'] = np.nan
  z_l.loc['2010'] = z_l.loc['2010']-z_l.loc['2010-6-23'].mean()

  z_l.loc['2011-1-1':'2011-6-19'] = np.nan
  z_l.loc['2011-8-24':'2011-12-31'] = np.nan
  z_l.loc['2011'] = z_l.loc['2011']-z_l.loc['2011-6-20'].mean()

  z_l.loc['2012-1-1':'2012-6-29'] = np.nan
  z_l.loc['2012-8-24':'2012-12-31'] = np.nan
  z_l.loc['2012'] = z_l.loc['2012']-z_l.loc['2012-6-30'].mean()

  z_l.loc['2013-1-1':'2013-6-3'] = np.nan
  z_l.loc['2013-8-29':'2013-12-31'] = np.nan
  z_l.loc['2013'] = z_l.loc['2013']-z_l.loc['2013-6-4'].mean()

  z_l.loc['2014-1-1':'2014-7-8'] = np.nan
  z_l.loc['2014-8-23':'2014-12-31'] = np.nan
  z_l.loc['2014'] = z_l.loc['2014']-z_l.loc['2014-7-8'].mean()

  z_l.loc['2015-1-1':'2015-7-3'] = np.nan
  z_l.loc['2015-8-21':'2015-12-31'] = np.nan
  z_l.loc['2015'] = z_l.loc['2015']-z_l.loc['2015-7-4'].mean()
  
  z_l.loc['2016-1-1':'2016-6-25'] = np.nan
  z_l.loc['2016-8-29':'2016-12-31'] = np.nan
  z_l.loc['2016'] = z_l.loc['2016']-z_l.loc['2016-6-26'].mean()

  z_l.loc['2017-1-1':'2017-6-27'] = np.nan
  z_l.loc['2017-8-31':'2017-12-31'] = np.nan
  z_l.loc['2017'] = z_l.loc['2017']-z_l.loc['2017-6-28'].mean()

  z_l.loc['2018-1-1':'2018-7-30'] = np.nan
  z_l.loc['2018-8-24':'2018-12-31'] = np.nan
  z_l.loc['2018'] = z_l.loc['2018']-z_l.loc['2018-7-31'].mean()

  z_l.loc['2019'] = np.nan

  z_l.loc['2020-1-1':'2020-6-19'] = np.nan
  z_l.loc['2020-8-28':'2020-12-31'] = np.nan
  z_l.loc['2020'] = z_l.loc['2020']-z_l.loc['2020-6-20'].mean()

  z_l.loc['2021-1-1':'2021-6-16'] = np.nan
  z_l.loc['2021-8-17':'2021-12-31'] = np.nan
  z_l.loc['2021'] = z_l.loc['2021']-z_l.loc['2021-6-17'].mean()




  month = z_l.index.month
  z_l[month<4] = np.nan
  z_l[month>9] = np.nan

  z_l_hour = z_l.resample('H').mean()
  z_l_day = z_l.resample('D').mean() 
  z_l_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_pta.csv', index = True, float_format = '%g')
  z_l_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_pta.csv', index = True, float_format = '%g')
  #+END_SRC

  #+RESULTS:


******* 2008
   #+BEGIN_SRC jupyter-python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'preQC/zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2008'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2008'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2008,7,2),ymin,ymax)

   lowering[:'2008-July-1'] = np.nan
   lowering['2008'] = lowering-lowering['2008-July-2'].mean()
   #+END_SRC

   #+RESULTS:
   :results:
   # Out [104]: 
   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/e83e63d284a0f4967bf4b8a95f2183cfbfe77467.png]]
   :end:


******* 2009
   #+BEGIN_SRC jupyter-python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2009'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2009'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2009,6,25),ymin,ymax)
   ax.vlines(datetime.datetime(2009,5,14), ymin,ymax)
   #z_l['2009'] = z_l['2009']+z_l['2009-June-26'].mean()
   z_l['2009-5-14':'2009-6-24']= np.nan
   #+END_SRC

   #+RESULTS:
   :results:
   # Out [12]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/b25e32aaa99c20fa3b7362736f94187525d551e7.png]]
   :end:


******* 2010
   #+BEGIN_SRC jupyter-python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2010'].plot(ax = ax)
   ax1 = ax.twinx()
   #albedo['2010'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2010,5,15),ymin,ymax)
   ax.vlines(datetime.datetime(2010,8,2),ymin,ymax)

   #z_l['2010-1-1':'2010-5-15'] = np.nan
   #z_l['2010'] = z_l['2010']-z_l['2010-5-16'].mean() 
   #z_l['2010-8-2':'2010-12-31'] = np.nan
   #+END_SRC

   #+RESULTS:
   :results:
   # Out [98]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/36e271e6526b10bd2e5c45b2cef6cc4d47ec006f.png]]
   :end:


******* 2011
   #+BEGIN_SRC jupyter-python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2011'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2011'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2011,5,7),ymin,ymax, color = 'red')


   z_l['2011-1-1':'2010-5-7'] = np.nan
   z_l['2011'] = z_l['2011']-z_l['2011-5-8'].mean() 

   #+END_SRC

   #+RESULTS:
   :results:
   # Out [114]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/c55b60b88003c43f616769be0e8d2b4fdc147b88.png]]
   :end:




******* 2012
   #+BEGIN_SRC jupyter-python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2012'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2012'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2012,4,18),ymin,ymax, color = 'red')


   #z_l['2012-1-1':'2012-4-18'] = np.nan
   #z_l['2012'] = z_l['2012']-z_l['2012-4-18'].mean() 

   #+END_SRC

   #+RESULTS:
   :results:
   # Out [124]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/5ed75104e9c20bdb4648e6be427a358b9cab2eca.png]]
   :end:

******* 2013

   #+BEGIN_SRC jupyter-python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2013'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2013'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2013,4,18),ymin,ymax, color = 'red')


   #z_l['2012-1-1':'2012-4-18'] = np.nan
   z_l['2013'] = z_l['2013']-z_l['2013-4-1':'2013-5-1'].mean() 

   #+END_SRC

   #+RESULTS:
   :results:
   # Out [131]: 
   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/0e90ee31623366285b06b2bf1bbcfef35ebca3b6.png]]
   :end:


******* 2014

   #+BEGIN_SRC jupyter-python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2014'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2014'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2014,4,18),ymin,ymax, color = 'red')


   #z_l['2012-1-1':'2012-4-18'] = np.nan
   z_l['2014'] = z_l['2014']-z_l['2014-5-1':'2014-6-1'].mean() 

   #+END_SRC

   #+RESULTS:
   :results:
   # Out [134]: 
   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/fea1b2eef56c24331f0e6d21f3df32679664ab6b.png]]
   :end:


******* 2015

   #+BEGIN_SRC jupyter-python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2015'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2015'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2015,6,2),ymin,ymax, color = 'red')


   #z_l['2015-1-1':'2015-6-2'] = np.nan
   z_l['2015'] = z_l['2015']-z_l['2015-6-3':'2015-6-15'].mean() 
   #z_l['2015-12-1':'2015-12-31'] = np.nan
   #+END_SRC

   #+RESULTS:
   :results:
   # Out [175]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/1a4082d7464247d2731202eaf80dd6b9b6e445c8.png]]
   :end:

******* 2016

   #+BEGIN_SRC jupyter-python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2016'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2016'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2016,5,1),ymin,ymax, color = 'red')


   z_l['2016-1-1':'2016-5-1'] = np.nan
   z_l['2016'] = z_l['2016']-z_l['2016-5-2':'2016-5-15'].mean() 

   #+END_SRC

   #+RESULTS:
   :results:
   # Out [146]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/292ab78ee12489e9c74c76bb840b02365fae52c5.png]]
   :end:


******* 2017

   #+BEGIN_SRC jupyter-python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2017'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2017'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2017,5,1),ymin,ymax, color = 'red')


   z_l['2017-1-1':'2016-5-1'] = np.nan
   z_l['2017'] = z_l['2017']-z_l['2017-5-2':'2017-5-15'].mean() 

   #+END_SRC

   #+RESULTS:
   :results:
   # Out [152]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/0261497536b15dfac40fa9634d771882212b968e.png]]
   :end:


******* 2018

   #+BEGIN_SRC jupyter-python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2018'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2018'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2018,5,1),ymin,ymax, color = 'red')


   z_l['2018-1-1':'2018-5-1'] = np.nan
   z_l['2018'] = z_l['2018']-z_l['2018-5-2':'2018-5-15'].mean() 

   #+END_SRC

   #+RESULTS:
   :results:
   # Out [159]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/7e2fbe2bd65d8ac3a4ee42033aa12c46f5c7cda1.png]]
   :end:


******* 2019

   #+BEGIN_SRC jupyter-python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2019'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2019'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2019,5,1),ymin,ymax, color = 'red')

   z_l['2019'] = np.nan
   #z_l['2018-1-1':'2018-5-1'] = np.nan
   #z_l['2018'] = z_l['2018']-z_l['2018-5-2':'2018-5-15'].mean() 

   #+END_SRC

   #+RESULTS:
   :results:
   # Out [162]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/1d8a64d218214436b6b758b2dde74118f37cabde.png]]
   :end:


******* 2020

   #+BEGIN_SRC jupyter-python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2020'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2020'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2019,5,1),ymin,ymax, color = 'red')

   #z_l['2020'] = np.nan
   #z_l['2018-1-1':'2018-5-1'] = np.nan
   z_l['2020'] = z_l['2020']-z_l['2020-5-2':'2020-5-15'].mean() 

   #+END_SRC

   #+RESULTS:
   :results:
   # Out [165]: 


   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/dde540a0359743eba4799efd5e0564b0071cfd33.png]]
   :end:


******* 2021

   #+BEGIN_SRC jupyter-python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_l = pd.read_csv(datapath+'zac_l_lowering.csv', parse_dates = True, index_col=0)
   rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
   albedo = (rad_l['usr'].resample('D').sum()/rad_l['usr'].resample('D').count())/(rad_l['dsr'].resample('D').sum()/rad_l['dsr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_l['2021'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2021'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2019,5,1),ymin,ymax, color = 'red')

   #z_l['2020'] = np.nan
   z_l['2021-9-1':'2021-12-31'] = np.nan
   z_l['2021'] = z_l['2021']-z_l['2021-5-2':'2021-5-15'].mean()


   #+END_SRC

   #+RESULTS:
   :results:
   # Out [168]: 
   # text/plain
   : <Figure size 360x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/e81e14238f8c7a1d69a8468ea480be04d909aaa1.png]]
   :end:

****** zac_u
#+BEGIN_SRC jupyter-python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
z_u = pd.read_csv(datapath+'raw/zac_u_pta.csv', parse_dates = True, index_col=0)



z_u.loc['2017'] = z_u.loc['2017']-z_u.loc['2017-July-17':'2017-July-17'].mean()

z_u.loc[:'2017-July-18'] = np.nan
z_u.loc['2018'] = np.nan
z_u.loc['2019'] = z_u.loc['2019']-z_u.loc['2019-June-28'].mean()
z_u.loc['2020'] = z_u.loc['2020']-z_u.loc['2020-June-15':'2020-June-22'].mean()
z_u.loc['2020-January-1':'2020-June-15'] = np.nan
z_u.loc['2020-August-12':'2020-December-31'] = np.nan

z_u.loc['2021'] = z_u.loc['2021']-z_u.loc['2021-April-15':'2021-April-22'].mean()
z_u.loc['2021-January-1':'2021-July-27'] = np.nan
z_u.loc['2021-September-1':'2021-December-31'] = np.nan

month = z_u.index.month
z_u[month<4] = np.nan
z_u[month>9] = np.nan

z_u_hour = z_u.resample('H').mean()
z_u_day = z_u.resample('D').mean() 
z_u_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_pta.csv', index = True, float_format = '%g')
z_u_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_pta.csv', index = True, float_format = '%g')

z_u_hour.plot()
#+END_SRC

#+RESULTS:
:RESULTS:
: <AxesSubplot: xlabel='time'>
[[file:./.ob-jupyter/8a328ad4d89692e07ed9db763c2d98bf42cdd0ba.png]]
:END:

******* 2017
   #+BEGIN_SRC jupyter-python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_u = pd.read_csv(datapath+'zac_u_hour_lowering.csv', parse_dates = True, index_col=0)
   rad_u = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)


   albedo = (rad_u['usr_corr'].resample('D').sum()/rad_u['usr_corr'].resample('D').count())/(rad_u['dsr_corr'].resample('D').sum()/rad_u['dsr_corr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_u['2017'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2017'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2017,7,18),ymin,ymax)
   ax.vlines(datetime.datetime(2017,8,31),ymin,ymax)

   #+END_SRC

   #+RESULTS:
   :RESULTS:
   : /tmp/ipykernel_9944/3634662671.py:14: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.
   :   z_u['2017'].plot(ax = ax)
   : <matplotlib.collections.LineCollection at 0x7f626065a550>
   [[file:./.ob-jupyter/8b12c71b89d6d0226bdde3144730105187b51b48.png]]
   :END:

******* 2020
   #+BEGIN_SRC jupyter-python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_u = pd.read_csv(datapath+'zac_u_hour_lowering.csv', parse_dates = True, index_col=0)
   rad_u = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)


   albedo = (rad_u['usr_corr'].resample('D').sum()/rad_u['usr_corr'].resample('D').count())/(rad_u['dsr_corr'].resample('D').sum()/rad_u['dsr_corr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_u['2020'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2020'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2020,6,22),ymin,ymax)
   ax.vlines(datetime.datetime(2020,8,12),ymin,ymax)

   #+END_SRC

   #+RESULTS:
   :RESULTS:
   : /tmp/ipykernel_9944/62226053.py:14: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.
   :   z_u['2020'].plot(ax = ax)
   : <matplotlib.collections.LineCollection at 0x7f625f672f70>
   [[file:./.ob-jupyter/c1e68d2286a0d89dc40d9827b4927e64d223f2bc.png]]
   :END:


******* 2021
   #+BEGIN_SRC jupyter-python
   <<load_libs>>
   datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
   z_u = pd.read_csv(datapath+'zac_u_hour_lowering.csv', parse_dates = True, index_col=0)
   rad_u = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)


   albedo = (rad_u['usr_corr'].resample('D').sum()/rad_u['usr_corr'].resample('D').count())/(rad_u['dsr_corr'].resample('D').sum()/rad_u['dsr_corr'].resample('D').count())

   fig,ax= plt.subplots(1,1,figsize= (5,5))
   z_u['2021'].plot(ax = ax)
   ax1 = ax.twinx()
   albedo['2021'].plot(ax=ax1, color = 'orange')
   ax1.set_ylim(0,1)

   ymin,ymax = ax.get_ylim()
   ax.vlines(datetime.datetime(2021,7,25),ymin,ymax)
   ax.vlines(datetime.datetime(2021,8,29),ymin,ymax)

   #+END_SRC

   #+RESULTS:
   :RESULTS:
   : /tmp/ipykernel_9944/257407625.py:14: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.
   :   z_u['2021'].plot(ax = ax)
   : <matplotlib.collections.LineCollection at 0x7f625db73070>
   [[file:./.ob-jupyter/c8cc83ed66fbd0b1ffab29fbd96b8885d342f8bd.png]]
   :END:













**** SR50 on stakes
***** preQC
****** zac_l
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_l_path) as ds:
     df = ds[['z_stake']].to_dataframe()

 count10min = df.resample('H').count()
 z_hour = df.resample('H').median()
 z_hour[count10min<6] = np.nan
 count_hours = z_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 z_day = z_hour.resample('D').median()
 z_day[count_hours<24 ] = np.nan

 z_day.plot()
 z_hour.plot()

 z_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_SR50_stake_height.csv', index = True, float_format = '%g')
 z_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_SR50_stake_height.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 [[file:./.ob-jupyter/6fc7a35ccb35ca925f247d1599554aa24adbfeaa.png]]
 [[file:./.ob-jupyter/a53258f286b8c99517c0dae8fbbf8dac6437bd9d.png]]
 [[file:./.ob-jupyter/037b3574dfa13a1709cd3f483b68996d51f2a141.png]]
 [[file:./.ob-jupyter/48b0d60f32c3261e7bf38a2cbbe1ef948a849fb3.png]]
 :END:

****** zac_u
 #+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

 with xr.open_dataset(zac_u_path) as ds:
     df = ds[['z_stake']].to_dataframe()

 count10min = df.resample('H').count()
 z_hour = df.resample('H').median()
 z_hour[count10min<6] = np.nan
 count_hours = z_hour.resample('D').count()
 count_hours.plot()
 count10min.plot()
 z_day = z_hour.resample('D').median()
 z_day[count_hours<24 ] = np.nan

 z_day.plot()
 z_hour.plot()

 z_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_SR50_stake_height.csv', index = True, float_format = '%g')
 z_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_SR50_stake_height.csv', index = True, float_format = '%g')
    
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 [[file:./.ob-jupyter/aa6acbabd6701a6bdca43e9a228fa929312771d0.png]]
 [[file:./.ob-jupyter/97797eacba6a507894c449ff5610433e1a57a2b4.png]]
 [[file:./.ob-jupyter/df66eafe87c2e39a015d44154e71c01ce3fed7ec.png]]
 [[file:./.ob-jupyter/1069e89db8322d637a0168fa902d2769a7221cc3.png]]
 :END:
 

***** Prepare preQC data for comparison with ice ablation, setting to zero at ice melt start)

****** zac_l
#+BEGIN_SRC jupyter-python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
z_l = pd.read_csv(datapath+'preQC/zac_l_hour_SR50_stake_height.csv', parse_dates = True, index_col=0)

#z_l.loc['2008'] = z_l.loc['2008']-z_l.loc['2008-July-2'].mean()
#z_l.loc['2009'] = z_l.loc['2009']-z_l.loc['2009-July-5'].mean()
#z_l.loc['2010'] = z_l.loc['2010']-z_l.loc['2010-6-23'].mean()
#z_l.loc['2011'] = z_l.loc['2011']-z_l.loc['2011-6-20'].mean()
#z_l.loc['2012'] = z_l.loc['2012']-z_l.loc['2012-6-30'].mean()
#z_l.loc['2013'] = z_l.loc['2013']-z_l.loc['2013-6-4'].mean()
#z_l.loc['2014'] = z_l.loc['2014']-z_l.loc['2014-7-9'].mean()
#z_l.loc['2015'] = z_l.loc['2015']-z_l.loc['2015-7-1'].mean() 
#z_l.loc['2016'] = z_l.loc['2016']-z_l.loc['2016-6-26'].mean()
#z_l.loc['2017'] = z_l.loc['2017']-z_l.loc['2017-6-28'].mean()
#z_l.loc['2018'] = z_l.loc['2018']-z_l.loc['2018-7-31'].mean() 
#z_l.loc['2019'] = z_l.loc['2019']-z_l.loc['2019-6-13'].mean()
#z_l.loc['2020'] = z_l.loc['2020']-z_l.loc['2020-6-20'].mean()
#z_l.loc['2021'] = z_l.loc['2021']-z_l.loc['2021-6-17'].mean()


z_l.loc[:'2008-7-1'] = np.nan
z_l.loc['2008-8-23':'2008-12-31'] = np.nan
z_l.loc['2008'] = z_l.loc['2008']-z_l.loc['2008-7-2'].mean()

z_l.loc['2009-1-1':'2009-7-4'] = np.nan
z_l.loc['2009-7-18':'2009-12-31'] = np.nan
z_l.loc['2009'] = z_l.loc['2009']-z_l.loc['2009-7-5'].mean()

z_l.loc['2010-1-1':'2010-6-22'] = np.nan
z_l.loc['2010-8-2':'2010-12-31'] = np.nan
z_l.loc['2010'] = z_l.loc['2010']-z_l.loc['2010-6-23'].mean()

z_l.loc['2011-1-1':'2011-6-19'] = np.nan
z_l.loc['2011-8-24':'2011-12-31'] = np.nan
z_l.loc['2011'] = z_l.loc['2011']-z_l.loc['2011-6-20'].mean()

z_l.loc['2012-1-1':'2012-6-29'] = np.nan
z_l.loc['2012-8-24':'2012-12-31'] = np.nan
z_l.loc['2012'] = z_l.loc['2012']-z_l.loc['2012-6-30'].mean()

z_l.loc['2013-1-1':'2013-6-3'] = np.nan
z_l.loc['2013-8-29':'2013-12-31'] = np.nan
z_l.loc['2013'] = z_l.loc['2013']-z_l.loc['2013-6-4'].mean()

z_l.loc['2014-1-1':'2014-7-8'] = np.nan
z_l.loc['2014-8-23':'2014-12-31'] = np.nan
z_l.loc['2014'] = z_l.loc['2014']-z_l.loc['2014-7-8'].mean()

z_l.loc['2015-1-1':'2015-7-3'] = np.nan
z_l.loc['2015-8-21':'2015-12-31'] = np.nan
z_l.loc['2015'] = z_l.loc['2015']-z_l.loc['2015-7-4'].mean()
  
z_l.loc['2016-1-1':'2016-6-25'] = np.nan
z_l.loc['2016-8-29':'2016-12-31'] = np.nan
z_l.loc['2016'] = z_l.loc['2016']-z_l.loc['2016-6-26'].mean()

z_l.loc['2017-1-1':'2017-6-27'] = np.nan
z_l.loc['2017-8-31':'2017-12-31'] = np.nan
z_l.loc['2017'] = z_l.loc['2017']-z_l.loc['2017-6-28'].mean()

z_l.loc['2018-1-1':'2018-7-30'] = np.nan
z_l.loc['2018-8-24':'2018-12-31'] = np.nan
z_l.loc['2018'] = z_l.loc['2018']-z_l.loc['2018-7-31'].mean()

z_l.loc['2019'] = np.nan

z_l.loc['2020-1-1':'2020-6-19'] = np.nan
z_l.loc['2020-8-28':'2020-12-31'] = np.nan
z_l.loc['2020'] = z_l.loc['2020']-z_l.loc['2020-6-20'].mean()

z_l.loc['2021-1-1':'2021-6-16'] = np.nan
z_l.loc['2021-8-17':'2021-12-31'] = np.nan
z_l.loc['2021'] = z_l.loc['2021']-z_l.loc['2021-6-17'].mean()



month = z_l.index.month
z_l[month<4] = np.nan
z_l[month>9] = np.nan

z_l_hour = z_l.resample('H').mean()
z_l_day = z_l.resample('D').mean() 
z_l_hour.loc['July-2009'].plot()
z_l_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_SR50_stake_height_ice.csv', index = True, float_format = '%g')
z_l_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_SR50_stake_height_ice.csv', index = True, float_format = '%g')
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/65c7e8c24419d9abd9c242d65bf01348d9d51bce.png]]

****** zac_u



#+BEGIN_SRC jupyter-python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
z_u = pd.read_csv(datapath+'preQC/zac_u_hour_SR50_stake_height.csv', parse_dates = True, index_col=0)

z_u.loc['2008'] = z_u.loc['2008']-z_u.loc['2008-July-2'].mean()
z_u.loc['2009'] = z_u.loc['2009']-z_u.loc['2009-July-5'].mean()
z_u.loc['2010'] = z_u.loc['2010']-z_u.loc['2010-July-5'].mean()
z_u.loc['2011'] = z_u.loc['2011']-z_u.loc['2011-June-20'].mean()
z_u.loc['2012'] = z_u.loc['2012']-z_u.loc['2012-July-11'].mean()

z_u.loc['2013'] = z_u.loc['2013']-z_u.loc['2013-June-4'].mean()
z_u.loc['2014'] = z_u.loc['2014']-z_u.loc['2014-July-20'].mean()
z_u.loc['2015'] = z_u.loc['2015']-z_u.loc['2015-July-4'].mean()
z_u.loc['2016'] = z_u.loc['2016']-z_u.loc['2016-July-1'].mean()
z_u.loc['2017'] = z_u.loc['2017']-z_u.loc['2017-July-18'].mean()

z_u.loc['2018'] = z_u.loc['2018']-z_u.loc['2018-July-31'].mean()
z_u.loc['2019'] = z_u.loc['2019']-z_u.loc['2019-June-27'].mean()
z_u.loc['2020'] = z_u.loc['2020']-z_u.loc['2020-June-22'].mean()
z_u.loc['2021'] = z_u.loc['2021']-z_u.loc['2021-June-17'].mean()
  


month = z_u.index.month
z_u[month<4] = np.nan
z_u[month>9] = np.nan

z_u_hour = z_u.resample('H').mean()
z_u_day = z_u.resample('D').mean() 
z_u_hour.plot()
z_u_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_SR50_stake_height_ice.csv', index = True, float_format = '%g')
z_u_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_SR50_stake_height_ice', index = True, float_format = '%g')
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/cecad3f35a38c9422bba7b6461cd385d8e514443.png]]


**** Compare PTA and SR50 and create an ice ablation dataseries
The QC process is iterative - I check PTA against SR50, but I also see if I am able to model the melt within realistic bounds. If not, there might be something wrong with the instrument - for example a wrong calibration coefficient. 


***** zac_l
#+BEGIN_SRC jupyter-python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
pta = pd.read_csv(datapath+'preQC/zac_l_hour_pta.csv', parse_dates = True, index_col=0)
sr50 = pd.read_csv(datapath+'preQC/zac_l_hour_SR50_stake_height_ice.csv', parse_dates = True, index_col=0)
#z_u = pd.read_csv(datapath+'zac_u_hour_lowering.csv', parse_dates = True, index_col=0)
pta = -pta




fig, ax = plt.subplots(6,1,figsize = (10,15))
#pta.loc['2008'].plot(ax = ax[0])
#sr50.loc['2008'].plot(ax = ax[0])
#pta.loc['2009'].plot(ax = ax[1])
#sr50.loc['2009'].plot(ax = ax[1])
#pta.loc['2010'].plot(ax = ax[2])
#sr50.loc['2010'].plot(ax = ax[2])
#pta.loc['2011'].plot(ax = ax[3])
#sr50.loc['2011'].plot(ax = ax[3])
#pta.loc['2012'].plot(ax = ax[4])
#sr50.loc['2012'].plot(ax = ax[4])
#pta.loc['2013'].plot(ax = ax[5])
#sr50.loc['2013'].plot(ax = ax[5])
pta.loc['2014'].plot(ax = ax[0])
sr50.loc['2014'].plot(ax = ax[0])
pta.loc['2015'].plot(ax = ax[1])
sr50.loc['2015'].plot(ax = ax[1])
pta.loc['2016'].plot(ax = ax[2])
sr50.loc['2016'].plot(ax = ax[2])
pta.loc['2017'].plot(ax = ax[3])
sr50.loc['2017'].plot(ax = ax[3])
pta.loc['2018'].plot(ax = ax[4])
sr50.loc['2018'].plot(ax = ax[4])
pta.loc['2019'].plot(ax = ax[5])
sr50.loc['2019'].plot(ax = ax[5])

#pta.loc['2020'].plot(ax = ax[0])
#sr50.loc['2020'].plot(ax = ax[0])
#pta.loc['2021'].plot(ax = ax[1])
#sr50.loc['2021'].plot(ax = ax[1])



ablation = pta.copy()
ablation.rename(columns = {'z_pt_corr':'ice_ablation'}, inplace = True)
#ablation.loc['2010'] = sr50.loc['2010']
#ablation.loc['2015'] = sr50.loc['2015']
#ablation.loc['2016'] = sr50.loc['2016']

ablation_hour = ablation.resample('H').mean()
ablation_day = ablation.resample('D').mean() 
ablation_hour.plot()
ablation_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_hour_ice_ablation.csv', index = True, float_format = '%g')
ablation_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_l_day_ice_ablation.csv', index = True, float_format = '%g')

#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/ae1bac728566fafd45235e8ea5a7913ed5f9a049.png]]
[[file:./.ob-jupyter/6ff466e5d9c24f1a77ddc4cdb996ca954ac39aef.png]]
:END:




***** zac_u
#+BEGIN_SRC jupyter-python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
pta = pd.read_csv(datapath+'preQC/zac_u_hour_pta.csv', parse_dates = True, index_col=0)
sr50 = pd.read_csv(datapath+'preQC/zac_u_hour_SR50_stake_height_ice.csv', parse_dates = True, index_col=0)
#z_u = pd.read_csv(datapath+'zac_u_hour_lowering.csv', parse_dates = True, index_col=0)
pta = -pta

fig, ax = plt.subplots(6,1,figsize = (10,15))
#pta.loc['2008'].plot(ax = ax[0])
#sr50.loc['2008'].plot(ax = ax[0])
#pta.loc['2009'].plot(ax = ax[1])
#sr50.loc['2009'].plot(ax = ax[1])
#pta.loc['2010'].plot(ax = ax[2])
#sr50.loc['2010'].plot(ax = ax[2])
#pta.loc['2011'].plot(ax = ax[3])
#sr50.loc['2011'].plot(ax = ax[3])
#pta.loc['2012'].plot(ax = ax[4])
#sr50.loc['2012'].plot(ax = ax[4])
#pta.loc['2013'].plot(ax = ax[5])
#sr50.loc['2013'].plot(ax = ax[5])

#pta.loc['2014'].plot(ax = ax[0])
#sr50.loc['2014'].plot(ax = ax[0])
#pta.loc['2015'].plot(ax = ax[1])
#sr50.loc['2015'].plot(ax = ax[1])
#pta.loc['2016'].plot(ax = ax[2])
#sr50.loc['2016'].plot(ax = ax[2])
#pta.loc['2017'].plot(ax = ax[3])
#sr50.loc['2017'].plot(ax = ax[3])
#pta.loc['2018'].plot(ax = ax[4])
#sr50.loc['2018'].plot(ax = ax[4])
#pta.loc['2019'].plot(ax = ax[5])
#sr50.loc['2019'].plot(ax = ax[5])
pta.loc['2020'].plot(ax = ax[0])
sr50.loc['2020'].plot(ax = ax[0])
pta.loc['2021'].plot(ax = ax[1])
sr50.loc['2021'].plot(ax = ax[1])
pta.loc['2022'].plot(ax = ax[2])
sr50.loc['2022'].plot(ax = ax[2])

ablation = pta.copy()
ablation.rename(columns = {'z_pt_corr':'ice_ablation'}, inplace = True)

ablation.loc['2011'] = sr50.loc['2011']
ablation.loc['2014'] = sr50.loc['2014']
ablation.loc['2016'] = sr50.loc['2016']

ablation_hour = ablation.resample('H').mean()
ablation_day = ablation.resample('D').mean() 
ablation_hour.plot()
ablation_hour.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_hour_ice_ablation.csv', index = True, float_format = '%g')
ablation_day.to_csv('data_v1.0/gem_database/2022/preQC/zac_u_day_ice_ablation.csv', index = True, float_format = '%g')

#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/ac6cc6bf9b95171b5b01d0e61bc76486a8ce042a.png]]
[[file:./.ob-jupyter/0c419218a07ee1a7bf9ca55c335e146a2fd4ea79.png]]
:END:

**** Final filtering of ice ablation
****** zac_l
This is just a plot
#+BEGIN_SRC jupyter-python
<<load_libs>>
import matplotlib.dates as mdates
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
pta = pd.read_csv(datapath+'preQC/zac_l_hour_pta.csv', parse_dates = True, index_col=0)
sr50 = pd.read_csv(datapath+'zac_l_hour_SR50_stake_height_ice.csv', parse_dates = True, index_col=0)
z_l = pd.read_csv(datapath+'preQC/zac_l_hour_ice_ablation.csv', parse_dates = True, index_col=0)
pta = -pta

pta = pta.resample('D').mean()
sr50 = sr50.resample('D').mean()

fig, ax = plt.subplots(2,3,figsize = (8,6), sharey = True)
pta.loc['20-June-2008':'1-Sep-2008'].plot(ax = ax[0,0])
sr50.loc['20-June-2008':'1-Sep-2008'].plot(ax = ax[0,0])
ax[0,0].set_ylim(0,2.7)

#ax[0,0].set_xticks([])

pta.loc['20-June-2009':'1-Sep-2009'].plot(ax = ax[0,1])
sr50.loc['20-June-2009':'1-Sep-2009'].plot(ax = ax[0,1])
ax[0,1].set_ylim(0,2.7)

pta.loc['20-June-2010':'1-Sep-2010'].plot(ax = ax[0,2])
sr50.loc['20-June-2010':'1-Sep-2010'].plot(ax = ax[0,2])
ax[0,2].set_ylim(0,2.7)

pta.loc['20-June-2012':'1-Sep-2012'].plot(ax = ax[1,0])
sr50.loc['20-June-2012':'1-Sep-2012'].plot(ax = ax[1,0])
ax[1,0].set_ylim(0,2.7)

pta.loc['20-June-2015':'1-Sep-2015'].plot(ax = ax[1,1])
sr50.loc['20-June-2015':'1-Sep-2015'].plot(ax = ax[1,1])
#ax[0].set_xlim(datetime.datetime(2015,6,1),datetime.datetime(2015,9,1))
ax[1,1].set_ylim(0,2.7)

pta.loc['20-June-2016':'1-Sep-2016'].plot(ax = ax[1,2])
sr50.loc['20-June-2016':'1-Sep-2016'].plot(ax = ax[1,2])
#ax[1].set_xlim(datetime.datetime(2016,6,1),datetime.datetime(2016,9,1))
ax[1,2].set_ylim(0,2.7)

ax[0,0].set_ylabel('m ice')
ax[1,0].set_ylabel('m ice')
fig.tight_layout()

#fig.savefig('QCfigs/PTA_vs_SR50.png', dpi = 300)
fig.savefig('../glaciobasis/essd/manuscript/figures/fig11.png', dpi = 300)
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/1cc7db42c0b12157b7f278775ad46eebf6d5c738.png]]

#+BEGIN_SRC jupyter-python

<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
pta = pd.read_csv(datapath+'preQC/zac_l_hour_pta.csv', parse_dates = True, index_col=0)
sr50 = pd.read_csv(datapath+'preQC/zac_l_hour_SR50_stake_height_ice.csv', parse_dates = True, index_col=0)
z_l = pd.read_csv(datapath+'preQC/zac_l_hour_ice_ablation.csv', parse_dates = True, index_col=0)
pta = -pta
year = 2009
fig, ax = plt.subplots(1,1,figsize = (10,5))
pta.loc[str(year)].plot(ax = ax)
sr50.loc[str(year)].plot(ax = ax)
z_l.loc[str(year)].plot(ax=ax)
#+END_SRC

#+RESULTS:
:RESULTS:
: <AxesSubplot: xlabel='time'>
[[file:./.ob-jupyter/0812aa62f77aa4c9c892415b2b6d7b4602c7c7e0.png]]
:END:

#+BEGIN_SRC jupyter-python

<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
z_l = pd.read_csv(datapath+'preQC/zac_l_hour_ice_ablation.csv', parse_dates = True, index_col=0)

month = z_l.index.month
z_l[month<4] = np.nan
z_l[month>9] = np.nan

z_l_hour = z_l.resample('H').mean()
z_l_day = z_l.resample('D').mean() 
z_l_hour.plot()
z_l_hour.to_csv('data_v1.0/gem_database/2022/zac_l_hour_ice_ablation.csv', index = True, float_format = '%g')
z_l_day.to_csv('data_v1.0/gem_database/2022/zac_l_day_ice_ablation.csv', index = True, float_format = '%g')
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/6ff466e5d9c24f1a77ddc4cdb996ca954ac39aef.png]]



****** zac_u



#+BEGIN_SRC jupyter-python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
z_u = pd.read_csv(datapath+'preQC/zac_u_hour_ice_ablation.csv', parse_dates = True, index_col=0)

year = 2022
fig, ax = plt.subplots(1,1,figsize = (10,5))
pta.loc[str(year)].plot(ax = ax)
sr50.loc[str(year)].plot(ax = ax)
z_u.loc[str(year)].plot(ax=ax)
#+END_SRC

#+RESULTS:
:RESULTS:
: <AxesSubplot: xlabel='time'>
[[file:./.ob-jupyter/cc8672cb3528a86399e03628bd3b06c0215db081.png]]
:END:

#+BEGIN_SRC jupyter-python

<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
z_u = pd.read_csv(datapath+'preQC/zac_u_hour_ice_ablation.csv', parse_dates = True, index_col=0)

z_u.loc['2011-Jan-1':'2011-June-19'] = np.nan
z_u.loc['2011-Aug-20':'2011-Dec-31'] = np.nan
z_u.loc['2014'] = np.nan
z_u.loc['2015'] = np.nan
z_u.loc['2016-Jan-1':'2016-June-29'] = np.nan
z_u.loc['2016-Aug-25':'2016-Dec-31'] = np.nan
z_u.loc['2017'] = np.nan
z_u.loc['2019-Jan-1':'2019-June-27'] = np.nan
z_u.loc['2016-Aug-26':'2016-Dec-31'] = np.nan
z_u.loc['2020'] = np.nan
z_u.loc['2021'] = np.nan



month = z_u.index.month
z_u[month<4] = np.nan
z_u[month>9] = np.nan

z_u_hour = z_u.resample('H').mean()
z_u_day = z_u.resample('D').mean() 
z_u_hour.plot()
z_u_hour.to_csv('data_v1.0/gem_database/2022/zac_u_hour_ice_ablation.csv', index = True, float_format = '%g')
z_u_day.to_csv('data_v1.0/gem_database/2022/zac_u_day_ice_ablation.csv', index = True, float_format = '%g')
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/325431f6e2b4dc83aa82a79ffca2094173b462d8.png]]


** QC


*** Overview

Plotting all preQC values against all postQC values

#+BEGIN_SRC jupyter-python
import matplotlib.pyplot as plt
import numpy as np

# Sample data
time = np.linspace(0, 10, 100)
temperature = np.sin(time) * 20
solar_radiation = np.abs(np.sin(time)) * 800
# ... continue for your other variables ...

# List of data and corresponding labels
data_list = [temperature, solar_radiation]  # Continue for other variables...
labels = ["Temperature", "Solar Radiation"]  # Continue for other variable names...

fig, axes = plt.subplots(len(data_list), 1, sharex=True, figsize=(10, 8))

for ax, data, label in zip(axes, data_list, labels):
    ax.plot(time, data, label=label)
    ax.set_ylabel(label)
    ax.legend()

axes[-1].set_xlabel("Time")

plt.tight_layout()
plt.show()


#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/1c8af2ebf73506ae6a94ecfb4848a1c32d387de3.png]]

#+BEGIN_SRC jupyter-python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

workingfolder = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

#zac_l
site = 'zac_l'
temp = pd.read_csv(workingfolder + site + '_hour_temperature.csv', index_col = 0)
rh = pd.read_csv(workingfolder + site + '_hour_relative_humidity.csv', index_col = 0)
p = pd.read_csv(workingfolder + site + '_hour_pressure.csv', index_col = 0)
rad = pd.read_csv(workingfolder + site + '_hour_radiation.csv', index_col = 0)
ws = pd.read_csv(workingfolder + site + '_hour_wind_speed.csv', index_col = 0)
height = pd.read_csv(workingfolder + site + '_hour_boom_height.csv', index_col = 0)
#surf = pd.read_csv(workingfolder + site + '_hour_lowering.csv', index_col = 0)
ablation = pd.read_csv(workingfolder + site + '_hour_ice_ablation.csv', index_col = 0)

merged = temp.join(rh, how='outer').join(p, how='outer').join(rad, how='outer').join(ws, how='outer').join(height, how='outer').join(ablation, how='outer')

merged.rename(columns = {'t_1':'T_air', 'rh': 'RH', 'p': 'P_air','dsr_corr':'SR_in', 'usr_corr':'SR_out', 'dlr':'LR_in', 'ulr':'LR_out', 'wspd':'WS', 'z_boom':'H', 'ice_ablation':'Ablation_ice' }, inplace = True)
#merged.plot(subplots = True, figsize = (12,12))

cols = ['T_air', 'RH', 'P_air', 'SR_in', 'SR_out', 'LR_in','LR_out', 'WS', 'H','Ablation_ice']
<<drop_head_and_tail_nans_fast>>
data = drop_nan_rows(merged)
dates = data.index
#fig, axes = plt.subplots(len(data[cols]), 1, sharex=True, figsize=(10, 8))

#for ax, data, label in zip(axes, data[cols], cols):
#    ax.plot(dates, data, label=label)
#    ax.set_ylabel(label)
#    ax.legend()

fig = data[cols].plot(subplots = True, figsize = (10,8))


# Set x-ticks to the beginning of each year
#years = [datetime.date(year, 1, 1) for year in range(2008, 2023)]
#axes[-1].set_xticks(years)
#axes[-1].set_xlabel("Year")
#fig.autofmt_xdate()  # Better formatting for dates

plt.tight_layout()


#+END_SRC

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example
  [0;31m---------------------------------------------------------------------------[0m
  [0;31mAttributeError[0m                            Traceback (most recent call last)
  Cell [0;32mIn[2], line 64[0m
  [1;32m     57[0m fig [38;5;241m=[39m data[cols][38;5;241m.[39mplot(subplots [38;5;241m=[39m [38;5;28;01mTrue[39;00m, figsize [38;5;241m=[39m ([38;5;241m10[39m,[38;5;241m8[39m))
  [1;32m     60[0m [38;5;66;03m# Set x-ticks to the beginning of each year[39;00m
  [1;32m     61[0m [38;5;66;03m#years = [datetime.date(year, 1, 1) for year in range(2008, 2023)][39;00m
  [1;32m     62[0m [38;5;66;03m#axes[-1].set_xticks(years)[39;00m
  [1;32m     63[0m [38;5;66;03m#axes[-1].set_xlabel("Year")[39;00m
  [0;32m---> 64[0m [43mfig[49m[38;5;241;43m.[39;49m[43mautofmt_xdate[49m()  [38;5;66;03m# Better formatting for dates[39;00m
  [1;32m     66[0m plt[38;5;241m.[39mtight_layout()

  [0;31mAttributeError[0m: 'numpy.ndarray' object has no attribute 'autofmt_xdate'
#+end_example
[[file:./.ob-jupyter/abc2a94d1a2546004a078bed1ea90033a95f066f.png]]
:END:

#+BEGIN_SRC jupyter-python




labels = ["T_air", "RH", "P_air", "SR_in", "SR_out", "LR_in", "LR_out", "WS", "H", "Ablation_ice"]
offsets = range(0, 8*10, 10)  # These values determine the distance between each plot on the y-axis

# Plotting
for var_data, label, offset in zip(data, labels, offsets):
    plt.plot(time, var_data + offset, label=label)

plt.yticks(offsets, labels)  # Re-label y-axis ticks with variable names
plt.xlabel("Time")
plt.ylabel("Variables")
plt.title("Multiple Climate Variables Over Time")
plt.legend(loc="upper left", bbox_to_anchor=(1, 1))  # Legend outside of the main plot area
plt.tight_layout()
plt.show()

#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/49c3eb82a1aa85831107e17eafb7b74b32bf7ac1.png]]




*** temperature
**** Utilities
#+NAME: plot_gradients_full_period
#+BEGIN_SRC jupyter-python
fig,ax = plt.subplots(5,1,figsize = (7.5,7), sharex=True)
panel1 = ax[0]
panel2 = ax[1]
panel3 = ax[2]
panel4 = ax[3]
panel5 = ax[4]


zac_l_pre[variable].plot(ax = panel1, label = 'zac_l', color = 'tab:blue')
zac_u_pre[variable].plot(ax = panel1, label = 'zac_u', color = 'tab:orange')
zac_a_pre[variable].plot(ax = panel1, label = 'zac_a', color = 'tab:green')
panel1.legend(ncol = 1)
panel1.set_title('Pre QC')
panel1.set_ylabel(ylabel)
#panel1.text(datetime.datetime(2008,7,1),15,'A')

zac_l[variable].plot(ax = panel2, label = '', color = 'tab:blue')
zac_u[variable].plot(ax = panel2, label = '', color = 'tab:orange')
zac_a[variable].plot(ax = panel2, label = '', color = 'tab:green')

#panel2.legend(ncol = 3)
panel2.set_title('Post QC')
panel2.set_ylabel(ylabel)

d_l_u = (zac_l-zac_u)/(zac_l_elev-zac_u_elev)*100
d_u_a = (zac_u-zac_a)/(zac_u_elev-zac_a_elev)*100
d_l_a = (zac_l-zac_a)/(zac_l_elev-zac_a_elev)*100

d_l_u_pre = (zac_l_pre-zac_u_pre)/(zac_l_elev-zac_u_elev)*100
d_u_a_pre = (zac_u_pre-zac_a_pre)/(zac_u_elev-zac_a_elev)*100
d_l_a_pre = (zac_l_pre-zac_a_pre)/(zac_l_elev-zac_a_elev)*100

d_l_u_pre[variable].plot(ax = panel3, label = 'Flagged in QC', color = 'tab:red', linewidth = 0.8)
d_u_a_pre[variable].plot(ax=panel4, label = 'Flagged in QC', color = 'tab:red', linewidth = 0.8)
d_l_a_pre[variable].plot(ax=panel5, label = 'Flagged in QC', color = 'tab:red', linewidth = 0.8)

d_l_u[variable].plot(ax=panel3, label = 'zac_l minus zac_u', color = 'gray')
d_u_a[variable].plot(ax=panel4, label = 'zac_u minus zac_a', color = 'gray')
d_l_a[variable].plot(ax=panel5, label = 'zac_l minus zac_a', color = 'gray')
panel3.set_title('Gradients')
panel3.legend(ncol = 2)
panel3.set_ylabel(ylabel + '/100m')
panel4.legend(ncol = 2)
panel4.set_ylabel(ylabel + '/100m')
panel5.legend(ncol = 2)
panel5.set_ylabel(ylabel + '/100m')
# Label each subplot
for i, ax in enumerate(ax, start=1):
    ax.text(0.01, 0.95, chr(96 + i), transform=ax.transAxes, 
            fontsize=16, fontweight='bold', va='top')

fig.tight_layout()

#+END_SRC

#+RESULTS: plot_gradients_full_period
[[file:./.ob-jupyter/9773108045260fe2f5ff01766896c651ee152732.png]]

#+NAME: plot_the_timeperiod
#+BEGIN_SRC jupyter-python
# The year with the problem
fig,ax = plt.subplots(4,2,figsize = (12,10))
temp_l[variable].plot(ax = ax[0,1], label = 'zac_l')
temp_u[variable].plot(ax = ax[0,1], label = 'zac_u')
temp_a[variable].plot(ax = ax[0,1], label = 'zac_a')
ax[0,1].legend()
ax[0,1].set_title(timeperiod1_title)
ax[0,1].set_xlim(timeperiod1)

d_l_u[variable].plot(ax=ax[1,1], label = 'zac_l minus zac_u')
d_u_a[variable].plot(ax=ax[2,1], label = 'zac_u minus zac_a')
d_l_a[variable].plot(ax=ax[3,1], label = 'zac_l minus zac_a')
ymin,ymax = ax[0,1].get_ylim()
ax[0,1].vlines(date_of_interest1,ymin,ymax, color = 'gray', linestyle = '--')
ax[0,1].vlines(date_of_interest2,ymin,ymax, color = 'gray', linestyle = '--')
ymin,ymax = ax[1,1].get_ylim()
ax[1,1].vlines(date_of_interest1,ymin,ymax, color = 'gray', linestyle = '--')
ax[1,1].vlines(date_of_interest2,ymin,ymax, color = 'gray', linestyle = '--')
ymin,ymax = ax[2,1].get_ylim()
ax[2,1].vlines(date_of_interest1,ymin,ymax, color = 'gray', linestyle = '--')
ax[2,1].vlines(date_of_interest2,ymin,ymax, color = 'gray', linestyle = '--')

ymin,ymax = ax[3,1].get_ylim()
ax[3,1].vlines(date_of_interest1,ymin,ymax, color = 'gray', linestyle = '--')
ax[3,1].vlines(date_of_interest2,ymin,ymax, color = 'gray', linestyle = '--')
ax[1,1].legend()
ax[2,1].legend()
ax[3,1].legend()
ax[1,1].set_xlim(timeperiod1)
ax[2,1].set_xlim(timeperiod1)
ax[3,1].set_xlim(timeperiod1)
ax[0,1].set_ylim(-40,20)
#ax[1,1].set_ylim(-0.05,0.05)
#ax[2,1].set_ylim(-0.05,0.05)
#ax[3,1].set_ylim(-0.05,0.05)


# The previous year for reference
temp_l[variable].plot(ax = ax[0,0], label = 'zac_l')
temp_u[variable].plot(ax = ax[0,0], label = 'zac_u')
temp_a[variable].plot(ax = ax[0,0], label = 'zac_a')
ax[0,0].legend()
ax[0,0].set_title(timeperiod2_title)

ax[0,0].set_xlim(timeperiod2)



d_l_u[variable].plot(ax=ax[1,0], label = 'zac_l minus zac_u')
d_u_a[variable].plot(ax=ax[2,0], label = 'zac_u minus zac_a')
d_l_a[variable].plot(ax=ax[3,0], label = 'zac_l minus zac_a')
ax[1,0].legend()
ax[2,0].legend()
ax[3,0].legend()
ax[1,0].set_xlim(timeperiod2)
ax[2,0].set_xlim(timeperiod2)
ax[3,0].set_xlim(timeperiod2)

ax[0,0].set_ylim(-40,20)
#ax[1,0].set_ylim(-0.05,0.05)
#ax[2,0].set_ylim(-0.05,0.05)
#ax[3,0].set_ylim(-0.05,0.05)




#+END_SRC



**** post QC check
***** data only
#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data
#rcParams.update({'font.size': 8})
zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_temperature.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)

variable = 't_1'
ylabel = '$^\circ$C'


zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan


fig,ax = plt.subplots(3,1,figsize = (8,6), sharex=True)
zac_l[variable].plot(ax = ax[0], label = 'ZAC_L', color = 'tab:blue')
zac_u[variable].plot(ax = ax[1], label = 'ZAC_U', color = 'tab:orange')
zac_a[variable].plot(ax = ax[2], label = 'ZAC_A', color = 'tab:green')

zac_l_pre[variable].plot(ax = ax[0], label = 'Discarded', color = 'tab:blue', alpha = 0.3)
zac_u_pre[variable].plot(ax = ax[1], label = 'Discarded', color = 'tab:orange', alpha = 0.3)
zac_a_pre[variable].plot(ax = ax[2], label = 'Discarded', color = 'tab:green', alpha = 0.3)

ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel(ylabel)
ax[1].set_ylabel(ylabel)
ax[2].set_ylabel(ylabel)
#ax[0].set_ylim(0,100)
#ax[1].set_ylim(0,100)
#ax[2].set_ylim(0,100)

ax[0].legend()
ax[1].legend()
ax[2].legend()


#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/d9005a5b31fda77e08df1cee03d63752d437cc34.png]]

***** Drift 
#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_temperature.csv', parse_dates = True, index_col=0)

variable = 't_1'
ylabel = '%'



fig,ax = plt.subplots(3,1,figsize = (10,10), sharex=True)
zac_l['t_1'].plot(ax = ax[0], label = 'Air temperature at ZAC_L', color = 'tab:blue')
zac_u['t_1'].plot(ax = ax[1], label = 'Air temperature at ZAC_U', color = 'tab:orange')
zac_a['t_1'].plot(ax = ax[2], label = 'Air temperature at ZAC_A', color = 'tab:pink')
ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel('$^\circ$C')
ax[1].set_ylabel('$^\circ$C')
ax[2].set_ylabel('$^\circ$C')
ax[0].legend()
ax[1].legend()
ax[2].legend()
fig.savefig('QCfigs/temperature_drift.png')

#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/3dcb625d876342232d260d51a9ec61a452de3acc.png]]




***** Gradients

#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_temperature.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)

variable = 't_1'
ylabel = '$^\circ$C'
<<plot_gradients_full_period>>

disc_data_l = (zac_l_pre[variable].count()-zac_l[variable].count())/zac_l_pre[variable].count()*100
disc_data_u = (zac_u_pre[variable].count()-zac_u[variable].count())/zac_u_pre[variable].count()*100
disc_data_a = (zac_a_pre[variable].count()-zac_a[variable].count())/zac_a_pre[variable].count()*100
print('The total percentage of discarded data at: ')
print('zac_l is '+ str(disc_data_l))
print('zac_u is '+ str(disc_data_u))
print('zac_a is '+ str(disc_data_a))
#fig.savefig('QCfigs/Temperature_final.png')
fig.savefig('../glaciobasis/essd/manuscript/figures/fig04.png', dpi = 300)
#+END_SRC

#+RESULTS:
:RESULTS:
: The total percentage of discarded data at: 
: zac_l is 0.0
: zac_u is 8.88088248376479
: zac_a is 3.625990251766395
[[file:./.ob-jupyter/9773108045260fe2f5ff01766896c651ee152732.png]]
:END:


**** QC
Gradients

#+BEGIN_SRC jupyter-python
<<import_libraries>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
variable = 't_1'
temp_l = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)

<<plot_gradients_full_period>>

temp_l = pd.read_csv(datapath+'preQC/zac_l_day_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_day_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_day_temperature.csv', parse_dates = True, index_col=0)

<<plot_gradients_full_period>>

#+END_SRC

#+RESULTS:
:results:
# Out [186]: 


# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/1888e82c18c583a9ca4794893cf44b28feac7802.png]]

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/76b53e9854be00a5f6d1087ab34f7546b6d328e3.png]]
:end:




There are issues with zac_u in winter 2014/2015 and likely with zac_l in winter 2020/2021








**** The issue at zac_u in 2020/2021

I will remove both data from zac_u between 2010-10-01 and 2021-07-01

#+BEGIN_SRC jupyter-python
<<import_libraries>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

temp_l = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)


timeperiod1_title = '2020-01-01 to 2021-12-31'
timeperiod1 = (datetime.datetime(2020,1,1), datetime.datetime(2021,12,31))
timeperiod2_title = '2019-01-01 to 2020-12-31'
timeperiod2 = (datetime.datetime(2019,1,1), datetime.datetime(2020,12,31))
date_of_interest1=datetime.datetime(2020,9,15)
date_of_interest2=datetime.datetime(2021,8,1)

<<plot_the_timeperiod>>

temp_l = pd.read_csv(datapath+'preQC/zac_l_day_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_day_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_day_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)


<<plot_the_timeperiod>>


#+END_SRC

#+RESULTS:
:results:
# Out [217]: 
# text/plain
: (-40.0, 20.0)

# text/plain
: <Figure size 864x720 with 8 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/f0efb8e1eac0183e6579cc4f965279a8a203ffe8.png]]

# text/plain
: <Figure size 864x720 with 8 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/cf4d5635c652e282d85448d4756162a2c0c874f3.png]]
:end:




**** The issue at zac_a in 2015

The data from zac_a between 2015-01-05 to 2015-05-01 looks strange and will be removed

#+BEGIN_SRC jupyter-python
<<import_libraries>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

# meta data
variable = 't_1'
zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

temp_l = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)

timeperiod1_title = '2014-11-1 to 2015-10-15'
timeperiod1 = (datetime.datetime(2014,11,1),datetime.datetime(2015,10,15))
timeperiod2_title = '2013-11-1 to 2014-10-15'
timeperiod2 = (datetime.datetime(2013,11,1),datetime.datetime(2014,10,15))
date_of_interest1=datetime.datetime(2015,1,5)
date_of_interest2=datetime.datetime(2015,5,1)

<<plot_the_timeperiod>>

temp_l = pd.read_csv(datapath+'preQC/zac_l_day_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_day_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_day_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)


<<plot_the_timeperiod>>

#+END_SRC

#+RESULTS:
:results:
# Out [197]: 
# text/plain
: (-40.0, 20.0)

# text/plain
: <Figure size 864x720 with 8 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/f815913459035854589ede4a846461cc22813b42.png]]

# text/plain
: <Figure size 864x720 with 8 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/6258f58b61585fe481916e0c2d6a901680037bdc.png]]
:end:


#+NAME: plot_the_2015_issue_at_zac_l
#+BEGIN_SRC jupyter-python
# The year with the problem
fig,ax = plt.subplots(4,2,figsize = (12,10))
temp_l['Air temperature, C'].plot(ax = ax[0,1], label = 'zac_l')
temp_u['Air temperature, C'].plot(ax = ax[0,1], label = 'zac_u')
temp_a['Air temperature, C'].plot(ax = ax[0,1], label = 'zac_a')
ax[0,1].legend()
ax[0,1].set_title('2015-1-1 to 2015-5-15')
ax[0,1].set_xlim(datetime.datetime(2015,1,1),datetime.datetime(2015,5,15))

d_l_u['Air temperature, C'].plot(ax=ax[1,1], label = 'zac_l minus zac_u')
d_u_a['Air temperature, C'].plot(ax=ax[2,1], label = 'zac_u minus zac_a')
d_l_a['Air temperature, C'].plot(ax=ax[3,1], label = 'zac_l minus zac_a')
ymin,ymax = ax[3,1].get_ylim()
ax[3,1].vlines(datetime.datetime(2015,1,12),ymin,ymax)
ax[1,1].legend()
ax[2,1].legend()
ax[3,1].legend()
ax[1,1].set_xlim(datetime.datetime(2015,1,1),datetime.datetime(2015,5,15))
ax[2,1].set_xlim(datetime.datetime(2015,1,1),datetime.datetime(2015,5,15))
ax[3,1].set_xlim(datetime.datetime(2015,1,1),datetime.datetime(2015,5,15))
ax[0,1].set_ylim(-40,20)
#ax[1,1].set_ylim(-0.05,0.05)
#ax[2,1].set_ylim(-0.05,0.05)
#ax[3,1].set_ylim(-0.05,0.05)


# The previous year for reference
temp_l['Air temperature, C'].plot(ax = ax[0,0], label = 'zac_l')
temp_u['Air temperature, C'].plot(ax = ax[0,0], label = 'zac_u')
temp_a['Air temperature, C'].plot(ax = ax[0,0], label = 'zac_a')
ax[0,0].legend()
ax[0,0].set_title('2013-1-1 to 2013-5-12')
timeperiod = (datetime.datetime(2013,1,1),datetime.datetime(2013,5,12))
ax[0,0].set_xlim(timeperiod)



d_l_u['Air temperature, C'].plot(ax=ax[1,0], label = 'zac_l minus zac_u')
d_u_a['Air temperature, C'].plot(ax=ax[2,0], label = 'zac_u minus zac_a')
d_l_a['Air temperature, C'].plot(ax=ax[3,0], label = 'zac_l minus zac_a')
ax[1,0].legend()
ax[2,0].legend()
ax[3,0].legend()
ax[1,0].set_xlim(timeperiod)
ax[2,0].set_xlim(timeperiod)
ax[3,0].set_xlim(timeperiod)

ax[0,0].set_ylim(-40,20)
#ax[1,0].set_ylim(-0.05,0.05)
#ax[2,0].set_ylim(-0.05,0.05)
#ax[3,0].set_ylim(-0.05,0.05)




#+END_SRC
**** The issue at zac_a beginning of record
We remove data from before 2009-08-08 21:00
#+BEGIN_SRC jupyter-python
<<import_libraries>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

# meta data
variable = 't_1'
zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

temp_l = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)

timeperiod1_title = '2009-8-1 to 2009-9-1'
timeperiod1 = (datetime.datetime(2009,8,5),datetime.datetime(2009,8,6))
timeperiod2_title = '2010-8-1 to 2010-9-1'
timeperiod2 = (datetime.datetime(2010,8,5),datetime.datetime(2010,8,6))
date_of_interest1=datetime.datetime(2015,1,5)
date_of_interest2=datetime.datetime(2015,5,1)

<<plot_the_timeperiod>>

#+END_SRC

#+RESULTS:
:results:
# Out [204]: 
# text/plain
: (-40.0, 20.0)

# text/plain
: <Figure size 864x720 with 8 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/52fcca926023e03aeafeff972491057f21667832.png]]
:end:

**** The issue at zac_u in 2015

The conclusion from the below investigation is that we will discard the zac_u data from 2015, the fan must have stopped running
The excat period that can be discarded at zac_u: 2014-10-30 to 2015-12-31
Hourly
#+BEGIN_SRC jupyter-python
<<import_libraries>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
# meta data
variable = 't_1'
zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

temp_l = pd.read_csv(datapath+'preQC/zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_hour_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_hour_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)

<<plot_the_2015_issue_at_zac_u>>

temp_l = pd.read_csv(datapath+'preQC/zac_l_day_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_day_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_day_temperature.csv', parse_dates = True, index_col=0)
d_l_u = (temp_l-temp_u)/(zac_l_elev-zac_u_elev)
d_u_a = (temp_u-temp_a)/(zac_u_elev-zac_a_elev)
d_l_a = (temp_l-temp_a)/(zac_l_elev-zac_a_elev)

<<plot_the_2015_issue_at_zac_u>>


#+END_SRC

#+RESULTS:
:results:
# Out [210]: 
# text/plain
: (-40.0, 20.0)

# text/plain
: <Figure size 864x720 with 8 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/bdf68acbc5ed059722c0bc40d9f930c298347c5d.png]]

# text/plain
: <Figure size 864x720 with 8 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/7a308c6d85ebcf64b1e75fd9fd3a9b2b0280ab30.png]]
:end:

#+NAME: plot_the_2015_issue_at_zac_u
#+BEGIN_SRC jupyter-python
# The year with the problem
fig,ax = plt.subplots(4,2,figsize = (12,10))
temp_l[variable].plot(ax = ax[0,1], label = 'zac_l')
temp_u[variable].plot(ax = ax[0,1], label = 'zac_u')
temp_a[variable].plot(ax = ax[0,1], label = 'zac_a')
ax[0,1].legend()
ax[0,1].set_title('2014-10-30 to 2015-12-31')
ax[0,1].set_xlim(datetime.datetime(2014,10,30),datetime.datetime(2015,12,31))



d_l_u[variable].plot(ax=ax[1,1], label = 'zac_l minus zac_u')
d_u_a[variable].plot(ax=ax[2,1], label = 'zac_u minus zac_a')
d_l_a[variable].plot(ax=ax[3,1], label = 'zac_l minus zac_a')
ax[1,1].legend()
ax[2,1].legend()
ax[3,1].legend()
ax[1,1].set_xlim(datetime.datetime(2014,10,30),datetime.datetime(2015,12,31))
ax[2,1].set_xlim(datetime.datetime(2014,10,30),datetime.datetime(2015,12,31))
ax[3,1].set_xlim(datetime.datetime(2014,10,30),datetime.datetime(2015,12,31))
ax[0,1].set_ylim(-40,20)
#ax[1,1].set_ylim(-0.05,0.05)
#ax[2,1].set_ylim(-0.05,0.05)
#ax[3,1].set_ylim(-0.05,0.05)


# The previous year for reference
temp_l[variable].plot(ax = ax[0,0], label = 'zac_l')
temp_u[variable].plot(ax = ax[0,0], label = 'zac_u')
temp_a[variable].plot(ax = ax[0,0], label = 'zac_a')
ax[0,0].legend()
ax[0,0].set_title('2013-10-30 to 2014-12-31')
ax[0,0].set_xlim(datetime.datetime(2013,10,30),datetime.datetime(2014,12,31))



d_l_u[variable].plot(ax=ax[1,0], label = 'zac_l minus zac_u')
d_u_a[variable].plot(ax=ax[2,0], label = 'zac_u minus zac_a')
d_l_a[variable].plot(ax=ax[3,0], label = 'zac_l minus zac_a')
ax[1,0].legend()
ax[2,0].legend()
ax[3,0].legend()
ax[1,0].set_xlim(datetime.datetime(2013,10,30),datetime.datetime(2014,12,31))
ax[2,0].set_xlim(datetime.datetime(2013,10,30),datetime.datetime(2014,12,31))
ax[3,0].set_xlim(datetime.datetime(2013,10,30),datetime.datetime(2014,12,31))

ax[0,0].set_ylim(-40,20)
#ax[1,0].set_ylim(-0.05,0.05)
#ax[2,0].set_ylim(-0.05,0.05)
#ax[3,0].set_ylim(-0.05,0.05)




#+END_SRC

#+RESULTS: plot_the_2015_issue_at_zac_u
:results:
# Out [208]: 
# output
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2888             try:
-> 2889                 return self._engine.get_loc(casted_key)
   2890             except KeyError as err:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 'Air temperature, C'

The above exception was the direct cause of the following exception:

KeyError                                  Traceback (most recent call last)
/tmp/ipykernel_8873/1461341412.py in <module>
     26 
     27 # The previous year for reference
---> 28 temp_l['Air temperature, C'].plot(ax = ax[0,0], label = 'zac_l')
     29 temp_u['Air temperature, C'].plot(ax = ax[0,0], label = 'zac_u')
     30 temp_a['Air temperature, C'].plot(ax = ax[0,0], label = 'zac_a')

~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/core/frame.py in __getitem__(self, key)
   2897             if self.columns.nlevels > 1:
   2898                 return self._getitem_multilevel(key)
-> 2899             indexer = self.columns.get_loc(key)
   2900             if is_integer(indexer):
   2901                 indexer = [indexer]

~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2889                 return self._engine.get_loc(casted_key)
   2890             except KeyError as err:
-> 2891                 raise KeyError(key) from err
   2892 
   2893         if tolerance is not None:

KeyError: 'Air temperature, C'
# text/plain
: <Figure size 864x720 with 8 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/022b1522d44b039d915d6f19c1c15a943c40a4e1.png]]
:end:

#+RESULTS:
:results:
# Out [74]: 
# text/plain
: (-0.05, 0.05)

# text/plain
: <Figure size 864x720 with 8 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/3de00a68d27b0ae967820a612e0cd6baf7ee3997.png]]
:end:








#+BEGIN_SRC jupyter-python
<<import_libraries>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
temp_l = pd.read_csv(datapath+'preQC/zac_l_day_temperature.csv', parse_dates = True, index_col=0)
temp_u = pd.read_csv(datapath+'preQC/zac_u_day_temperature.csv', parse_dates = True, index_col=0)
temp_a = pd.read_csv(datapath+'preQC/zac_a_day_temperature.csv', parse_dates = True, index_col=0)


fig,ax = plt.subplots(4,1,figsize = (10,10), sharex=True)
temp_l['Air temperature, C'].plot(ax = ax[0], label = 'zac_l')
temp_u['Air temperature, C'].plot(ax = ax[0], label = 'zac_u')
temp_a['Air temperature, C'].plot(ax = ax[0], label = 'zac_a')
ax[0].legend()

d_l_u = temp_l-temp_u
d_u_a = temp_u-temp_a
d_l_a = temp_l-temp_a

d_l_u['Air temperature, C'].plot(ax=ax[1], label = 'zac_l minus zac_u')
d_u_a['Air temperature, C'].plot(ax=ax[2], label = 'zac_u minus zac_a')
d_l_a['Air temperature, C'].plot(ax=ax[3], label = 'zac_l minus zac_a')
ax[1].legend()
ax[2].legend()
ax[3].legend()


#+END_SRC

#+RESULTS:
:results:
# Out [23]: 
# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/8fba46d9acf993b9adae626bcb97605b84e58093/eda6a101171a844b87facea93bd9b6a9cbc6467b.png]]
:end:




*** Tilt

#+BEGIN_SRC jupyter-python
 <<load_libs>>
 <<data_file_paths>>

fig, ax = plt.subplots(3,1,figsize = (7.5,5), sharex = True)
 with xr.open_dataset(zac_l_path) as ds:
     df = ds[['tilt_x','tilt_y']].to_dataframe()
     df = df.where(df<22, np.nan)
     df = df.where(df>-20, np.nan)
     df = df.where(df!=0, np.nan)

 df['tilt_x'].plot(ax = ax[0], color = 'tab:blue', label = '$ZAC\_L$: $Tilt_x$')
 df['tilt_y'].loc[:'2020-Jan'].plot(ax = ax[0], color = 'deepskyblue', label = '$ZAC\_L$: $Tilt_y$')
 df['tilt_y'].loc['2020-Jan':].plot(ax = ax[0], color = 'gray', alpha = 0.3, label = '$ZAC\_L$: $Tilt_y$ discarded')


# ZAC_L Tilt_y looks wrong in 2020-2022

 with xr.open_dataset(zac_u_path) as ds:
     df = ds[['tilt_x','tilt_y']].to_dataframe()
     df = df.where(df<22, np.nan)
     df = df.where(df>-20, np.nan)
     df = df.where(df!=0, np.nan)
 df['tilt_y'].plot(ax= ax[1], color = 'tab:orange', label = '$ZAC\_U$: $Tilt_x$')
 df['tilt_x'].plot(ax= ax[1], color = 'darkgoldenrod', label = '$ZAC\_U$: $Tilt_y$')
 #df['tilt_y'].plot(ax= ax[1], color = 'tab:orange')
 #df['tilt_x'].plot(ax= ax[1], color = 'tab:blue')

# ZAC_A and ZAC_U, it looks like the data is a factor 10 off, but I don't really understand how that error could be made? 
 with xr.open_dataset(zac_a_path) as ds:
     df = ds[['tilt_x','tilt_y']].to_dataframe()
     df = df.where(df<22, np.nan)
     df = df.where(df>-20, np.nan)
     df = df.where(df!=0, np.nan)
 df['tilt_y'].plot(ax= ax[2], color = 'tab:green', label = '$ZAC\_A$: $Tilt_x$')
 df['tilt_x'].plot(ax= ax[2], color = 'limegreen', label = '$ZAC\_A$: $Tilt_y$')

ax[0].set_ylabel('Degree')
ax[1].set_ylabel('Degree')
ax[2].set_ylabel('Degree')
ax[0].legend(loc=3)
ax[1].legend(loc=3)
ax[2].legend(loc=3)


for i, ax in enumerate(ax, start=1):
    ax.text(0.01, 0.95, chr(96 + i), transform=ax.transAxes, 
            fontsize=16, fontweight='bold', va='top')

fig.tight_layout()

#fig.savefig('QCfigs/tilt.png')
fig.savefig('../glaciobasis/essd/manuscript/figures/fig06.png', dpi = 300)
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/86b300d922e7c0ed85a3a3f34f06f4e3fd07d4b6.png]]


*** Radiation
**** Utilities
#+NAME: read_in_hourly_preQC_radiation
#+BEGIN_SRC jupyter-python
rad_l = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

hourmax = 1000
maximum = hourmax

rad_l[variable][rad_l[variable]>maximum] = np.nan
rad_l[variable][rad_l[variable]<0] = np.nan

rad_u[variable][rad_u[variable]>maximum] = np.nan
rad_u[variable][rad_u[variable]<0] = np.nan

rad_a[variable][rad_a[variable]>maximum] = np.nan
rad_a[variable][rad_a[variable]<0] = np.nan
#+END_SRC

#+NAME: read_in_hourly_radiation_postQC
#+BEGIN_SRC jupyter-python
rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

#+END_SRC

#+NAME: read_in_daily_preQC_radiation
#+BEGIN_SRC jupyter-python
rad_l = pd.read_csv(datapath+'preQC/zac_l_day_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'preQC/zac_u_day_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'preQC/zac_a_day_radiation.csv', parse_dates = True, index_col=0)
daymax = 500
maximum = daymax


rad_l[variable][rad_l[variable]>maximum] = np.nan
rad_l[variable][rad_l[variable]<0] = np.nan

rad_u[variable][rad_u[variable]>maximum] = np.nan
rad_u[variable][rad_u[variable]<0] = np.nan

rad_a[variable][rad_a[variable]>maximum] = np.nan
rad_a[variable][rad_a[variable]<0] = np.nan
#+END_SRC


#+NAME: plot_rad_gradients_full_period
#+BEGIN_SRC jupyter-python
fig,ax = plt.subplots(4,1,figsize = (10,10), sharex=True)
rad_l[variable].plot(ax = ax[0], label = 'zac_l')
rad_u[variable].plot(ax = ax[0], label = 'zac_u')
rad_a[variable].plot(ax = ax[0], label = 'zac_a')
ax[0].legend()

d_l_u = (rad_l-rad_u)
d_u_a = (rad_u-rad_a)
d_l_a = (rad_l-rad_a)

d_l_u[variable].plot(ax=ax[1], label = 'zac_l minus zac_u')
d_u_a[variable].plot(ax=ax[2], label = 'zac_u minus zac_a')
d_l_a[variable].plot(ax=ax[3], label = 'zac_l minus zac_a')
ax[1].legend()
ax[2].legend()
ax[3].legend()

#+END_SRC

#+RESULTS: plot_rad_gradients_full_period
:results:
# Out [88]: 


# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/253f13095695a2517675fd67e52fe2676781336a.png]]
:end:

#+NAME: plot_rad_gradients_selected_period
#+BEGIN_SRC jupyter-python
<<plot_rad_gradients_full_period>>
ax[3].set_xlim(startdate,enddate)
#+END_SRC

**** Post QC check



#+BEGIN_SRC jupyter-python
  <<load_libs>>
  datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
  variable = 'dsr_corr'
  rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
  rad_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
  rad_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)



<<plot_rad_gradients_full_period>>
#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.legend.Legend at 0x7f1479b48880>
[[file:./.ob-jupyter/0a0c648be9a6b8d4d9726ed21cdfb68a1e030f26.png]]
:END:



***** SRin
****** Final

#+BEGIN_SRC jupyter-python
<<load_libs>>

# meta data
#rcParams.update({'font.size': 8})
zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

variable = 'dsr_corr'
ylabel = 'W/m2'


zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan


fig,ax = plt.subplots(3,1,figsize = (7.5,5))
zac_l[variable].plot(ax = ax[0], label = 'ZAC_L', color = 'tab:blue')
zac_u[variable].plot(ax = ax[1], label = 'ZAC_U', color = 'tab:orange')
zac_a[variable].plot(ax = ax[2], label = 'ZAC_A', color = 'tab:green')

#zac_l_pre[variable].plot(ax = ax[0], label = 'Discarded', color = 'tab:blue', alpha = 0.3)
zac_u_pre[variable].plot(ax = ax[1], label = 'Discarded', color = 'tab:orange', alpha = 0.3)
zac_a_pre[variable].plot(ax = ax[2], label = 'Discarded', color = 'tab:green', alpha = 0.3)

ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel(ylabel)
ax[1].set_ylabel(ylabel)
ax[2].set_ylabel(ylabel)
#ax[0].set_ylim(0,100)
#ax[1].set_ylim(0,100)
#ax[2].set_ylim(0,100)
ax[0].set_xlim(datetime.datetime(2008,4,1),datetime.datetime(2022,5,1))
ax[1].set_xlim(datetime.datetime(2008,4,1),datetime.datetime(2022,5,1))
ax[2].set_xlim(datetime.datetime(2008,4,1),datetime.datetime(2022,5,1))

ax[0].legend()
ax[1].legend()
ax[2].legend()
#fig.savefig('QCfigs/SRin_compare.png')
fig.savefig('../glaciobasis/essd/manuscript/figures/fig07.png', dpi = 300)
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/e0a0aae8b274bcb7af99cd0d15723dcb4c9ecaa9.png]]



****** Gradients

#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)

zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

variable = 'dsr_corr'
ylabel = 'W m$^{-2}$'
zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan
<<plot_gradients_full_period>>


disc_data_l = (zac_l_pre[variable].count()-zac_l[variable].count())/zac_l_pre[variable].count()*100
disc_data_u = (zac_u_pre[variable].count()-zac_u[variable].count())/zac_u_pre[variable].count()*100
disc_data_a = (zac_a_pre[variable].count()-zac_a[variable].count())/zac_a_pre[variable].count()*100
print('The total percentage of discarded data at: ')
print('zac_l is '+ str(disc_data_l))
print('zac_u is '+ str(disc_data_u))
print('zac_a is '+ str(disc_data_a))
fig.savefig('QCfigs/SRin_final.png')

#+END_SRC

#+RESULTS:
:RESULTS:
: The total percentage of discarded data at: 
: zac_l is 8.429268292682927
: zac_u is 8.400696081733468
: zac_a is 0.7916920414115837
[[file:./.ob-jupyter/13814d16c643366bd27a57f9a83e3539806989a9.png]]
:END:

****** Checking for shifts in maximum 
#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

variable = 'dsr_corr'
ylabel = 'W m$^{-2}$'

fig, ax = plt.subplots(2,1)
zac_l[variable].resample('D').median().resample('Y').mean().plot(ax = ax[0])
zac_l['cloud_cov'].resample('Y').median().plot()
#fig.savefig('QCfigs/SRin_final.png')

#+END_SRC

#+RESULTS:
:RESULTS:
: <AxesSubplot: xlabel='time'>
[[file:./.ob-jupyter/8d1f0f022937bb842013db3fef639790d587bb21.png]]
:END:
****** Calculating potential incoming solar radiation for zac_l


******* Create a G folder with the location of the GIMP dem (epsg 3413):
#+BEGIN_SRC sh :results none 
grass -e -c EPSG:3413 G
#+END_SRC

******* Load in the gimp tiles and patch together + calculate aspect and slope
    #+BEGIN_SRC sh :results none 
    grass --text ./G/PERMANENT 
    # Wollaston region
    # g.region s=74.0300000 n=74.8700000 e=-18.7100000 w=-22.2500000 res=0.01 -aps
    #g.mapset PERMANENT
    #v.in.ogr input=shp/Zackenberg_river_catchment.gpkg output=zackenberg
    #g.region vector=zackenberg res=0.0001 -aps


    r.in.gdal input=~/data/GIMP/tile_5_3_fit_30m_dem.tif output=dem53 -o --o
    r.in.gdal input=~/data/GIMP/tile_5_4_fit_30m_dem.tif output=dem54 -o --o
    g.region raster=dem53,dem54
    r.patch input=dem53,dem54 output=demNE --o
    g.remove -f type=raster pattern=dem* exclude=demNE
    #g.region raster=demNE -p
    #v.in.ogr input=shp/zackenberg_river_catchment.shp output=zackenberg --o
    #r.in.gdal input=/home/shl@geus.dk/data/gem/geobasis/Zackenberg_LastDoyOfObservedSnow_2021_epsg3413.tif output=SED_2021 --o

    v.in.ogr input=GRASS_files/aoi.shp output=aoi --o
    #v.in.ogr input=GRASS_files/mast_aoi.shp output=aoi --o
    #g.region raster=SED_2021
    g.region vector=aoi -aps

    #g.region raster=demNE -p

    r.slope.aspect elevation=demNE aspect=aspect.dem slope=slope.dem --overwrite

    #+END_SRC



******* Calculating the sun parameters
    #+BEGIN_SRC sh :results none
    g.mapset -c SUN
    g.region -dp

    r.horizon elevation=demNE step=1 output=horangle

    #g.remove -f type=raster pattern=horangle*
    #g.remove -f type=raster pattern=glob*
    #g.remove -f type=raster pattern=it*
    #g.remove -f type=raster pattern=b05*
    #g.remove -f type=raster pattern=glob05*


    #+END_SRC



    #+BEGIN_SRC sh
    grass --text ./G/SUN
    g.region -dp
    for i in {1..365..1}
    do
    r.sun elevation=demNE linke_value=1.0 distance_step=1.5 horizon_basename=horangle horizon_step=1  aspect=aspect.dem slope=slope.dem albedo_value=0.2 day=$i glob_rad=glob$i insol_time=it$i nproc=4 --overwrite
    done
    #+END_SRC

    #+RESULTS:

******* Creating the strds and write out as netcdf

******** Writing lists for the strds to order the days from 1 to 365
    #+BEGIN_SRC sh
    #grass --text ./G/SUN 
    variable=glob
    g.list raster pattern="${variable}?" sep=newline > ${variable}_list.txt

    for i in {1..9..1}
    do
    g.list raster pattern="${variable}${i}?" sep=newline >> ${variable}_list.txt
    done

    for i in {10..36..1}
    do
    g.list raster pattern="${variable}${i}?" sep=newline >> ${variable}_list.txt
    done
    #+END_SRC

    #+RESULTS:


******** Creating the temporal datasets and write out netcdfs
    #+BEGIN_SRC sh :results none
    grass --text ./G/SUN 

    #g.region -dp
    # Setting the three dimensional region
    #g.region s=74.0300000 n=74.8700000 e=-18.7100000 w=-22.2500000 t=366.0 b=1.0 res=0.01 tbres=1 res3=0.01 -p3

    #g.region raster=demNE -ap
    #g.region vector=aoi -ap

    r.out.gdal in=demNE output=GRASS_files/mast_aoi.tif --overwrite

    t.create type=strds temporaltype=relative output=glob_radiation_daily title="Daily incoming radiation" description="Dataset with calculated daily incoming solar radiation" --overwrite
 
    t.register -i type=raster input=glob_radiation_daily file=glob_list.txt start=1 increment=1 unit='days' --overwrite 

    t.info input=glob_radiation_daily

    g.region raster=demNE vector=aoi tbres=1 b=100 t=200 -ap3


    t.rast.to.rast3 input=glob_radiation_daily output=glob_radiation_daily_r3 --overwrite


    r3.info glob_radiation_daily_r3
    #g.region raster_3d=glob_radiation_daily_r3 -p

    r3.out.netcdf input=glob_radiation_daily_r3 output=GRASS_files/glob_radiation_daily.nc --overwrite


    #t.create type=strds temporaltype=relative output=insolation_time_daily title="Daily direct beam radiation" description="Dataset with calculated insolation time" --overwrite 

    #t.register -i type=raster input=insolation_time_daily file=it_list.txt start=1 increment=1 unit='days' --overwrite 
    #t.rast.to.rast3 input=insolation_time_daily output=insolation_time_daily_r3
    #g.region raster_3d=insolation_time_daily_r3
    #r3.out.netcdf input=insolation_time_daily_r3 output=netcdfs/data/wollaston/insolation_time_daily.nc --overwrite



    #+END_SRC

    Sampling the strds
    #+BEGIN_SRC sh 

    v.in.ogr input=GRASS_files/zac_l_pos.shp output=zac_l_pos
    v.in.ogr input=GRASS_files/mast_pos.shp output=mast_pos

    t.rast.what points=mast_pos strds=glob_radiation_daily output=GRASS_files/glob_mast.csv null_value=NA separator=comma --overwrite
    t.rast.what points=zac_l_pos strds=glob_radiation_daily output=GRASS_files/glob_zac_l.csv null_value=NA separator=comma --overwrite

    #+END_SRC








****** Comparing potential global radiation to observations at zac_l

#+BEGIN_SRC jupyter-python
<<load_libs>>
from matplotlib import rcParams
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)


zac_l['I'] = zac_l['I'].where(zac_l['I']>=0,0)

rcParams.update({'font.size': 8})

for year in range(2008,2022+1):
    fig, ax = plt.subplots(1,1, figsize = (3,2))
    zac_l[['I','dsr_corr']].loc[str(year)].plot(ax = ax)
    ax.set_ylim(0,900)
    fig.tight_layout()
    fig.savefig('QCfigs/zac_l_SRin_vs_I_'+str(year)+'.png')

zac_u['I'] = zac_u['I'].where(zac_u['I']>=0,0)

for year in range(2012,2022+1):
    fig, ax = plt.subplots(1,1, figsize = (3,2))
    zac_u[['I','dsr_corr']].loc[str(year)].plot(ax = ax)
    ax.set_ylim(0,900)
    fig.tight_layout()
    fig.savefig('QCfigs/zac_u_SRin_vs_I_'+str(year)+'.png')

zac_a['I'] = zac_a['I'].where(zac_a['I']>=0,0)

for year in range(2009,2019+1):
    fig, ax = plt.subplots(1,1, figsize = (3,2))
    zac_a[['I','dsr_corr']].loc[str(year)].plot(ax = ax)
    ax.set_ylim(0,900)
    fig.tight_layout()
    fig.savefig('QCfigs/zac_a_SRin_vs_I_'+str(year)+'.png')


#+END_SRC

#+RESULTS:
:RESULTS:
: /tmp/ipykernel_11697/2881825465.py:34: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
:   fig, ax = plt.subplots(1,1, figsize = (3,2))
[[file:./.ob-jupyter/40f8136395de506179f733219868211f05ddd1e3.png]]
[[file:./.ob-jupyter/7351cc073c0eff82807d2fcee054dd1ae897a808.png]]
[[file:./.ob-jupyter/76119b39a0017f6b80210fba54b6f00cc62401b5.png]]
[[file:./.ob-jupyter/0818ef3b9c1ef439c85115e07eb0803d4c0ed8d1.png]]
[[file:./.ob-jupyter/f2bb225da03eb5e7567e4108bc03fd935c255371.png]]
[[file:./.ob-jupyter/ff3a1a3d3a4b05cf4f0ff10d7ac2543f63ed93b9.png]]
[[file:./.ob-jupyter/8dbf7c24f3388916eaaf9a2f184e8343075c84c6.png]]
[[file:./.ob-jupyter/d90a5d87e6cc8086ae246068cd109fdfa3bd59cf.png]]
[[file:./.ob-jupyter/f546029ccdb3dcaea0b53025d8586834a67d86b7.png]]
[[file:./.ob-jupyter/fb28d97dc880e6ec9354b0c4125aa11f04f3c738.png]]
[[file:./.ob-jupyter/13c55a7e9d551e258183a3e59236fdeceef3942a.png]]
[[file:./.ob-jupyter/5ee37afc979a60c50439b1aae41d09ad044044a0.png]]
[[file:./.ob-jupyter/6163b8b9614f8cf75e6e358809d65a6ddb6c9fc0.png]]
[[file:./.ob-jupyter/dd66aa5370f92c3b8da38d72e601e76ceebe2049.png]]
[[file:./.ob-jupyter/70d10aabcda0c2363a3144a1e875c2ce64894a94.png]]
[[file:./.ob-jupyter/e7b53fda55ac64e0c12838ee77e6a5ca16dff7ef.png]]
[[file:./.ob-jupyter/f967a2b2c99b15ab4fe0f234b328cae17eb2a3d7.png]]
[[file:./.ob-jupyter/12728fbc0b80ff13aaaad5311a0622922a42c182.png]]
[[file:./.ob-jupyter/e5108d07b291e3fd5f67db31f8a5fe3af24c86e7.png]]
[[file:./.ob-jupyter/e479a334383c4d3d5e6f59daf2c37a6d505e1c88.png]]
[[file:./.ob-jupyter/067059b8b66a8faf3f15963496d81155862800ab.png]]
[[file:./.ob-jupyter/e77951de0579fa5c734fbfeacf5f53c8111a35a0.png]]
[[file:./.ob-jupyter/92e433424c1d066f3220a0bf0c917fb92517e811.png]]
[[file:./.ob-jupyter/61ba5ba123919d5d96c42af57afe43aa11a02c8b.png]]
[[file:./.ob-jupyter/f017cd88965dde7c31f41020a3e8e666dbbeb4d8.png]]
[[file:./.ob-jupyter/969940acf77f2bc94641ab9eda07d505a2e79046.png]]
[[file:./.ob-jupyter/953c018af9ae2010ffb4a3b7d8680eed56df4fde.png]]
[[file:./.ob-jupyter/3e6a069aafc3451f27de3d839aeff7a543c02c88.png]]
[[file:./.ob-jupyter/5f5acd82977c08995a5afe7b8cf39e6e86acf204.png]]
[[file:./.ob-jupyter/4e4b42a9d1b3aae486a34602ff1879989f1db8f1.png]]
[[file:./.ob-jupyter/67f57fd2ffdee611b1b8fd2833b9dd855f871735.png]]
[[file:./.ob-jupyter/756b0acaefff4a70762bc4ef86a9873677d7a137.png]]
[[file:./.ob-jupyter/fa11fac4943994ac45bcc6e502bd14ed222dda39.png]]
[[file:./.ob-jupyter/7b555c0f2920eaa05e40d714e1a0b4f94dd086f6.png]]
[[file:./.ob-jupyter/4c695345546b32fba559717c4533998ddd71f567.png]]
[[file:./.ob-jupyter/11718c4b54fcf1b8d61b4fe9937a0912f0b24169.png]]
[[file:./.ob-jupyter/ba4ae1b85d628c7bec9fec2e4a2a1672a951d30a.png]]
:END:

#+BEGIN_SRC jupyter-python
#mast_swin = pd.read_csv('/home/shl@geus.dk/data/gem/climatebasis/climate_mast/View_ClimateBasis_Zackenberg_Data_Radiation_Short_wave_incoming_radiation_@_200_cm__5min_average_W_m2270920221347180965.csv', sep = '\t', index_col = 0, parse_dates = {'date':['Date','Time']}, na_values = -9999)


#glob_rad = pd.read_csv('GRASS_files/glob_zac_l.csv', header = None)
#glob_rad.columns = ['x', 'y', 'doy','doy+1','glob_rad']
#glob_rad.set_index('doy', inplace = True)
#glob_rad = pd.DataFrame(glob_rad['glob_rad'])


variable = 'dsr_corr'
ylabel = 'W m$^{-2}$'

dsr_corr_sum = pd.DataFrame(zac_l['dsr_corr'].interpolate().resample('D').sum())
#usr_corr_sum = pd.DataFrame(zac_l['usr_corr'].interpolate().resample('D').sum())

# calculate the difference in days between each date
df = pd.DataFrame(zac_l['dsr_corr']).dropna()
df['diff'] = df.index.to_series().diff()

# only interpolate where the difference to the previous date is <= 1 day
df.loc[df['diff'] <= pd.Timedelta(days=1), 'dsr_corr'] = df.loc[df['diff'] <= pd.Timedelta(days=1), 'dsr_corr'].interpolate()
usr_corr_sum = df['dsr_corr'].resample('D').sum()

dsr_sum = pd.DataFrame(zac_l['dsr'].resample('D').sum())
usr_sum = pd.DataFrame(zac_l['usr'].resample('D').sum())

alb_raw = zac_l['usr'].resample('D').sum()/zac_l['dsr'].resample('D').sum()
mast_dsr_sum = pd.DataFrame(mast_swin['SRI (W/m2)'].resample('H').mean().resample('D').sum())

day_max = pd.DataFrame(zac_l[variable].resample('D').max())

years = []
#day_max.groupby([day_max.index.dayofyear]).max().plot(ax = ax)

from scipy.signal import savgol_filter
from scipy.ndimage.filters import maximum_filter1d

y_max = maximum_filter1d(dsr_corr_sum['dsr_corr'], size=7)
top = savgol_filter(y_max, 101, 3)  # window size 51, polynomial order 3
dsr_corr_sum['max'] = top
#+END_SRC

#+BEGIN_SRC jupyter-python
SRin = pd.DataFrame(dsr_corr_sum['max'])
mask = (SRin.index.year != 2015) & (SRin.index.year != 2020) & (SRin.index.year != 2021) & (SRin.index.year != 2022)
SRin = SRin[mask]


SRin_stats = pd.DataFrame(SRin.groupby(SRin.index.dayofyear).max())
SRin_stats['min'] = SRin.groupby(SRin.index.dayofyear).min()
SRin_stats['mean'] = SRin.groupby(SRin.index.dayofyear).mean()
SRin_stats['std'] = SRin.groupby(SRin.index.dayofyear).std()

fig, ax = plt.subplots(1,1, figsize = (12,5))

years = [2008,2009,2010,2011,2012,2013,2014,2016,2017,2018,2019]
legend = []



ax.fill_between(SRin_stats.index, SRin_stats['mean']-2*SRin_stats['std'], SRin_stats['mean']+2*SRin_stats['std'], alpha = 0.5, color = 'tab:blue')
legend.append('2 x std')

glob_rad.plot(ax=ax, linewidth = 3, color = 'black', linestyle = '--')
legend.append('glob')

for year in years:
    
    
    ax.plot(dsr_corr_sum.loc[str(year)].index.dayofyear,dsr_corr_sum.loc[str(year)]['max'], label = 'dsr_corr max- '+str(year))
    legend.append(str(year))
SRin_stats['mean'].plot(ax = ax, linewidth = 3, color = 'black')
legend.append('mean')
ax.legend(legend)
ax.set_ylabel('Wh/day')
fig.savefig('QCfigs/SRin_stats.png', dpi = 300)
  #+END_SRC

  #+RESULTS:
  [[file:./.ob-jupyter/1deace5ed9ed00c349386024267e5eaf772be845.png]]

  #+BEGIN_SRC jupyter-python
#years = []
fig, ax = plt.subplots(1,1, figsize = (12,3))

years = [2008,2009,2010,2011,2012,2013,2014,2016,2017,2018,2019]
legend = []
legend.append('glob')
glob_rad.plot(ax=ax, linewidth = 3, label = 'Potential incoming radiation')
for year in years:
    
    ax.plot(dsr_corr_sum.loc[str(year)].index.dayofyear,dsr_corr_sum.loc[str(year)]['max'], label = 'dsr_corr max- '+str(year))
    legend.append(str(year))

ax.legend(legend)

#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/5662a75ac6701360dc0a926160379af0d3006b1d.png]]

#+BEGIN_SRC jupyter-python

for year in np.arange(2008,2021):
    #ax.plot(day_max.loc[str(year)].index.dayofyear,day_max.loc[str(year)])
    fig, ax = plt.subplots(1,1, figsize = (12,3))
    #ax1 = ax.twinx()
    glob_rad.plot(ax=ax, linewidth = 3, label = 'Potential incoming radiation')
    ax.plot(dsr_corr_sum.loc[str(year)].index.dayofyear,dsr_corr_sum.loc[str(year)]['dsr_corr'], label = 'dsr_corr - '+str(year))
    ax.plot(dsr_corr_sum.loc[str(year)].index.dayofyear,dsr_corr_sum.loc[str(year)]['max'], label = 'dsr_corr max- '+str(year))
    ax.plot(dsr_sum.loc[str(year)].index.dayofyear,dsr_sum.loc[str(year)], label = 'dsr - '+str(year))
    #ax1.plot(alb_raw.loc[str(year)].index.dayofyear,alb_raw.loc[str(year)], label = 'albedo')
    #ax.plot(mast_dsr_sum.loc[str(year)].index.dayofyear,mast_dsr_sum.loc[str(year)], label = 'mast dsr - '+str(year))
    ax.plot(usr_sum.loc[str(year)].index.dayofyear,usr_sum.loc[str(year)], label = 'usr - '+str(year))
    ax.legend()
#    ax1.set_ylim(-0.5,1.5)
    fig.savefig('QCfigs/SRin/zac_l_SRin_SRout_glob_'+str(year)+'.png', dpi = 300)
    #years.append(str(year))
#ax.legend(years)


#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/d9e527af1243a8db3a5808029046b1b2d602db9e.png]]
[[file:./.ob-jupyter/1bafd650487ff7589190bd6c6f30c681eb76bcee.png]]
[[file:./.ob-jupyter/3fcb43957a9f7247765802943d8da2fed0b449a7.png]]
[[file:./.ob-jupyter/57e8adccb668bdae4fe65f099e7fcae432f95da7.png]]
[[file:./.ob-jupyter/8a74fef2c4f05b51c73b923f2a28bdc3c0adb090.png]]
[[file:./.ob-jupyter/4e8da959f5085036d9468557aa6786c643e836c9.png]]
[[file:./.ob-jupyter/5bc88ac1ee7b0b73bdc9f5b2048bb1a10020927f.png]]
[[file:./.ob-jupyter/ffe633b961345ba421cb832348708494948aefba.png]]
[[file:./.ob-jupyter/a729ff184f5b0f9ec2dea38196717e68d516ade7.png]]
[[file:./.ob-jupyter/307d3bb1b004cf220bab92755a5fb7bb13d3c8f8.png]]
[[file:./.ob-jupyter/94524784f7680c6eec0fcf24c1dab1b577c105da.png]]
[[file:./.ob-jupyter/6a45aa586e2c66578e5623b08e803ff6e7bea897.png]]
[[file:./.ob-jupyter/3fde2a7f5c28c561fad41b86de07ef7efa821341.png]]
:END:


#+BEGIN_SRC jupyter-python
import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import savgol_filter

# This is your time-series data, replace it with your actual data
y = np.random.normal(size=500)
y = np.cumsum(y)
x = np.linspace(0, 4*np.pi, 500)
y = np.sin(x) + y / 10

# This applies the Savitzky-Golay filter
# The first argument is your data
# The second argument, window_length, is the number of points to use for the polynomial fit (must be odd)
# The third argument is the polynomial order, in this case 3.
yhat = savgol_filter(y, 51, 3)  # window size 51, polynomial order 3

plt.plot(y)
plt.plot(yhat, color='red')
plt.show()
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/aecda041b449bbc2b26efcc7d48851c533bf98f1.png]]




****** Comparing potential global radiation to observations at climate mast to compare


#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

mast = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
mast_swin = pd.read_csv('/home/shl@geus.dk/data/gem/climatebasis/climate_mast/View_ClimateBasis_Zackenberg_Data_Radiation_Short_wave_incoming_radiation_@_200_cm__5min_average_W_m2270920221347180965.csv', sep = '\t', index_col = 0, parse_dates = {'date':['Date','Time']}, na_values = -9999)


glob_rad = pd.read_csv('GRASS_files/glob_mast.csv', header = None)
glob_rad.columns = ['x', 'y', 'doy','doy+1','glob_rad']
glob_rad.set_index('doy', inplace = True)
glob_rad = pd.DataFrame(glob_rad['glob_rad'])


variable = 'dsr_corr'
ylabel = 'W m$^{-2}$'



dsr_sum = pd.DataFrame(mast_swin['SRI (W/m2)'].resample('H').mean().resample('D').sum())


day_max = pd.DataFrame(zac_l[variable].resample('D').max())

years = []
#day_max.groupby([day_max.index.dayofyear]).max().plot(ax = ax)

for year in np.arange(2012,2021):
    #ax.plot(day_max.loc[str(year)].index.dayofyear,day_max.loc[str(year)])
    fig, ax = plt.subplots(1,1, figsize = (12,3))
    glob_rad.plot(ax=ax, linewidth = 3, label = 'Potential incoming radiation')
    ax.plot(dsr_sum.loc[str(year)].index.dayofyear,dsr_sum.loc[str(year)], label = 'dsr - '+str(year))
    #ax.plot(usr_sum.loc[str(year)].index.dayofyear,usr_sum.loc[str(year)], label = 'usr - '+str(year))
    ax.legend()
    fig.savefig('QCfigs/SRin/mast_SRin_glob_'+str(year)+'.png', dpi = 300)
    #years.append(str(year))
#ax.legend(years)


#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/eb0dee2d88b15bf8e736529bd0a234ec9f2e42b8.png]]
[[file:./.ob-jupyter/a316bc7362571c7590f9b2157ac0b99fc71c64d4.png]]
[[file:./.ob-jupyter/2c9d593b6a3531d4b353bbede81d092a2b361d84.png]]
[[file:./.ob-jupyter/c9c9c87d892a8863c72ceea716b99d80c625ac41.png]]
[[file:./.ob-jupyter/ca3495bf4c66ff66eef1e2b0c5e60b5912a59aea.png]]
[[file:./.ob-jupyter/8b54b37a6da9138fcfc478c1e0b4794b36dccf39.png]]
[[file:./.ob-jupyter/b798d125df2da37bd8ba78f0ac437677e4ef74e7.png]]
[[file:./.ob-jupyter/bea48ac79b8a54a6ff6c38a0a8922d260b5fdad9.png]]
[[file:./.ob-jupyter/815bf18d8518736ef3b7eb52c58dbca18854107f.png]]
:END:




***** SRout

#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)

zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

variable = 'usr_corr'
ylabel = 'W m$^{-2}$'
zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan
<<plot_gradients_full_period>>


disc_data_l = (zac_l_pre[variable].count()-zac_l[variable].count())/zac_l_pre[variable].count()*100
disc_data_u = (zac_u_pre[variable].count()-zac_u[variable].count())/zac_u_pre[variable].count()*100
disc_data_a = (zac_a_pre[variable].count()-zac_a[variable].count())/zac_a_pre[variable].count()*100
print('The total percentage of discarded data at: ')
print('zac_l is '+ str(disc_data_l))
print('zac_u is '+ str(disc_data_u))
print('zac_a is '+ str(disc_data_a))
fig.savefig('QCfigs/SRout_final.png')

#+END_SRC

#+RESULTS:
:RESULTS:
: The total percentage of discarded data at: 
: zac_l is 26.250683967794885
: zac_u is 14.664375358016976
: zac_a is 19.590042984538396
[[file:./.ob-jupyter/1890e0b63792d2546acd7acb996e3af1a499c73e.png]]
:END:


:results:
# Out [17]: 


# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/c6ed94dcc81c7bb7979b6e5c67e9916ff6590894.png]]
:end:


***** LWin

****** Final

#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data
#rcParams.update({'font.size': 8})
zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

variable1 = 'dlr'
variable2 = 'ulr'
ylabel = 'W m$^{-2}$'


#zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
#zac_l[variable][zac_l[variable]==0] = np.nan
#zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
#zac_u[variable][zac_u[variable]==0] = np.nan
#zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
#zac_a[variable][zac_a[variable]==0] = np.nan


fig,ax = plt.subplots(3,1,figsize = (7.5,5))
zac_l[variable1].plot(ax = ax[0], label = '$ZAC\_L$: $LR_{in}$', color = 'tab:blue')
zac_u[variable1].plot(ax = ax[1], label = '$ZAC\_U$: $LR_{in}$', color = 'tab:orange')
zac_a[variable1].plot(ax = ax[2], label = '$ZAC\_A$: $LR_{in}$', color = 'tab:green')

zac_l[variable2].plot(ax = ax[0], label = '$ZAC\_L$: $LR_{out}$', color = 'deepskyblue')
zac_u[variable2].plot(ax = ax[1], label = '$ZAC\_U$: $LR_{out}$', color = 'darkgoldenrod')
zac_a[variable2].plot(ax = ax[2], label = '$ZAC\_A$: $LR_{out}$', color = 'limegreen')


zac_l_pre[variable1].plot(ax = ax[0], label = '', color = 'tab:blue', alpha = 0.3)
zac_u_pre[variable1].plot(ax = ax[1], label = '', color = 'tab:orange', alpha = 0.3)
zac_a_pre[variable1].plot(ax = ax[2], label = '', color = 'tab:green', alpha = 0.3)

zac_l_pre[variable2].plot(ax = ax[0], label = '', color = 'deepskyblue', alpha = 0.3)
zac_u_pre[variable2].plot(ax = ax[1], label = '', color = 'darkgoldenrod', alpha = 0.3)
zac_a_pre[variable2].plot(ax = ax[2], label = '', color = 'limegreen', alpha = 0.3)

ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel(ylabel)
ax[1].set_ylabel(ylabel)
ax[2].set_ylabel(ylabel)
#ax[0].set_title('ZAC_L')
#ax[1].set_title('ZAC_U')
#ax[2].set_title('ZAC_A')
#ax[0].set_ylim(0,100)
#ax[1].set_ylim(0,100)
#ax[2].set_ylim(0,100)
ax[0].set_xlim(datetime.datetime(2008,4,1),datetime.datetime(2022,5,1))
ax[1].set_xlim(datetime.datetime(2008,4,1),datetime.datetime(2022,5,1))
ax[2].set_xlim(datetime.datetime(2008,4,1),datetime.datetime(2022,5,1))
ax[0].set_xlabel('')
ax[1].set_xlabel('')
ax[2].set_xlabel('')
ax[0].legend(loc = 3)
ax[1].legend(loc = 3)
ax[2].legend()

for i, ax in enumerate(ax, start=1):
    ax.text(0.01, 0.95, chr(96 + i), transform=ax.transAxes, 
            fontsize=16, fontweight='bold', va='top')

fig.tight_layout()
#fig.savefig('QCfigs/LRin_compare.png')
fig.savefig('../glaciobasis/essd/manuscript/figures/fig10.png', dpi = 300)
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/c1c6cd26ceaaa9a2877a718ca02327308d2e6127.png]]



****** gradients
#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)

zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

variable = 'dlr'
ylabel = 'W m$^{-2}$'
zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan
<<plot_gradients_full_period>>


disc_data_l = (zac_l_pre[variable].count()-zac_l[variable].count())/zac_l_pre[variable].count()*100
disc_data_u = (zac_u_pre[variable].count()-zac_u[variable].count())/zac_u_pre[variable].count()*100
disc_data_a = (zac_a_pre[variable].count()-zac_a[variable].count())/zac_a_pre[variable].count()*100
print('The total percentage of discarded data at: ')
print('zac_l is '+ str(disc_data_l))
print('zac_u is '+ str(disc_data_u))
print('zac_a is '+ str(disc_data_a))
fig.savefig('QCfigs/LRin_final.png')

#+END_SRC

#+RESULTS:
:RESULTS:
: The total percentage of discarded data at: 
: zac_l is 0.0828868551701744
: zac_u is 14.527123715363603
: zac_a is 4.292407677968663
[[file:./.ob-jupyter/1a59c7407de1347ae304ce72b70ad75d618e66a9.png]]
:END:


:results:
# Out [17]: 


# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/c6ed94dcc81c7bb7979b6e5c67e9916ff6590894.png]]
:end:


***** LWout

#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)

zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

variable = 'ulr'
ylabel = 'W m$^{-2}$'
zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan
<<plot_gradients_full_period>>


disc_data_l = (zac_l_pre[variable].count()-zac_l[variable].count())/zac_l_pre[variable].count()*100
disc_data_u = (zac_u_pre[variable].count()-zac_u[variable].count())/zac_u_pre[variable].count()*100
disc_data_a = (zac_a_pre[variable].count()-zac_a[variable].count())/zac_a_pre[variable].count()*100
print('The total percentage of discarded data at: ')
print('zac_l is '+ str(disc_data_l))
print('zac_u is '+ str(disc_data_u))
print('zac_a is '+ str(disc_data_a))
fig.savefig('QCfigs/LRout_final.png')

#+END_SRC

#+RESULTS:
:RESULTS:
: The total percentage of discarded data at: 
: zac_l is 0.0
: zac_u is 14.479485012029391
: zac_a is 3.6891027255781146
[[file:./.ob-jupyter/2b6a23b5c87e474ac66979fe924681b8a29e9297.png]]
:END:


:results:
# Out [17]: 


# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/c6ed94dcc81c7bb7979b6e5c67e9916ff6590894.png]]
:end:




***** Net LW
Comparing with KPC data

#+BEGIN_SRC jupyter-python
<<load_libs>>
kpc_l = pd.read_csv('~/Downloads/KPC_L_hour.csv', index_col = 0, parse_dates = True)

lr_net = kpc_l['dlr']-kpc_l['ulr']
fig,ax = plt.subplots(2,1)
kpc_l.loc['2014']['dlr'].plot(ax=ax[0])
kpc_l.loc['2014']['ulr'].plot(ax=ax[0])
lr_net.loc['2014'].plot(ax=ax[1])
ax[0].legend()


rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)
lr_net = rad_l['dlr']-rad_l['ulr']
fig,ax = plt.subplots(2,1)
rad_l.loc['2012']['dlr'].plot(ax=ax[0])
rad_l.loc['2012']['ulr'].plot(ax=ax[0])
lr_net.loc['2012'].plot(ax=ax[1])
ax[0].legend()

#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.legend.Legend at 0x7f52d705eb50>
[[file:./.ob-jupyter/90072033330df50d82d64e61383a70327d7d9310.png]]
:END:



***** Albedo
#+BEGIN_SRC jupyter-python
<<load_libs>>
from sklearn.metrics import mean_squared_error,r2_score
import matplotlib.colors as mcolors
# meta data
#rcParams.update({'font.size': 8})
zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)

zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)
deep_purple = pd.read_csv('shunan_feng_albedo/zac_l_alb.csv')
deep_purple.index = pd.to_datetime(deep_purple['system:time_start'], format='%b %d, %Y')
deep_purple.drop(['system:time_start'], axis = 1, inplace = True)
deep_purple.index.name = 'Date'
deep_purple.rename(columns={'visnirAlbedo':'Albedo'}, inplace = True)

albedo = pd.DataFrame(zac_l['albedo'].dropna().resample('D').mean())
albedo.index.name = 'Date'

albedo_m2 = pd.DataFrame(zac_l['usr_corr'].resample('D').sum()/zac_l['dsr_corr'].resample('D').sum())
albedo_m2 = albedo_m2.where(albedo_m2> 0, np.nan)
albedo_m2 = albedo_m2.where(albedo_m2< 1, np.nan)
albedo_m2.index.name = 'Date'
albedo_m2.rename(columns = {0:'albedo'}, inplace = True)
print(albedo_m2)
#fig,ax = plt.subplots(1,1, figsize = (10,5))
#albedo.loc['2008'].plot(ax = ax)
#albedo_m2.loc['2008'].plot(ax = ax)

df = pd.merge(albedo_m2, deep_purple['Albedo'], on='Date', how='outer')

#print(df)
df = df.dropna()

years = df.index.year
#cmap = plt.cm.tab20
#norm = mcolors.Normalize(vmin=df['Year'].min(), vmax=df['Year'].max())
unique_years = np.unique(years)
color_map = plt.get_cmap('nipy_spectral', len(unique_years))
norm = plt.Normalize(min(unique_years), max(unique_years)+1)

fig, ax = plt.subplots(1,1, figsize = (3.5,3))
ax.plot([0, 1], [0, 1], color = 'gray', alpha = 0.5)
scatter = ax.scatter(df['albedo'], df['Albedo'], s = 5, alpha = 0.8)
#scatter = ax.scatter(df['albedo'], df['Albedo'], c=years, cmap=color_map, norm=norm, s = 5, alpha = 0.8)
#cbar = plt.colorbar(scatter, ticks=unique_years)
#cbar.set_label('Year')   
 

r2 = 1-(((df['albedo']-df['Albedo'])**2).sum())/(((df['albedo']-df['albedo'].mean())**2).sum())
r2 = np.round(r2*100)/100
#r2 = r2_score(observed_ice_melt_clean,modelled_ice_melt_clean) 
rmse = np.sqrt(mean_squared_error(df['albedo'], df['Albedo']))
rmse = np.round(rmse*100)/100

ax.text(0.01, 0.95, 'R² = '+str(r2), transform=ax.transAxes, fontsize=10, va='top')
ax.text(0.01, 0.85, 'rmse = '+str(rmse), transform=ax.transAxes, fontsize=10, va='top')

# Scatterplot of 'Value1' vs 'Value2' with colors by 'Year'
#ax.scatter(df['albedo'], df['Albedo'], color=cmap(norm(df['Year'].values)))

# Create a colorbar
#sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
#fig.colorbar(sm)

ax.set_ylabel('Satellite derived albedo')
ax.set_xlabel('In-situ albedo')
ax.set_ylim(0,1)
ax.set_xlim(0,1)
fig.tight_layout()
#fig.savefig('QCfigs/albedo_vs_deep_purple_albedo.png', dpi = 300)
fig.savefig('../glaciobasis/essd/manuscript/figures/fig09.png', dpi = 300)
#+END_SRC

#+RESULTS:
:RESULTS:
#+begin_example
                albedo
  Date                
  2008-03-29       NaN
  2008-03-30  0.644618
  2008-03-31  0.645293
  2008-04-01  0.644978
  2008-04-02  0.621005
  ...              ...
  2022-04-15       NaN
  2022-04-16       NaN
  2022-04-17       NaN
  2022-04-18       NaN
  2022-04-19       NaN

  [5135 rows x 1 columns]
#+end_example
[[file:./.ob-jupyter/b5ca8d9df2baa68bae324deca2f7b815e064755f.png]]
:END:

#+BEGIN_SRC jupyter-python

for year in range(2008,2022):
    fig,ax = plt.subplots(1,1, figsize = (10,5))
    ax.plot(albedo.loc[str(year)].index.dayofyear,albedo.loc[str(year)])
    ax.scatter(deep_purple.loc[str(year)].index.dayofyear,deep_purple.loc[str(year)], color = 'purple')
    ax.set_xlim(120,255) 
    fig.savefig('QCfigs/zac_l_albedo_deep_purple_'+str(year)+'.png', dpi = 300)
#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/fe844c2d2e777ce1afe2e83e38ed0a2a6e4319e1.png]]
[[file:./.ob-jupyter/84d2d51615f5ed9c538188f42b57c5065c2d959d.png]]
[[file:./.ob-jupyter/8499eabc1f95aad8fe34a192dad9421abe8863b7.png]]
[[file:./.ob-jupyter/645f8461d27b7efaf8673347ca254b5a4cf8ee7e.png]]
[[file:./.ob-jupyter/c67bfa8379f943cef2f7094603be64fc85f7f37e.png]]
[[file:./.ob-jupyter/5400ac3a2bf92f4f71ac264b92984a647c51df34.png]]
[[file:./.ob-jupyter/aa48d15cf5a1eb3e7e275f792f51f81f76809515.png]]
[[file:./.ob-jupyter/5c9cde55fe08d91362977ef9dce547edf1809cba.png]]
[[file:./.ob-jupyter/e519318f65d01348984eea02274253a3805445a9.png]]
[[file:./.ob-jupyter/8e0d46694b9ff346ba1f139cce33f3af1c4f7b96.png]]
[[file:./.ob-jupyter/48d3d7bb1fc5ee081937c6505d4f117c8445f0c2.png]]
[[file:./.ob-jupyter/9089200ae0fd97d254b6e3d5bce7efcb2033ce9d.png]]
[[file:./.ob-jupyter/7e9d4bd89b75c734b4848d6ae4051d5c67c30bd6.png]]
[[file:./.ob-jupyter/d7b49e9252528522e44319bc09b83c692c631470.png]]
:END:



#+BEGIN_SRC jupyter-python




legend = []
fig,ax = plt.subplots(1,1, figsize = (15,15))
for year in range(2008,2022):
    ax.plot(albedo.loc[str(year)].index.dayofyear,albedo.loc[str(year)])
    legend.append(str(year))
ax.set_xlim(200,220)    
ax.legend(legend)
#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.legend.Legend at 0x7f7754909460>
[[file:./.ob-jupyter/b07335654080efbfddb0e4f0eedbff5e0d0ca1d6.png]]
:END:


**** QC
***** paper plot
#+BEGIN_SRC jupyter-python
<<load_libs>>
from matplotlib import rcParams
# meta data


datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

rcParams.update({'font.size': 8})
startyear = 2008
endyear = 2022
station = zac_l
station_name = 'zac_l'

station['I'] = station['I'].where(station['I']>=0,0)
#fig1, ax1 = plt.subplots(1,1, figsize = (10,3))
#fig2, ax2 = plt.subplots(1,1,figsize =(10,3))
#station['dsr_corr'].resample('D').max().plot(ax=ax2)

fig, axs = plt.subplots(2,1,figsize = (3.5, 5))
ax1 = axs[0]
ax2 = axs[1]

year = 2009
ax = ax1
station['I'].loc[str(year)].plot(ax = ax, color = 'gray', alpha = 0.5, label = '$I_{toa}$' )
station['dsr'].loc[str(year)].resample('D').max().plot(ax = ax,color = 'gray', label =  'SRin max')
station['dsr'].loc[str(year)].resample('D').min().plot(ax = ax, color = 'gray', linestyle = '--', label = 'SRin min')
station['dsr_corr'].loc[str(year)].resample('D').max().plot(ax = ax, color = 'gold', label =  'SRin_corr max', alpha = 0.8)
station['dsr_corr'].loc[str(year)].resample('D').min().plot(ax = ax, color = 'gold', linestyle = '--', label = 'SRin_corr min')

    
ax.set_ylim(0,1200)
ax.legend(loc = 2, ncols = 2)
ax.set_ylabel('$Wm^{⁻2}$')

year = 2016
ax = ax2
station['I'].loc[str(year)].plot(ax = ax, color = 'gray', alpha = 0.5, label = '$I_{toa}$')
station['dsr'].loc[str(year)].resample('D').max().plot(ax = ax,color = 'gray', label =  'SRin max')
station['dsr'].loc[str(year)].resample('D').min().plot(ax = ax, color = 'gray', linestyle = '--', label = 'SRin min')
station['dsr_corr'].loc[str(year)].resample('D').max().plot(ax = ax, color = 'gold', label =  'SRin_corr max', alpha = 0.8)
station['dsr_corr'].loc[str(year)].resample('D').min().plot(ax = ax, color = 'gold', linestyle = '--', label = 'SRin_corr min')

    
ax.set_ylim(0,1200)
#ax.legend()g
ax.set_ylabel('$Wm^{⁻2}$')

fig.tight_layout()
fig.savefig('../glaciobasis/essd/manuscript/figures/fig08.png',dpi = 300)
#fig2.savefig('QCfigs/'+station_name+'_SRin_corr_max_all_years.png')

#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/db402c2e042a62d4a5885e3c8ec937e9b7a39a2d.png]]

***** Comparing potential global radiation to observations at zac_l

#+BEGIN_SRC jupyter-python
<<load_libs>>
from matplotlib import rcParams
# meta data


datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

rcParams.update({'font.size': 6})
startyear = 2008
endyear = 2022
station = zac_l
station_name = 'zac_l'

<<Plot_all_SR_in_max_min_along_with_I>>

startyear = 2012
endyear = 2022
station = zac_u
station_name = 'zac_u'

<<Plot_all_SR_in_max_min_along_with_I>>

startyear = 2009
endyear = 2019
station = zac_a
station_name = 'zac_a'

<<Plot_all_SR_in_max_min_along_with_I>>
#+END_SRC

#+RESULTS:
:RESULTS:
: /tmp/ipykernel_11697/2865738812.py:64: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
:   fig, ax = plt.subplots(1,1, figsize = (3,2))
[[file:./.ob-jupyter/13d51ab52ca9a42346c188ae40a768714b6c3bc5.png]]
[[file:./.ob-jupyter/4dc674a13aeb25f88bc2027c09f81f9c7a9b6c94.png]]
[[file:./.ob-jupyter/f32393af606fa7ca8f45cdd3065ae82bcccff34d.png]]
[[file:./.ob-jupyter/de98c50c10f99bcabe3c7c3ef0d15b6cd7def0a1.png]]
[[file:./.ob-jupyter/e44f6b29b113dd0fefef2a08a7417035ff40aa93.png]]
[[file:./.ob-jupyter/640db9ff567b86ded39a6f1571f2d7de35807118.png]]
[[file:./.ob-jupyter/a01a9fc57fb125da6a4a58a32007d42a8ca1fe1f.png]]
[[file:./.ob-jupyter/77b69c9cba39b6c6e397581a3637e64d2e9d587a.png]]
[[file:./.ob-jupyter/c4c83fdf709ba2d0e9c9238646b82206934e325d.png]]
[[file:./.ob-jupyter/0b5f49d0c1cfc946efe6e8d99ebfb5d04499edf0.png]]
[[file:./.ob-jupyter/a67d4e24bc270d8f5b9db1b9d9b048cc8675527e.png]]
[[file:./.ob-jupyter/1f1db16b4add2e3e68b2e3bde7065fc12de57dd7.png]]
[[file:./.ob-jupyter/cf0cf8de45857f0a38189ed5413949571cfaa883.png]]
[[file:./.ob-jupyter/61caf959345739b368e89938b19c7186da3a728c.png]]
[[file:./.ob-jupyter/321d7f86aa1d3353af22036770e9214aa9f04def.png]]
[[file:./.ob-jupyter/73fc3b6b8f1f5503addfd7ead6d1464eeef22f56.png]]
[[file:./.ob-jupyter/3b67c8280f4709a61f6c50b23400b2897d2d5d0e.png]]
[[file:./.ob-jupyter/73a1f77f3c0de8d349b859a3ae8788a6a8be1233.png]]
[[file:./.ob-jupyter/5ac943156e32619cd2cce33841c781e81ab49e6e.png]]
[[file:./.ob-jupyter/9cce4b8b8d82382ba72227a669ec34523314c589.png]]
[[file:./.ob-jupyter/229c221c0184597fe9094a4f8bac6595a0698def.png]]
[[file:./.ob-jupyter/9cd381700942249adb1bbdef4b8d2dba7bf401fc.png]]
[[file:./.ob-jupyter/80e939c10ac9c94fa7c9f0a530954434995186f5.png]]
[[file:./.ob-jupyter/8dab44f74a6d9765c56347a16a5ff3f43531517f.png]]
[[file:./.ob-jupyter/7ff65c6f1ce583922294aac3304350ba22f99a48.png]]
[[file:./.ob-jupyter/cd2be54f7270212fd9cfa9b8d421288c27dc82cc.png]]
[[file:./.ob-jupyter/6db99383354684a347340e2313f6ff35e0c4751e.png]]
[[file:./.ob-jupyter/f658125ca66465036dddcefc0535382089fdb547.png]]
[[file:./.ob-jupyter/c9058304c1297984b33fc79b17a6620f35d19b43.png]]
[[file:./.ob-jupyter/f2af2774f4088922fc503bcb25335c0375c96635.png]]
[[file:./.ob-jupyter/aff957fc043eba1dc0471e09c2228dbc9d4de9f2.png]]
[[file:./.ob-jupyter/f4de197f31de79c140b193012da5065e47344ada.png]]
[[file:./.ob-jupyter/741071845854dbfdfce210a57957287dd951e5ae.png]]
[[file:./.ob-jupyter/d3e43e800c854ccaf969c105dd965330daf7c0f7.png]]
[[file:./.ob-jupyter/d449d14f6303ff0cdd57bbedfc445a31a9c265a7.png]]
[[file:./.ob-jupyter/8dd502fa0f69407a22c4b05b04b33afebe77169f.png]]
[[file:./.ob-jupyter/28b3cdc7e4c692846a3169015fd7133c2327a6b6.png]]
[[file:./.ob-jupyter/9447aa97421b75360122acbb6ff2506b25d742e9.png]]
[[file:./.ob-jupyter/c53b0f0cd2697d0945748a7e853a237cd154fcff.png]]
[[file:./.ob-jupyter/f5ff4778886f2bc56e18017c4b5381da2283783b.png]]
[[file:./.ob-jupyter/6a984ef11b234690328663be9021de91a37d7087.png]]
[[file:./.ob-jupyter/d0dfec227c14738e8186eba4e65569a9ce3c44e9.png]]
[[file:./.ob-jupyter/bf0cd5b3f3fa037ade376c9d316054452b0838a5.png]]
:END:

#+NAME: Plot_all_SR_in_max_min_along_with_I
#+BEGIN_SRC jupyter-python
station['I'] = station['I'].where(station['I']>=0,0)
fig1, ax1 = plt.subplots(1,1, figsize = (10,3))
fig2, ax2 = plt.subplots(1,1,figsize =(10,3))
station['dsr_corr'].resample('D').max().plot(ax=ax2)
for year in range(startyear,endyear+1):
    y = station['dsr_corr'].loc[str(year)].resample('D').max()
    x = station['dsr_corr'].loc[str(year)].resample('D').max().index.dayofyear

    ax1.plot(x,y.rolling(14, center = True).max(), label = str(year))

    fig, ax = plt.subplots(1,1, figsize = (3,2))
    station['I'].loc[str(year)].plot(ax = ax)
    station['dsr'].loc[str(year)].resample('D').max().plot(ax = ax,color = 'tab:green', label =  'SRin max')
    station['dsr'].loc[str(year)].resample('D').min().plot(ax = ax, color = 'tab:green', linestyle = '--', label = 'SRin min')
    #station['dsr_corr'].loc[str(year)].plot(ax = ax, color = 'tab:orange')
    station['dsr_corr'].loc[str(year)].resample('D').max().plot(ax = ax, color = 'tab:orange', label =  'SRin_corr max')
    station['dsr_corr'].loc[str(year)].resample('D').min().plot(ax = ax, color = 'tab:orange', linestyle = '--', label = 'SRin_corr min')
    #station['dsr'].loc[str(year)].plot(ax = ax, linestyle = '--', linewidth = 0.5, alpha = 0.5, color = 'tab:green')
    
    ax.set_ylim(0,900)
    ax.legend()
    fig.tight_layout()
    fig.savefig('QCfigs/'+station_name+'_SRin_vs_I_'+str(year)+'.png')
fig1.savefig('QCfigs/'+station_name+'_SRin_corr_max_all_years_rolling.png')
fig2.savefig('QCfigs/'+station_name+'_SRin_corr_max_all_years.png')

#+END_SRC

***** Outliers

#+BEGIN_SRC jupyter-python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rad_l = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)


fig,ax = plt.subplots(2,1,figsize = (10,5))
rad_l[['dsr_corr','usr_corr']].plot(ax = ax[0])
rad_l[['dlr','ulr']].plot(ax = ax[1])

fig,ax = plt.subplots(2,1,figsize = (10,5))
rad_u[['dsr_corr','usr_corr']].plot(ax = ax[0])
rad_u[['dlr','ulr']].plot(ax = ax[1])

fig,ax = plt.subplots(2,1,figsize = (10,5))
rad_a[['dsr_corr','usr_corr']].plot(ax = ax[0])
rad_a[['dlr','ulr']].plot(ax = ax[1])

#+END_SRC

#+RESULTS:
:RESULTS:
: <AxesSubplot: xlabel='time'>
[[file:./.ob-jupyter/c34a9d71378fe23ef60e293e82761a37a015c702.png]]
[[file:./.ob-jupyter/71f1cc8aa784fda4ad04f83d9fa7b62dbece93d9.png]]
[[file:./.ob-jupyter/7dfb7e60e27e3723e6348715a40c0cf08eeccf96.png]]
:END:


***** Annual mean
#+BEGIN_SRC jupyter-python
<<import_libraries>>

#+END_SRC


I have a suspicion that the radiometers calibration is not really good enough
But it looks like that dsr was just higher during 2016-2019 or something like that?

#+BEGIN_SRC jupyter-python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rad_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)
mast_dsr = pd.read_csv('/home/shl@geus.dk/data/gem/climatebasis/climate_mast/View_ClimateBasis_Zackenberg_Data_Radiation_Short_wave_incoming_radiation_@_200_cm__5min_average_W_m2270920221347180965.csv', sep = '\t', index_col = 0, parse_dates = {'date':['Date','Time']}, na_values = -9999)

variable = 'dsr'
fig, ax = plt.subplots(1,1, figsize = (7,7))
rad_l[variable].resample('Y').mean().plot(ax = ax)
rad_a[variable].resample('Y').mean().plot(ax = ax)
rad_u[variable].resample('Y').mean().plot(ax = ax)
mast_dsr.resample('Y').mean().plot(ax=ax)
#ax1 = ax[0].twinx()
#rad_l[variable].resample('Y').count().plot(ax=ax1, linestyle = '--')

#+END_SRC

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example
  [0;31m---------------------------------------------------------------------------[0m
  [0;31mKeyError[0m                                  Traceback (most recent call last)
  File [0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/core/indexes/base.py:3629[0m, in [0;36mIndex.get_loc[0;34m(self, key, method, tolerance)[0m
  [1;32m   3628[0m [38;5;28;01mtry[39;00m:
  [0;32m-> 3629[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_engine[49m[38;5;241;43m.[39;49m[43mget_loc[49m[43m([49m[43mcasted_key[49m[43m)[49m
  [1;32m   3630[0m [38;5;28;01mexcept[39;00m [38;5;167;01mKeyError[39;00m [38;5;28;01mas[39;00m err:

  File [0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/_libs/index.pyx:136[0m, in [0;36mpandas._libs.index.IndexEngine.get_loc[0;34m()[0m

  File [0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/_libs/index.pyx:163[0m, in [0;36mpandas._libs.index.IndexEngine.get_loc[0;34m()[0m

  File [0;32mpandas/_libs/hashtable_class_helper.pxi:5198[0m, in [0;36mpandas._libs.hashtable.PyObjectHashTable.get_item[0;34m()[0m

  File [0;32mpandas/_libs/hashtable_class_helper.pxi:5206[0m, in [0;36mpandas._libs.hashtable.PyObjectHashTable.get_item[0;34m()[0m

  [0;31mKeyError[0m: 'dsr'

  The above exception was the direct cause of the following exception:

  [0;31mKeyError[0m                                  Traceback (most recent call last)
  Cell [0;32mIn[3], line 14[0m
  [1;32m     12[0m variable [38;5;241m=[39m [38;5;124m'[39m[38;5;124mdsr[39m[38;5;124m'[39m
  [1;32m     13[0m fig, ax [38;5;241m=[39m plt[38;5;241m.[39msubplots([38;5;241m1[39m,[38;5;241m1[39m, figsize [38;5;241m=[39m ([38;5;241m7[39m,[38;5;241m7[39m))
  [0;32m---> 14[0m [43mrad_l[49m[43m[[49m[43mvariable[49m[43m][49m[38;5;241m.[39mresample([38;5;124m'[39m[38;5;124mY[39m[38;5;124m'[39m)[38;5;241m.[39mmean()[38;5;241m.[39mplot(ax [38;5;241m=[39m ax)
  [1;32m     15[0m rad_a[variable][38;5;241m.[39mresample([38;5;124m'[39m[38;5;124mY[39m[38;5;124m'[39m)[38;5;241m.[39mmean()[38;5;241m.[39mplot(ax [38;5;241m=[39m ax)
  [1;32m     16[0m rad_u[variable][38;5;241m.[39mresample([38;5;124m'[39m[38;5;124mY[39m[38;5;124m'[39m)[38;5;241m.[39mmean()[38;5;241m.[39mplot(ax [38;5;241m=[39m ax)

  File [0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/core/frame.py:3505[0m, in [0;36mDataFrame.__getitem__[0;34m(self, key)[0m
  [1;32m   3503[0m [38;5;28;01mif[39;00m [38;5;28mself[39m[38;5;241m.[39mcolumns[38;5;241m.[39mnlevels [38;5;241m>[39m [38;5;241m1[39m:
  [1;32m   3504[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_getitem_multilevel(key)
  [0;32m-> 3505[0m indexer [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mcolumns[49m[38;5;241;43m.[39;49m[43mget_loc[49m[43m([49m[43mkey[49m[43m)[49m
  [1;32m   3506[0m [38;5;28;01mif[39;00m is_integer(indexer):
  [1;32m   3507[0m     indexer [38;5;241m=[39m [indexer]

  File [0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/core/indexes/base.py:3631[0m, in [0;36mIndex.get_loc[0;34m(self, key, method, tolerance)[0m
  [1;32m   3629[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_engine[38;5;241m.[39mget_loc(casted_key)
  [1;32m   3630[0m [38;5;28;01mexcept[39;00m [38;5;167;01mKeyError[39;00m [38;5;28;01mas[39;00m err:
  [0;32m-> 3631[0m     [38;5;28;01mraise[39;00m [38;5;167;01mKeyError[39;00m(key) [38;5;28;01mfrom[39;00m [38;5;21;01merr[39;00m
  [1;32m   3632[0m [38;5;28;01mexcept[39;00m [38;5;167;01mTypeError[39;00m:
  [1;32m   3633[0m     [38;5;66;03m# If we have a listlike key, _check_indexing_error will raise[39;00m
  [1;32m   3634[0m     [38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise[39;00m
  [1;32m   3635[0m     [38;5;66;03m#  the TypeError.[39;00m
  [1;32m   3636[0m     [38;5;28mself[39m[38;5;241m.[39m_check_indexing_error(key)

  [0;31mKeyError[0m: 'dsr'
#+end_example
[[file:./.ob-jupyter/8c4e1acd4e4684aa02f58661ad2e055d4fb23321.png]]
:END:



***** Drift
#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

variable = 'dsr_corr'
ylabel = 'W$m^{-2}$'



fig,ax = plt.subplots(3,1,figsize = (10,10), sharex=True)
zac_l[variable].plot(ax = ax[0], label = 'SRin at ZAC_L', color = 'tab:blue')
zac_u[variable].plot(ax = ax[1], label = 'SRin at ZAC_U', color = 'tab:orange')
zac_a[variable].plot(ax = ax[2], label = 'SRin at ZAC_A', color = 'tab:pink')
ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel(ylabel)
ax[1].set_ylabel(ylabel)
ax[2].set_ylabel(ylabel)
ax[0].legend()
ax[1].legend()
ax[2].legend()
fig.savefig('QCfigs/SRin_drift.png')

#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/e037a71eedcd51482c7acf51d0b9022011fff232.png]]



***** Gradients
     #+BEGIN_SRC jupyter-python
<<import_libraries>>
# meta data

#zac_l_elev = 644
#zac_u_elev = 877
#zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
variable = 'dsr_corr'

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rad_l = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

#<<read_in_hourly_preQC_radiation>>
#<<plot_rad_gradients_full_period>>
startdate=datetime.datetime(2008,1,1)
enddate= datetime.datetime(2008,12,31)
# We will remove 2011-03-15 13:00 to 2011-03-15 23:00

d_l_u = (rad_l-rad_u)
d_u_a = (rad_u-rad_a)
d_l_a = (rad_l-rad_a)


for year in range(2008, 2022+1):
    fig,ax = plt.subplots(4,1,figsize = (10,10), sharex=True)
    if year >= 2009 and year <2020:
        rad_a[variable][str(year)].plot(ax = ax[0], label = 'zac_a')
    ax[0].legend()
    ax[0].set_ylim(0,600)
    rad_u[variable][str(year)].plot(ax = ax[0], label = 'zac_u')
    rad_l[variable][str(year)].plot(ax = ax[0], label = 'zac_l')
    
    


    d_l_u[variable][str(year)].plot(ax=ax[1], label = 'zac_l minus zac_u')
    d_u_a[variable][str(year)].plot(ax=ax[2], label = 'zac_u minus zac_a')
    d_l_a[variable][str(year)].plot(ax=ax[3], label = 'zac_l minus zac_a')
    ax[1].legend()
    ax[2].legend()
    ax[3].legend()
    fig.savefig('QCfigs/'+variable+'_'+str(year)+'.png')


#+END_SRC

#+RESULTS:
:RESULTS:
: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[[file:./.ob-jupyter/8a0dec176edcd546de649fea449222a00333cabb.png]]
[[file:./.ob-jupyter/e32d13d6449ca38130caada8145bef89cb573cb5.png]]
[[file:./.ob-jupyter/ef61f90b79a0b5457df792dcad15f21a4703acfa.png]]
[[file:./.ob-jupyter/b203716bf4f8296126060eaed2cb58a2eae2f67d.png]]
[[file:./.ob-jupyter/9741524bc634e9b4b2ed478073a47673867157a8.png]]
[[file:./.ob-jupyter/5756d1cca961bbf6af35661c2660879b5db8bd4d.png]]
[[file:./.ob-jupyter/3033d8b4b08ddb5271d6c129849360e5de536f11.png]]
[[file:./.ob-jupyter/51c68102532c119b1e355f3acd726aa21d9e7b32.png]]
[[file:./.ob-jupyter/0f7a47c57500d83aaa03c7d758de81d8e2d4e2f6.png]]
[[file:./.ob-jupyter/4c073005a65ae4aa10832dcadff6a7788b49871b.png]]
[[file:./.ob-jupyter/13396659ffecebd7299a06846e119e3ee9387084.png]]
[[file:./.ob-jupyter/e3b0b1c7d69d709813518d250a763cad05b778df.png]]
[[file:./.ob-jupyter/c5669617ce88158893b1de445f17310f146d46db.png]]
[[file:./.ob-jupyter/ff46d2c8de0940431e042003d8a4298bf33dcd4f.png]]
[[file:./.ob-jupyter/77f22f219dd30ab660403c041b344967dac6efc1.png]]
:END:



**** The issue from setting up zac_a

#+BEGIN_SRC jupyter-python
<<read_in_hourly_radiation>>
startdate=datetime.datetime(2009,8,5)
enddate= datetime.datetime(2009,8,6)
# Startdate of record: 2009-08-06 21:00
<<plot_rad_gradients_selected_period>>
#+END_SRC

#+RESULTS:
:results:
# Out [102]: 
# text/plain
: (347064.0, 347088.0)

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/b890a52128f9867bd50d60a9eaa93448c2c3da7c.png]]
:end:



**** The visit in 2010
#+BEGIN_SRC jupyter-python
<<read_in_hourly_radiation>>
startdate=datetime.datetime(2010,5,10)
enddate= datetime.datetime(2010,5,17)
<<plot_rad_gradients_selected_period>>
#+END_SRC

#+RESULTS:
:results:
# Out [155]: 
# text/plain
: (353736.0, 353904.0)

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/9c1de3d847d37cc37f9ac270a1f1f025cd5dee96.png]]
:end:


**** The several issues at zac_a in early 2011
#+BEGIN_SRC jupyter-python
<<read_in_hourly_radiation>>
startdate=datetime.datetime(2011,3,14)
enddate= datetime.datetime(2011,3,17)
# We will remove 2011-03-15 13:00 to 2011-03-15 23:00
<<plot_rad_gradients_selected_period>>
#+END_SRC

#+RESULTS:
:results:
# Out [118]: 
# text/plain
: (361128.0, 361200.0)

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/0f573b6bb5939ca316b05244438a76481c0b79cb.png]]
:end:

#+BEGIN_SRC jupyter-python
<<read_in_hourly_radiation>>
#<<read_in_hourly_radiation_postQC>>
startdate=datetime.datetime(2011,5,4)
enddate= datetime.datetime(2011,5,6)
# We will remove 2011-05-04 12:00 to 2011-05-05 17:00
<<plot_rad_gradients_selected_period>>
#+END_SRC

#+RESULTS:
:results:
# Out [130]: 
# text/plain
: (362352.0, 362400.0)

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/8e563233c1094ed78915f3288c4dd30548c86d6b.png]]
:end:

#+BEGIN_SRC jupyter-python
<<read_in_hourly_radiation>>
#<<read_in_hourly_radiation_postQC>>
startdate=datetime.datetime(2011,1,1)
enddate= datetime.datetime(2011,11,1)
# We will remove 2011-05-04 12:00 to 2011-05-05 17:00
<<plot_rad_gradients_selected_period>>
#+END_SRC

#+RESULTS:
:results:
# Out [134]: 
# text/plain
: (359400.0, 366696.0)

# text/plain
: <Figure size 720x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/1e6c9133963dd7d9acd5dc10bc6b43a394f544d6.png]]
:end:


**** The missing values in 2015


#+BEGIN_SRC jupyter-python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
T_air = pd.read_csv(datapath+'zac_l_hour_temperature.csv', parse_dates = True, index_col=0)
#print(T_air)
#<<read_in_hourly_radiation_postQC>>
<<read_in_hourly_preQC_radiation>>
startdate=datetime.datetime(2015,3,1)
#visit1 = datetime.datetime(2020,7,1)
#visit2 = datetime.datetime(2021,7,22)
enddate= datetime.datetime(2015,6,1)
fig, ax = plt.subplots(2,1)
ax1 = ax[0].twinx()
rad_l.loc[startdate:enddate].dsr_corr.plot(ax = ax[0])
rad_l.loc[startdate:enddate].usr_corr.plot(ax = ax[0])
rad_l.loc[startdate:enddate].dlr.plot(ax = ax[1])
rad_l.loc[startdate:enddate].ulr.plot(ax = ax[1])

T_air.loc[startdate:enddate].t_1.plot(ax = ax1, color = 'black', linewidth = 3)
#ymin,ymax = ax[1].get_ylim()
#print(ymax)
#ax[1].vlines(visit1, ymin, ymax, colors = 'black', linestyle = '--')
#ax[1].vlines(visit2, ymin, ymax, colors = 'black', linestyle = '--')
ax[0].legend()
ax[1].legend()

#ax[0].set_xlim(startdate,enddate)
#ax[1].set_xlim(startdate,enddate)

#fig.savefig('QCfigs/LR.png', dpi = 100)
#+END_SRC

#+RESULTS:
:RESULTS:
#+begin_example
                           t_1
  time                        
  2008-03-29 15:00:00      NaN
  2008-03-29 16:00:00 -16.4576
  2008-03-29 17:00:00 -15.3733
  2008-03-29 18:00:00 -15.9197
  2008-03-29 19:00:00 -15.2297
  ...                      ...
  2022-04-19 08:00:00 -13.3936
  2022-04-19 09:00:00 -12.7611
  2022-04-19 10:00:00 -13.5455
  2022-04-19 11:00:00 -11.5440
  2022-04-19 12:00:00      NaN

  [123214 rows x 1 columns]
#+end_example
: <matplotlib.legend.Legend at 0x7fdf94e2cca0>
[[file:./.ob-jupyter/0366bfcceb8de5dac96795634113caed1d571e3b.png]]
:END:



**** Zac l in 2021 before the visit

the dsr values look suspecious, maybe the station has been too tilted or the instrument has problems. I will in any case remove the data up until the visit i July
Have a look at the annual figures in QCfigs

#+BEGIN_SRC jupyter-python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
#<<read_in_hourly_radiation_postQC>>
<<read_in_hourly_preQC_radiation>>
startdate=datetime.datetime(2019,4,1)
visit1 = datetime.datetime(2020,7,1)
visit2 = datetime.datetime(2021,7,22)
enddate= datetime.datetime(2022,2,1)
fig, ax = plt.subplots(2,1)

rad_l.dsr_corr.plot(ax = ax[0])
rad_l.usr_corr.plot(ax = ax[0])
rad_l.dlr.plot(ax = ax[1])
rad_l.ulr.plot(ax = ax[1])
ymin,ymax = ax[1].get_ylim()
print(ymax)
ax[1].vlines(visit1, ymin, ymax, colors = 'black', linestyle = '--')
ax[1].vlines(visit2, ymin, ymax, colors = 'black', linestyle = '--')
ax[0].legend()
ax[1].legend()

ax[0].set_xlim(startdate,enddate)
ax[1].set_xlim(startdate,enddate)

fig.savefig('QCfigs/LR.png', dpi = 100)
#+END_SRC

#+RESULTS:
:RESULTS:
: 367.830855
[[file:./.ob-jupyter/63d34f1cff1b8190471914e26f9d37c8f0d41c0f.png]]
:END:




*** Relative humidity



Load in the data
#+BEGIN_SRC jupyter-python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rh_l = pd.read_csv(datapath+'zac_l_hour_relative_humidity.csv', parse_dates = True, index_col=0)
rh_u = pd.read_csv(datapath+'zac_u_hour_relative_humidity.csv', parse_dates = True, index_col=0)
rh_a = pd.read_csv(datapath+'zac_a_hour_relative_humidity.csv', parse_dates = True, index_col=0)

#+END_SRC

#+RESULTS:
:results:
# Out [58]: 
:end:

**** Post QC check

#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_relative_humidity.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_relative_humidity.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_relative_humidity.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_relative_humidity.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_relative_humidity.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_relative_humidity.csv', parse_dates = True, index_col=0)

boom_l = pd.read_csv(datapath+'zac_l_hour_boom_height.csv', parse_dates = True, index_col=0)
boom_u = pd.read_csv(datapath+'zac_u_hour_boom_height.csv', parse_dates = True, index_col=0)
boom_a = pd.read_csv(datapath+'zac_a_hour_boom_height.csv', parse_dates = True, index_col=0)

variable = 'rh'
ylabel = '%'


zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan


fig,ax = plt.subplots(3,1,figsize = (7.5,5), sharex=True)
zac_l['rh'].plot(ax = ax[0], label = 'ZAC_L', color = 'tab:blue')
zac_u['rh'].plot(ax = ax[1], label = 'ZAC_U', color = 'tab:orange')
zac_a['rh'].plot(ax = ax[2], label = 'ZAC_A', color = 'tab:green')

zac_l_pre['rh'].plot(ax = ax[0], label = 'Discarded', color = 'tab:blue', alpha = 0.3)
zac_u_pre['rh'].plot(ax = ax[1], label = 'Discarded', color = 'tab:orange', alpha = 0.3)
zac_a_pre['rh'].plot(ax = ax[2], label = 'Discarded', color = 'tab:green', alpha = 0.3)


#ax0 = ax[0].twinx()
#ax1 = ax[1].twinx()
#ax2 = ax[2].twinx()
#boom_l['z_boom'].plot(ax = ax0, color = 'gray', alpha = 0.5)
#boom_u['z_boom'].plot(ax = ax1, color = 'gray', alpha = 0.5)
#boom_a['z_boom'].plot(ax = ax2, color = 'gray', alpha = 0.5)


ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel('%')
ax[1].set_ylabel('%')
ax[2].set_ylabel('%')
ax[0].set_ylim(0,100)
ax[1].set_ylim(0,100)
ax[2].set_ylim(0,100)

ax[0].legend(loc = 3)
ax[1].legend(loc = 3)
ax[2].legend(loc = 3)

# Label each subplot
for i, ax in enumerate(ax, start=1):
    ax.text(0.01, 0.95, chr(96 + i), transform=ax.transAxes, 
            fontsize=16, fontweight='bold', va='top')
fig.tight_layout()
#fig.savefig('QCfigs/RH_compare.png')
fig.savefig('../glaciobasis/essd/manuscript/figures/fig05.png', dpi = 300)
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/72db4d0315dab3adbc7fe5b7618fcccdc2ce40e3.png]]

#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_relative_humidity.csv', parse_dates = True, index_col=0)

zac_u = pd.read_csv(datapath+'zac_u_hour_relative_humidity.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_relative_humidity.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_relative_humidity.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_relative_humidity.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_relative_humidity.csv', parse_dates = True, index_col=0)

variable = 'rh'
ylabel = '%'

zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan
<<plot_gradients_full_period>>


disc_data_l = (zac_l_pre[variable].count()-zac_l[variable].count())/zac_l_pre[variable].count()*100
disc_data_u = (zac_u_pre[variable].count()-zac_u[variable].count())/zac_u_pre[variable].count()*100
disc_data_a = (zac_a_pre[variable].count()-zac_a[variable].count())/zac_a_pre[variable].count()*100
print('The total percentage of discarded data at: ')
print('zac_l is '+ str(disc_data_l))
print('zac_u is '+ str(disc_data_u))
print('zac_a is '+ str(disc_data_a))
fig.savefig('QCfigs/RH_final.png')

#+END_SRC

#+RESULTS:
:RESULTS:
: The total percentage of discarded data at: 
: zac_l is 0.1724977156861908
: zac_u is 7.535239450397972
: zac_a is 4.605735444137983
[[file:./.ob-jupyter/d7f4edd6005b84ab452f1d5206b99c112681c0cd.png]]
:END:

Just checking that the RH errors at ZAC_A is not related to the correction - it is not.

#+BEGIN_SRC jupyter-python
# RH raw
<<load_libs>>
import glob
import nead
workingdir ='/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/'  
station = 'zac_a'
filelist = glob.glob(workingdir+'data_v1.0/L0M/'+station+'/'+station+'**.csv')
for infile in filelist:
    print(str(infile))
    <<read_infile_from_filelist>>
    <<add_variable_metadata>>
    ds = add_variable_metadata(ds)
    ds.rh.plot()

#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/4af3ee3a194c35cb3303255408aba9a561705f12.png]]
:END:

**** Checking for outliers
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(3,1,figsize=(10,10))
rh_l.plot(ax= ax[0])
rh_u.plot(ax= ax[1])
rh_a.plot(ax= ax[2])


#+END_SRC

#+RESULTS:
:results:
# Out [24]: 
# text/plain
: <AxesSubplot:xlabel='time'>

# text/plain
: <Figure size 720x720 with 3 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/aac3e258e7140322be07d180636f64957abc7531.png]]
:end:

**** Gradients
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (rh_l-rh_u)
d_u_a = (rh_u-rh_a)
d_l_a = (rh_l-rh_a)

rh_l['rh'].plot(ax= ax[0], label = 'zac_l')
rh_u['rh'].plot(ax= ax[0], label = 'zac_u')
rh_a['rh'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])


#+END_SRC

#+RESULTS:
:results:
# Out [45]: 
# text/plain
: <AxesSubplot:xlabel='time'>

# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/d00952effe542dc8ef228fc974256a0725e632b9.png]]
:end:

***** The issue at zac_u in 2020-2021

#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (rh_l-rh_u)
d_u_a = (rh_u-rh_a)
d_l_a = (rh_l-rh_a)

rh_l['rh'].plot(ax= ax[0], label = 'zac_l')
rh_u['rh'].plot(ax= ax[0], label = 'zac_u')
rh_a['rh'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2020,1,1),datetime.datetime(2021,9,1))
startday = datetime.datetime(2020,8,15)
endday = datetime.datetime(2021,7,21)
ymin,ymax = 0, 100
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC

#+RESULTS:
:results:
# Out [42]: 


# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/41832a329cabf6a88232d57333b9d28a69a9566b.png]]
:end:


***** zac_a data appears to be a bit more unstable - but maybe its real

We will take a look at 2013-2014
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (rh_l-rh_u)
d_u_a = (rh_u-rh_a)
d_l_a = (rh_l-rh_a)

rh_l['rh'].plot(ax= ax[0], label = 'zac_l')
rh_u['rh'].plot(ax= ax[0], label = 'zac_u')
rh_a['rh'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2013,1,1),datetime.datetime(2014,12,1))
startday = datetime.datetime(2013,1,1)
endday = datetime.datetime(2014,12,1)
ymin,ymax = 0, 100
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC

#+RESULTS:
:results:
# Out [47]: 


# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/7321fb67ebcbb97aa41988457b4399c6c263b516.png]]
:end:


*** Windspeed

**** post QC check

#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data
rcParams.update({'font.size': 8})
zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_wind_speed.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_wind_speed.csv', parse_dates = True, index_col=0)

variable = 'wspd'
ylabel = 'm/s'


zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan


fig,ax = plt.subplots(3,1,figsize = (8,6), sharex=True)
zac_l[variable].plot(ax = ax[0], label = 'ZAC_L', color = 'tab:blue')
zac_u[variable].plot(ax = ax[1], label = 'ZAC_U', color = 'tab:orange')
zac_a[variable].plot(ax = ax[2], label = 'ZAC_A', color = 'tab:pink')

zac_l_pre[variable].plot(ax = ax[0], label = 'Discarded', color = 'tab:blue', alpha = 0.3)
zac_u_pre[variable].plot(ax = ax[1], label = 'Discarded', color = 'tab:orange', alpha = 0.3)
zac_a_pre[variable].plot(ax = ax[2], label = 'Discarded', color = 'tab:pink', alpha = 0.3)

ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel(ylabel)
ax[1].set_ylabel(ylabel)
ax[2].set_ylabel(ylabel)
#ax[0].set_ylim(0,100)
#ax[1].set_ylim(0,100)
#ax[2].set_ylim(0,100)

ax[0].legend()
ax[1].legend()
ax[2].legend()
fig.savefig('QCfigs/WindSpeed_compare.png')

#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/1a1111da4821d2ce8e9c922574ec5db4bdcd3757.png]]

***** Drift 
#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_wind_speed.csv', parse_dates = True, index_col=0)

variable = 'wspd'
ylabel = 'm/s'



fig,ax = plt.subplots(3,1,figsize = (10,10), sharex=True)
zac_l[variable].plot(ax = ax[0], label = 'Wind_speed at ZAC_L', color = 'tab:blue')
zac_u[variable].plot(ax = ax[1], label = 'Wind_speed at ZAC_U', color = 'tab:orange')
zac_a[variable].plot(ax = ax[2], label = 'Wind_speed at ZAC_A', color = 'tab:pink')
ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel(ylabel)
ax[1].set_ylabel(ylabel)
ax[2].set_ylabel(ylabel)
ax[0].legend()
ax[1].legend()
ax[2].legend()
fig.savefig('QCfigs/wind_speed_drift.png')

#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/f153c22669cc894e7cacb226c3826cbdc4096469.png]]




***** Gradients

#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_wind_speed.csv', parse_dates = True, index_col=0)

zac_l_pre = pd.read_csv(datapath+'preQC/zac_l_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'preQC/zac_u_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'preQC/zac_a_hour_wind_speed.csv', parse_dates = True, index_col=0)

variable = 'wspd'
ylabel = 'm/s'
<<plot_gradients_full_period>>

disc_data_l = (zac_l_pre[variable].count()-zac_l[variable].count())/zac_l_pre[variable].count()*100
disc_data_u = (zac_u_pre[variable].count()-zac_u[variable].count())/zac_u_pre[variable].count()*100
disc_data_a = (zac_a_pre[variable].count()-zac_a[variable].count())/zac_a_pre[variable].count()*100
print('The total percentage of discarded data at: ')
print('zac_l is '+ str(disc_data_l))
print('zac_u is '+ str(disc_data_u))
print('zac_a is '+ str(disc_data_a))
fig.savefig('QCfigs/Wind_Speed_final.png')

#+END_SRC

#+RESULTS:
:RESULTS:
: The total percentage of discarded data at: 
: zac_l is 0.0
: zac_u is 13.074214373366056
: zac_a is 4.589189461488086
[[file:./.ob-jupyter/e766aba7fe1aa237375e89d2dce725e874df5aa2.png]]
:END:


#+BEGIN_SRC jupyter-python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
wspd_l = pd.read_csv(datapath+'zac_l_hour_wind_speed.csv', parse_dates = True, index_col=0)
wspd_u = pd.read_csv(datapath+'zac_u_hour_wind_speed.csv', parse_dates = True, index_col=0)
wspd_a = pd.read_csv(datapath+'zac_a_hour_wind_speed.csv', parse_dates = True, index_col=0)

#+END_SRC

#+RESULTS:
:results:
# Out [79]: 
:end:


**** Checking for outliers

It seems like windspeed is pretty solid
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(3,1,figsize=(10,10))
wspd_l.plot(ax= ax[0])
wspd_u.plot(ax= ax[1])
wspd_a.plot(ax= ax[2])


#+END_SRC

#+RESULTS:
:results:
# Out [80]: 
# text/plain
: <AxesSubplot:xlabel='time'>

# text/plain
: <Figure size 720x720 with 3 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/e75519e218cd7236a01cda12921e29f0908219ff.png]]
:end:



***** Drift
#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_wind_speed.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_wind_speed.csv', parse_dates = True, index_col=0)

variable = 'wspd'
ylabel = 'm/s'



fig,ax = plt.subplots(3,1,figsize = (10,10), sharex=True)
zac_l[variable].plot(ax = ax[0], label = 'Wind speed at ZAC_L', color = 'tab:blue')
zac_u[variable].plot(ax = ax[1], label = 'Wind speed at ZAC_U', color = 'tab:orange')
zac_a[variable].plot(ax = ax[2], label = 'Wind speed at ZAC_A', color = 'tab:pink')
ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel(ylabel)
ax[1].set_ylabel(ylabel)
ax[2].set_ylabel(ylabel)
ax[0].legend()
ax[1].legend()
ax[2].legend()
fig.savefig('QCfigs/wind_speed_drift.png')

#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/5fa38754d3522ba1d20a876287bd673193da0127.png]]



***** Gradients
     #+BEGIN_SRC jupyter-python
<<import_libraries>>
# meta data

#zac_l_elev = 644
#zac_u_elev = 877
#zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
variable = 'dsr_corr'

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
rad_l = pd.read_csv(datapath+'preQC/zac_l_hour_radiation.csv', parse_dates = True, index_col=0)
rad_u = pd.read_csv(datapath+'preQC/zac_u_hour_radiation.csv', parse_dates = True, index_col=0)
rad_a = pd.read_csv(datapath+'preQC/zac_a_hour_radiation.csv', parse_dates = True, index_col=0)

#<<read_in_hourly_preQC_radiation>>
#<<plot_rad_gradients_full_period>>
startdate=datetime.datetime(2008,1,1)
enddate= datetime.datetime(2008,12,31)
# We will remove 2011-03-15 13:00 to 2011-03-15 23:00

d_l_u = (rad_l-rad_u)
d_u_a = (rad_u-rad_a)
d_l_a = (rad_l-rad_a)


for year in range(2008, 2022+1):
    fig,ax = plt.subplots(4,1,figsize = (10,10), sharex=True)
    if year >= 2009 and year <2020:
        rad_a[variable][str(year)].plot(ax = ax[0], label = 'zac_a')
    ax[0].legend()
    ax[0].set_ylim(0,600)
    rad_u[variable][str(year)].plot(ax = ax[0], label = 'zac_u')
    rad_l[variable][str(year)].plot(ax = ax[0], label = 'zac_l')
    
    


    d_l_u[variable][str(year)].plot(ax=ax[1], label = 'zac_l minus zac_u')
    d_u_a[variable][str(year)].plot(ax=ax[2], label = 'zac_u minus zac_a')
    d_l_a[variable][str(year)].plot(ax=ax[3], label = 'zac_l minus zac_a')
    ax[1].legend()
    ax[2].legend()
    ax[3].legend()
    fig.savefig('QCfigs/'+variable+'_'+str(year)+'.png')


#+END_SRC

#+RESULTS:
:RESULTS:
: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[[file:./.ob-jupyter/8a0dec176edcd546de649fea449222a00333cabb.png]]
[[file:./.ob-jupyter/e32d13d6449ca38130caada8145bef89cb573cb5.png]]
[[file:./.ob-jupyter/ef61f90b79a0b5457df792dcad15f21a4703acfa.png]]
[[file:./.ob-jupyter/b203716bf4f8296126060eaed2cb58a2eae2f67d.png]]
[[file:./.ob-jupyter/9741524bc634e9b4b2ed478073a47673867157a8.png]]
[[file:./.ob-jupyter/5756d1cca961bbf6af35661c2660879b5db8bd4d.png]]
[[file:./.ob-jupyter/3033d8b4b08ddb5271d6c129849360e5de536f11.png]]
[[file:./.ob-jupyter/51c68102532c119b1e355f3acd726aa21d9e7b32.png]]
[[file:./.ob-jupyter/0f7a47c57500d83aaa03c7d758de81d8e2d4e2f6.png]]
[[file:./.ob-jupyter/4c073005a65ae4aa10832dcadff6a7788b49871b.png]]
[[file:./.ob-jupyter/13396659ffecebd7299a06846e119e3ee9387084.png]]
[[file:./.ob-jupyter/e3b0b1c7d69d709813518d250a763cad05b778df.png]]
[[file:./.ob-jupyter/c5669617ce88158893b1de445f17310f146d46db.png]]
[[file:./.ob-jupyter/ff46d2c8de0940431e042003d8a4298bf33dcd4f.png]]
[[file:./.ob-jupyter/77f22f219dd30ab660403c041b344967dac6efc1.png]]
:END:



**** Gradients old
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (wspd_l-wspd_u)
d_u_a = (wspd_u-wspd_a)
d_l_a = (wspd_l-wspd_a)

wspd_l['wspd'].plot(ax= ax[0], label = 'zac_l')
wspd_u['wspd'].plot(ax= ax[0], label = 'zac_u')
wspd_a['wspd'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])


#+END_SRC

#+RESULTS:
:results:
# Out [81]: 
# text/plain
: <AxesSubplot:xlabel='time'>

# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/974a9d31e0f76cd890c2907d729c3e1707ed39ee.png]]
:end:

***** The issue at zac_u in 2020 (station tilting)

#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (wspd_l-wspd_u)
d_u_a = (wspd_u-wspd_a)
d_l_a = (wspd_l-wspd_a)

wspd_l['wspd'].plot(ax= ax[0], label = 'zac_l')
wspd_u['wspd'].plot(ax= ax[0], label = 'zac_u')
wspd_a['wspd'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2020,1,1),datetime.datetime(2022,4,25))
startday = datetime.datetime(2020,8,15)
endday = datetime.datetime(2022,4,21)
ymin,ymax = ax[0].get_ylim()
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC

#+RESULTS:
:results:
# Out [68]: 


# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/a39c5117508c93e4bf5c2165c10d9c9dcff8def8.png]]
:end:



*** Pressure

#+BEGIN_SRC jupyter-python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
p_l = pd.read_csv(datapath+'zac_l_hour_pressure.csv', parse_dates = True, index_col=0)
p_u = pd.read_csv(datapath+'zac_u_hour_pressure.csv', parse_dates = True, index_col=0)
p_a = pd.read_csv(datapath+'zac_a_hour_pressure.csv', parse_dates = True, index_col=0)

#+END_SRC

#+RESULTS:
:results:
# Out [154]: 
:end:

**** Post QC check

***** Drift 
#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_pressure.csv', parse_dates = True, index_col=0)
zac_u = pd.read_csv(datapath+'zac_u_hour_pressure.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_pressure.csv', parse_dates = True, index_col=0)

variable = 'p'
ylabel = '%'



fig,ax = plt.subplots(3,1,figsize = (10,10), sharex=True)
zac_l['p'].plot(ax = ax[0], label = 'Air pressure at ZAC_L', color = 'tab:blue')
zac_u['p'].plot(ax = ax[1], label = 'Air pressure at ZAC_U', color = 'tab:orange')
zac_a['p'].plot(ax = ax[2], label = 'Air pressure at ZAC_A', color = 'tab:pink')
ax[0].grid(True)
ax[1].grid(True)
ax[2].grid(True)
ax[0].set_ylabel('hPa')
ax[1].set_ylabel('hPa')
ax[2].set_ylabel('hPa')
ax[0].legend()
ax[1].legend()
ax[2].legend()
fig.savefig('QCfigs/Pressure_drift.png')

#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/006f14a19cd72ffa63ccdc1826d2c9ca9afe3179.png]]


***** Gradient
#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

zac_l = pd.read_csv(datapath+'zac_l_hour_pressure.csv', parse_dates = True, index_col=0)

zac_u = pd.read_csv(datapath+'zac_u_hour_pressure.csv', parse_dates = True, index_col=0)
zac_a = pd.read_csv(datapath+'zac_a_hour_pressure.csv', parse_dates = True, index_col=0)

# Only outliers are removed - and I will not plot them here - so I just plotting the post QC data twice
zac_l_pre = pd.read_csv(datapath+'zac_l_hour_pressure.csv', parse_dates = True, index_col=0)
zac_u_pre = pd.read_csv(datapath+'zac_u_hour_pressure.csv', parse_dates = True, index_col=0)
zac_a_pre = pd.read_csv(datapath+'zac_a_hour_pressure.csv', parse_dates = True, index_col=0)

variable = 'p'
ylabel = 'hPa'

zac_l_pre[variable][zac_l_pre[variable]==0] = np.nan
zac_l[variable][zac_l[variable]==0] = np.nan
zac_u_pre[variable][zac_u_pre[variable]==0] = np.nan
zac_u[variable][zac_u[variable]==0] = np.nan
zac_a_pre[variable][zac_a_pre[variable]==0] = np.nan
zac_a[variable][zac_a[variable]==0] = np.nan
<<plot_gradients_full_period>>
ax[1].set_ylim(-13,-10)
ax[2].set_ylim(-13,-10)
ax[3].set_ylim(-13,-10)

disc_data_l = (zac_l_pre[variable].count()-zac_l[variable].count())/zac_l_pre[variable].count()*100
disc_data_u = (zac_u_pre[variable].count()-zac_u[variable].count())/zac_u_pre[variable].count()*100
disc_data_a = (zac_a_pre[variable].count()-zac_a[variable].count())/zac_a_pre[variable].count()*100
print('The total percentage of discarded data at: ')
print('zac_l is '+ str(disc_data_l))
print('zac_u is '+ str(disc_data_u))
print('zac_a is '+ str(disc_data_a))
fig.savefig('QCfigs/pressure_final.png')

#+END_SRC

#+RESULTS:
:RESULTS:
: The total percentage of discarded data at: 
: zac_l is 0.0
: zac_u is 0.0
: zac_a is 0.0
[[file:./.ob-jupyter/b2d4ea35bbfb57b16065951c434c621c62449cf0.png]]
:END:



**** Checking for outliers

It seems like windspeed is pretty solid
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(3,1,figsize=(10,10))
p_l.plot(ax= ax[0])
p_u.plot(ax= ax[1])
p_a.plot(ax= ax[2])


#+END_SRC

#+RESULTS:
:results:
# Out [100]: 
# text/plain
: <AxesSubplot:xlabel='time'>

# text/plain
: <Figure size 720x720 with 3 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/10c09556fcc02dcbb9723a5fbc90d4f86f5fffec.png]]
:end:

**** Gradients
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (p_l-p_u)
d_u_a = (p_u-p_a)
d_l_a = (p_l-p_a)

p_l['p'].plot(ax= ax[0], label = 'zac_l')
p_u['p'].plot(ax= ax[0], label = 'zac_u')
p_a['p'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])


#+END_SRC

#+RESULTS:
:results:
# Out [155]: 
# text/plain
: <AxesSubplot:xlabel='time'>

# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/4b8aea1ce833ce11c0350d163ab28386dd6d5d75.png]]
:end:

***** The issue in 2016 at zac_l
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (p_l-p_u)
d_u_a = (p_u-p_a)
d_l_a = (p_l-p_a)

p_l['p'].plot(ax= ax[0], label = 'zac_l')
p_u['p'].plot(ax= ax[0], label = 'zac_u')
p_a['p'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2016,1,1),datetime.datetime(2016,5,1))
startday = datetime.datetime(2016,2,26)
endday = datetime.datetime(2016,3,1)
ymin,ymax = ax[0].get_ylim()
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[2].get_ylim()
ax[2].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[3].get_ylim()
ax[3].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC

#+RESULTS:
:results:
# Out [109]: 


# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/7c0aa8ea276757a4ca76744be0a8311a47019db2.png]]
:end:





***** The issue in 2016 at zac_u
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (p_l-p_u)
d_u_a = (p_u-p_a)
d_l_a = (p_l-p_a)

p_l['p'].plot(ax= ax[0], label = 'zac_l')
p_u['p'].plot(ax= ax[0], label = 'zac_u')
p_a['p'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2016,1,1),datetime.datetime(2016,5,1))
startday = datetime.datetime(2016,4,5)
endday = datetime.datetime(2016,4,18)
ymin,ymax = ax[0].get_ylim()
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[2].get_ylim()
ax[2].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[3].get_ylim()
ax[3].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC

#+RESULTS:
:results:
# Out [118]: 


# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/d3937292b5aea274f7b8ca43e4737dda4266deff.png]]
:end:






***** The issue at zac_l in 2016-2017
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (p_l-p_u)
d_u_a = (p_u-p_a)
d_l_a = (p_l-p_a)

p_l['p'].plot(ax= ax[0], label = 'zac_l')
p_u['p'].plot(ax= ax[0], label = 'zac_u')
p_a['p'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2016,10,1),datetime.datetime(2017,4,1))
startday = datetime.datetime(2017,1,5)
endday = datetime.datetime(2017,2,22)
ymin,ymax = ax[0].get_ylim()
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[2].get_ylim()
ax[2].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[3].get_ylim()
ax[3].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC

#+RESULTS:
:results:
# Out [127]: 


# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/49166008e08a68fc1c133f8022711194d5d87bea.png]]
:end:

***** The issue at zac_u in 2016-2017
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (p_l-p_u)
d_u_a = (p_u-p_a)
d_l_a = (p_l-p_a)

p_l['p'].plot(ax= ax[0], label = 'zac_l')
p_u['p'].plot(ax= ax[0], label = 'zac_u')
p_a['p'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2016,10,1),datetime.datetime(2017,4,1))
startday = datetime.datetime(2017,1,23)
endday = datetime.datetime(2017,3,2)
ymin,ymax = ax[0].get_ylim()
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[2].get_ylim()
ax[2].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[3].get_ylim()
ax[3].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC

#+RESULTS:
:results:
# Out [142]: 


# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/46e76368edde8796fa2cc114dee3e7daec7b4c86.png]]
:end:


***** The issue at zac_l in 2018
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(4,1,figsize=(15,10), sharex = True)

d_l_u = (p_l-p_u)
d_u_a = (p_u-p_a)
d_l_a = (p_l-p_a)

p_l['p'].plot(ax= ax[0], label = 'zac_l')
p_u['p'].plot(ax= ax[0], label = 'zac_u')
p_a['p'].plot(ax= ax[0], label = 'zac_a')
ax[0].legend()
d_l_u.plot(ax = ax[1])
d_u_a.plot(ax = ax[2])
d_l_a.plot(ax = ax[3])

ax[0].set_xlim(datetime.datetime(2018,1,1),datetime.datetime(2018,6,1))
startday = datetime.datetime(2018,2,21)
endday = datetime.datetime(2018,2,28)
ymin,ymax = ax[0].get_ylim()
ax[0].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[1].get_ylim()
ax[1].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[2].get_ylim()
ax[2].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
ymin,ymax = ax[3].get_ylim()
ax[3].vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')
#+END_SRC

#+RESULTS:
:results:
# Out [152]: 


# text/plain
: <Figure size 1080x720 with 4 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/ffbfb9760db97528ddbf21a1f29c810d4cd4c0ed.png]]
:end:



*** Instrument height
**** post QC check
Gradients
boom_a.rolling(24,center=True).median().plot()
#+BEGIN_SRC jupyter-python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
variable = 'z_boom'
boom_l = pd.read_csv(datapath+'zac_l_hour_boom_height.csv', parse_dates = True, index_col=0)
boom_u = pd.read_csv(datapath+'zac_u_hour_boom_height.csv', parse_dates = True, index_col=0)
boom_a = pd.read_csv(datapath+'zac_a_hour_boom_height.csv', parse_dates = True, index_col=0)

fig, ax = plt.subplots(3,1,figsize=(10,10), sharex = True, sharey = True)
boom_l.plot(ax=ax[0])
boom_l.rolling(24,center=True).median().rolling(24*7,center=True).median().plot(ax = ax[0])

boom_u.plot(ax=ax[1])
boom_u.rolling(24,center=True).median().plot(ax = ax[1])
boom_a.plot(ax=ax[2])
boom_a.rolling(24,center=True).median().plot(ax = ax[2])
ax[2].set_xlim(datetime.datetime(2008,1,1),datetime.datetime(2022,12,31))
#+END_SRC

#+RESULTS:
:RESULTS:
| 333096.0 | 464568.0 |
[[file:./.ob-jupyter/bcd36ee3d30f9763450e8db3ba8b636d6984d84f.png]]
:END:

#+BEGIN_SRC jupyter-python
<<load_libs>>
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
variable = 'z_boom'
boom_l = pd.read_csv(datapath+'zac_l_day_boom_height.csv', parse_dates = True, index_col=0)
boom_u = pd.read_csv(datapath+'zac_u_day_boom_height.csv', parse_dates = True, index_col=0)
boom_a = pd.read_csv(datapath+'zac_a_day_boom_height.csv', parse_dates = True, index_col=0)

fig, ax = plt.subplots(3,1,figsize=(10,10), sharex = True, sharey = True)
boom_l.plot(ax=ax[0])
boom_l.rolling(3,center=True).median().plot(ax = ax[0])

boom_u.plot(ax=ax[1])
boom_u.rolling(3,center=True).median().plot(ax = ax[1])
boom_a.plot(ax=ax[2])
boom_a.rolling(3,center=True).median().plot(ax = ax[2])
ax[2].set_xlim(datetime.datetime(2008,1,1),datetime.datetime(2022,12,31))
#+END_SRC

#+RESULTS:
:RESULTS:
| 13879.0 | 19357.0 |
[[file:./.ob-jupyter/81fb4d89fdc56b4e19e0c8e887899f079463af9c.png]]
:END:


**** QC


#+BEGIN_SRC jupyter-python
<<load_libs>>
# meta data

zac_l_elev = 644
zac_u_elev = 877
zac_a_elev = 1477

datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
variable = 'z_boom'
boom_l = pd.read_csv(datapath+'preQC/zac_l_hour_boom_height.csv', parse_dates = True, index_col=0)
#boom_u = pd.read_csv(datapath+'preQC/zac_u_hour_boom_height.csv', parse_dates = True, index_col=0)
#boom_a = pd.read_csv(datapath+'preQC/zac_a_hour_boom_height.csv', parse_dates = True, index_col=0)

boom_l.plot()
#boom_u.plot()
#boom_a.plot()

#.rolling(144,center=True).mean()
#+END_SRC

#+RESULTS:
:results:
# Out [130]: 
# text/plain
: <AxesSubplot:xlabel='time'>

# text/plain
: <Figure size 432x288 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/ab5f59d73878b54be22848d9c33b27f0ab159ea1.png]]
:end:

**** The issue at zac_l from 2010 to 2011
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(1,1,figsize=(10,5))

boom_l['2009':'2012']['z_boom'].plot(ax= ax, label = 'zac_l')
ax.legend()

startday = datetime.datetime(2011,1,25)
endday = datetime.datetime(2011,5,3)
ymin,ymax = ax.get_ylim()
ax.vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')


#+END_SRC

#+RESULTS:
:results:
# Out [139]: 


# text/plain
: <Figure size 720x360 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/7b682b734ebf11b6257603780f0f850c373ba9a9.png]]
:end:


**** The issue at zac_u from 2019 to 2021
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(1,1,figsize=(10,5))

boom_u['2018':'2022']['z_boom'].plot(ax= ax, label = 'zac_u')
ax.legend()

startday = datetime.datetime(2019,6,20)
endday = datetime.datetime(2021,7,26)
ymin,ymax = ax.get_ylim()
ax.vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')


#+END_SRC

#+RESULTS:
:results:
# Out [121]: 


# text/plain
: <Figure size 720x360 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/207a96db64917e79532b8fc984c64a12629574d3.png]]
:end:

**** The issue at zac_u from 2012 to 2016
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(1,1,figsize=(10,5))

boom_u['2012':'2020']['z_boom'].plot(ax= ax, label = 'zac_u')
ax.legend()

startday = datetime.datetime(2012,1,1)
endday = datetime.datetime(2016,4,20)
ymin,ymax = ax.get_ylim()
ax.vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')


#+END_SRC

#+RESULTS:
:results:
# Out [97]: 


# text/plain
: <Figure size 720x360 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/c0a57fd815ac60a427f3f133d5e4c3abdaca6c95.png]]
:end:

**** The issue at zac_a in 2012 and 2013

#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(1,1,figsize=(10,5))

boom_a['2012':'2013']['z_boom'].plot(ax= ax, label = 'zac_a')
ax.legend()

startday = datetime.datetime(2012,4,16)
endday = datetime.datetime(2013,8,28)
ymin,ymax = ax.get_ylim()
ax.vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')


#+END_SRC

#+RESULTS:
:results:
# Out [25]: 


# text/plain
: <Figure size 720x360 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/578d4a062859b4162dc690910adf1f7c23c41d07.png]]
:end:


**** The issue at zac_a in dec 2013 to April 2014
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(1,1,figsize=(10,5))

boom_a['December-2013':'April-2014']['z_boom'].plot(ax= ax, label = 'zac_a')
ax.legend()

startday = datetime.datetime(2013,12,20)
endday = datetime.datetime(2014,4,22)
ymin,ymax = ax.get_ylim()
ax.vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')


#+END_SRC

#+RESULTS:
:results:
# Out [49]: 


# text/plain
: <Figure size 720x360 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/e4e83da8e1338559b3d7be496e974d6786ef3ff1.png]]
:end:


**** The issue at zac_a in Jan 2015 to April 2016
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(1,1,figsize=(10,5))

boom_a['December-2014':'April-2016']['z_boom'].plot(ax= ax, label = 'zac_a')
ax.legend()

startday = datetime.datetime(2015,1,2)
endday = datetime.datetime(2016,4,22)
ymin,ymax = ax.get_ylim()
ax.vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')


#+END_SRC

#+RESULTS:
:results:
# Out [56]: 


# text/plain
: <Figure size 720x360 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/04b3a8f41ca3a7d8f60d4b1e7b26959ead03262e.png]]
:end:

**** The issue at zac_a in April 2018 
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(1,1,figsize=(10,5))

boom_a['April-2018':'May-2018']['z_boom'].plot(ax= ax, label = 'zac_a')
ax.legend()

startday = datetime.datetime(2018,4,1)
endday = datetime.datetime(2018,4,23)
ymin,ymax = ax.get_ylim()
ax.vlines([startday,endday], ymin,ymax, linestyle = '--', color = 'gray')


#+END_SRC

#+RESULTS:
:results:
# Out [78]: 


# text/plain
: <Figure size 720x360 with 1 Axes>

# image/png
[[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/b757155742396f8ff1a51feb4866c8b5f8a6c636.png]]
:end:





** Utilities


#+NAME: data_file_paths
#+BEGIN_SRC jupyter-python
station = 'zac_l'
filename = 'zac_l-2008-2022.nc'
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/'+station+'/'
zac_l_path = datapath+filename

station = 'zac_u'
filename = 'zac_u-2008-2022.nc'
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/'+station+'/'
zac_u_path = datapath+filename

station = 'zac_a'
filename = 'zac_a-2009-2020.nc'
datapath = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/L1/'+station+'/'
zac_a_path = datapath+filename
#+END_SRC

#+NAME: Load_transmitted_data
#+BEGIN_SRC jupyter-python
datadir = '/home/shl@geus.dk/OneDrive/projects/glaciobasis/data/aws_transmitted/'
station = 'zac_a'
filename = 'AWS_300234061218540.txt'
diagnostic = 'AWS_300234061218540-D.txt'
transmitted = pd.read_csv(datadir+station+'/'+filename,header=0,skiprows=[1,2,3],sep=',',engine='python')
transmitted.index = pd.to_datetime(transmitted[' timestamp'])
transmitted = transmitted.drop(' timestamp', axis = 1)
diag = pd.read_csv(datadir+station+'/'+diagnostic,header=0,skiprows=[1],sep=',',engine='python')
#print(data.keys().tolist())
#print(diag.keys())



#+END_SRC

#+NAME: load_transmitted_trusted
#+BEGIN_SRC jupyter-python
<<Load_transmitted_data>>
transmitted = transmitted[:'2019-August-30'].astype(float)


# The transmitted data is divided into timestamps so 
timestep = transmitted.index[1:]-transmitted.index[0:-1]

timestep_hour = pd.DataFrame(timestep.components.hours)
timestep_hour.index =  transmitted.index[0:-1]
transmitted_hour = transmitted[0:-1][(timestep_hour==1).values]

timestep_day = timestep.components.days
timestep_day.index =  transmitted.index[0:-1]
transmitted_day = transmitted[0:-1][(timestep_day==1).values]


#+END_SRC

#+RESULTS: load_transmitted_trusted
:results:
# Out [29]: 
# output
                     seconds_since_1990  airpressure  temperature  \
 timestamp                                                          
2019-05-29 17:00:00         927997200.0        849.0       -4.244   
2019-05-29 18:00:00         928000800.0        849.0       -4.488   
2019-05-29 19:00:00         928004400.0        849.0       -4.897   
2019-05-29 20:00:00         928008000.0        849.0       -4.805   
2019-05-29 21:00:00         928011600.0        849.0       -5.417   
...                                 ...          ...          ...   
2019-08-30 19:00:00         936039600.0        838.0       -4.746   
2019-08-30 20:00:00         936043200.0        838.0       -4.402   
2019-08-30 21:00:00         936046800.0        838.0       -4.266   
2019-08-30 22:00:00         936050400.0        839.0       -4.162   
2019-08-30 23:00:00         936054000.0        839.0       -4.151   

                     temperature2  relativehumidity  windspeed_1  windspeed_2  \
 timestamp                                                                      
2019-05-29 17:00:00        -4.209              87.3        5.432        204.7   
2019-05-29 18:00:00        -4.436              90.0        4.688        200.3   
2019-05-29 19:00:00        -4.853              94.6        3.992        211.9   
2019-05-29 20:00:00        -4.852              91.1        5.808        193.0   
2019-05-29 21:00:00        -5.425              95.8        5.895        192.9   
...                           ...               ...          ...          ...   
2019-08-30 19:00:00        -4.967              99.8        6.051        193.7   
2019-08-30 20:00:00        -4.688              99.8        6.582        202.7   
2019-08-30 21:00:00        -4.525              99.8        7.606        205.9   
2019-08-30 22:00:00        -4.409              99.8        7.408        204.1   
2019-08-30 23:00:00        -4.382              99.8        6.989        202.0   

                     shortwaveradiationin  shortwaveradiationout  \
 timestamp                                                         
2019-05-29 17:00:00               445.300                426.300   
2019-05-29 18:00:00               358.600                345.600   
2019-05-29 19:00:00               287.100                273.000   
2019-05-29 20:00:00               408.900                361.500   
2019-05-29 21:00:00               280.700                254.900   
...                                   ...                    ...   
2019-08-30 19:00:00                16.610                 15.440   
2019-08-30 20:00:00                12.710                 12.430   
2019-08-30 21:00:00                 6.910                  5.350   
2019-08-30 22:00:00                 3.009                  1.059   
2019-08-30 23:00:00                 2.173                  0.111   

                     longwaveradiationin  ...  temperature.4  temperature2.3  \
 timestamp                                ...                                  
2019-05-29 17:00:00              -15.400  ...            NaN             NaN   
2019-05-29 18:00:00              -14.520  ...            NaN             NaN   
2019-05-29 19:00:00              -17.320  ...            NaN             NaN   
2019-05-29 20:00:00              -29.970  ...            NaN             NaN   
2019-05-29 21:00:00              -45.900  ...            NaN             NaN   
...                                  ...  ...            ...             ...   
2019-08-30 19:00:00               -1.444  ...            NaN             NaN   
2019-08-30 20:00:00               -2.300  ...            NaN             NaN   
2019-08-30 21:00:00               -1.259  ...            NaN             NaN   
2019-08-30 22:00:00               -0.890  ...            NaN             NaN   
2019-08-30 23:00:00               -0.923  ...            NaN             NaN   

                     relativehumidity.4  windspeed_1.2  windspeed_2.1  \
 timestamp                                                              
2019-05-29 17:00:00                 NaN            NaN            NaN   
2019-05-29 18:00:00                 NaN            NaN            NaN   
2019-05-29 19:00:00                 NaN            NaN            NaN   
2019-05-29 20:00:00                 NaN            NaN            NaN   
2019-05-29 21:00:00                 NaN            NaN            NaN   
...                                 ...            ...            ...   
2019-08-30 19:00:00                 NaN            NaN            NaN   
2019-08-30 20:00:00                 NaN            NaN            NaN   
2019-08-30 21:00:00                 NaN            NaN            NaN   
2019-08-30 22:00:00                 NaN            NaN            NaN   
2019-08-30 23:00:00                 NaN            NaN            NaN   

                     shortwaveradiationin.3  shortwaveradiationout.3  \
 timestamp                                                             
2019-05-29 17:00:00                     NaN                      NaN   
2019-05-29 18:00:00                     NaN                      NaN   
2019-05-29 19:00:00                     NaN                      NaN   
2019-05-29 20:00:00                     NaN                      NaN   
2019-05-29 21:00:00                     NaN                      NaN   
...                                     ...                      ...   
2019-08-30 19:00:00                     NaN                      NaN   
2019-08-30 20:00:00                     NaN                      NaN   
2019-08-30 21:00:00                     NaN                      NaN   
2019-08-30 22:00:00                     NaN                      NaN   
2019-08-30 23:00:00                     NaN                      NaN   

                     longwaveradiationin.3  longwaveradiationout.3  \
 timestamp                                                           
2019-05-29 17:00:00                    NaN                     NaN   
2019-05-29 18:00:00                    NaN                     NaN   
2019-05-29 19:00:00                    NaN                     NaN   
2019-05-29 20:00:00                    NaN                     NaN   
2019-05-29 21:00:00                    NaN                     NaN   
...                                    ...                     ...   
2019-08-30 19:00:00                    NaN                     NaN   
2019-08-30 20:00:00                    NaN                     NaN   
2019-08-30 21:00:00                    NaN                     NaN   
2019-08-30 22:00:00                    NaN                     NaN   
2019-08-30 23:00:00                    NaN                     NaN   

                     temperatureradsensor.2  
 timestamp                                   
2019-05-29 17:00:00                     NaN  
2019-05-29 18:00:00                     NaN  
2019-05-29 19:00:00                     NaN  
2019-05-29 20:00:00                     NaN  
2019-05-29 21:00:00                     NaN  
...                                     ...  
2019-08-30 19:00:00                     NaN  
2019-08-30 20:00:00                     NaN  
2019-08-30 21:00:00                     NaN  
2019-08-30 22:00:00                     NaN  
2019-08-30 23:00:00                     NaN  

[2239 rows x 135 columns]

:end:


#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(2,1,figsize=(10,5))
ax[0].plot(transmitted.index[1:], timestep_hour )
ax[0].plot(transmitted.index[1:], timestep_day )
ax[1].plot(transmitted_hour['temperature'])
ax[1].plot(transmitted_day['temperature'])


   #+END_SRC

   #+RESULTS:
   :results:
   # Out [4]: 
   # text/plain
   : [<matplotlib.lines.Line2D at 0x7f3b78b61550>]

   # text/plain
   : <Figure size 720x360 with 2 Axes>

   # image/png
   [[file:obipy-resources/9e526126f6e919503e0f2bfb2c43391732b7318e/4479d71f944b6501c2709a338cb7265a0ea16930.png]]
   :end:


#+END_SRC

#+RESULTS: Load_transmitted_data
:results:
# Out [27]: 
:end:

** Pre 2022 database delivery

*** Near surface climate

 #+BEGIN_SRC jupyter-python
 import xarray as xr

 destdir = '/home/shl@geus.dk/OneDrive/projects/glaciobasis/gem_database/data/2022/'

 filename = 'data_v1.0/L1/zac_l/zac_l-2008-2021.nc'
 destname = 'GlacioBasis_Zackenberg_Near_surface_weather_AP_Olsen_AWS_Zack_M'
 <<extract_the_near_surface_climate_columns>>
 data_to_file.to_csv(destdir+destname+'.txt',sep = '\t', index = False, float_format = '%g')

 filename = 'data_v1.0/L1/zac_u/zac_u-2008-2021.nc'
 destname = 'GlacioBasis_Zackenberg_Near_surface_weather_AP_Olsen_AWS_Zack_S'
 <<extract_the_near_surface_climate_columns>>
 data_to_file.to_csv(destdir+destname+'.txt',sep = '\t', index = False, float_format = '%g')

 filename = 'data_v1.0/L1/zac_a/zac_a-2009-2020.nc'
 destname = 'GlacioBasis_Zackenberg_Near_surface_weather_AP_Olsen_AWS_Zack_T'
 <<extract_the_near_surface_climate_columns>>
 data_to_file.to_csv(destdir+destname+'.txt',sep = '\t', index = False, float_format = '%g')
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [2]: 
 :end:



 #+NAME: extract_the_near_surface_climate_columns
 #+BEGIN_SRC jupyter-python
 with xr.open_dataset(filename) as ds:
     data_ds = ds[['p','t_1','rh','wspd','wdir','dsr_corr','usr_corr','dlr','ulr','t_rad','tilt_x','tilt_y']]

 data = data_ds.to_dataframe()
 data['datetime'] = data.index
 data.index.name = 'datetime1'
 data["date"] = data["datetime"].dt.date
 data["time"] = data["datetime"].dt.time

 data.rename(columns = {'p':'p_atm','t_1':'T_air','rh':'RH_water','wspd':'ws','wdir':'wd','dsr_corr':'SW_in','usr_corr':'SW_out','dlr':'LW_in','ulr':'LW_out'}, inplace = True)

 data_to_file = data[['date','time','p_atm', 'T_air', 'RH_water', 'ws','wd','SW_in','SW_out','LW_in','LW_out']]

 #+END_SRC


*** Snow sonic ranger

 date	time	h_snow_M	h_snow_M_qual	h_snow_S	h_snow_S_qual	h_snow_T	h_snow_T_qual
 GlacioBasis_Zackenberg_Snow_cover_AP_Olsen_Snow_sonic_ranger_height.txt
 z_boom
 z_boom_q

 #+BEGIN_SRC jupyter-python
 import xarray as xr
 import pandas as pd

 destdir = '/home/shl@geus.dk/OneDrive/projects/glaciobasis/gem_database/data/2022/'

 filename = 'data_v1.0/L1/zac_l/zac_l-2008-2021.nc'
 destname = 'GlacioBasis_Zackenberg_Snow_cover_AP_Olsen_Snow_sonic_ranger_height'
 with xr.open_dataset(filename) as ds:
     data_ds = ds[['z_boom','z_boom_q']]

 data = data_ds.to_dataframe()
 data['datetime'] = pd.to_datetime(data.index)
 data_M = data.rename(columns = {'z_boom':'h_snow_M','z_boom_q':'h_snow_M_qual'})


 filename = 'data_v1.0/L1/zac_u/zac_u-2008-2021.nc'
 with xr.open_dataset(filename) as ds:
     data_ds = ds[['z_boom','z_boom_q']]

 data = data_ds.to_dataframe()
 data['datetime'] = pd.to_datetime(data.index)
 data_S = data.rename(columns = {'z_boom':'h_snow_S','z_boom_q':'h_snow_S_qual'})

 filename = 'data_v1.0/L1/zac_a/zac_a-2009-2020.nc'
 with xr.open_dataset(filename) as ds:
     data_ds = ds[['z_boom','z_boom_q']]

 data = data_ds.to_dataframe()
 data['datetime'] = pd.to_datetime(data.index)
 data_T = data.rename(columns = {'z_boom':'h_snow_T','z_boom_q':'h_snow_T_qual'})

 data_to_file = pd.merge(data_M,data_T, on = 'datetime', how = 'outer')
 data_to_file = pd.merge(data_to_file,data_S,on = 'datetime', how = 'outer')
 data_to_file["date"] = data_to_file["datetime"].dt.date
 data_to_file["time"] = data_to_file["datetime"].dt.time
 data_to_file = data_to_file[['date','time','h_snow_M','h_snow_M_qual','h_snow_S','h_snow_S_qual','h_snow_T','h_snow_T_qual']]
 #data_to_file = data_M.merge(data_S, on = 'datetime', how = 'outer').merge(data_T,on='datetime', how='outer')

 data_to_file.to_csv(destdir+destname+'.txt',sep = '\t', index = False, float_format = '%g')
 #+END_SRC

 #+RESULTS:
 :results:
 # Out [17]: 
 :end:



 #+NAME: extract_the_SR50_snow_height
 #+BEGIN_SRC jupyter-python
 with xr.open_dataset(filename) as ds:
     data_ds = ds[['z_boom','z_boom_q']]

 data = data_ds.to_dataframe()
 data['datetime'] = data.index
 data.index.name = 'datetime1'
 data["date"] = data["datetime"].dt.date
 data["time"] = data["datetime"].dt.time



 #+END_SRC

 
* Assemble variables in one netcdf and one csv per site

** Pre QC (supplementary)

#+BEGIN_SRC jupyter-python
import pandas as pd
import xarray as xr
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rcParams
import matplotlib.dates as mdates

rcParams.update({'font.size': 8})
workingfolder = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/preQC/'

#zac_l
site = 'zac_l'
temp = pd.read_csv(workingfolder + site + '_hour_temperature.csv', index_col = 0)
rh = pd.read_csv(workingfolder + site + '_hour_relative_humidity.csv', index_col = 0)
p = pd.read_csv(workingfolder + site + '_hour_pressure.csv', index_col = 0)
rad = pd.read_csv(workingfolder + site + '_hour_radiation.csv', index_col = 0)
ws = pd.read_csv(workingfolder + site + '_hour_wind_speed.csv', index_col = 0)
height = pd.read_csv(workingfolder + site + '_hour_boom_height.csv', index_col = 0)
#surf = pd.read_csv(workingfolder + site + '_hour_lowering.csv', index_col = 0)
ablation = pd.read_csv(workingfolder + site + '_hour_ice_ablation.csv', index_col = 0)

merged = temp.join(rh, how='outer').join(p, how='outer').join(rad, how='outer').join(ws, how='outer').join(height, how='outer').join(ablation, how='outer')

merged.rename(columns = {'t_1':'t_u', 'rh': 'rh_u_corr', 'p': 'p_u', 'dsr':'dsr', 'usr':'usr','dsr_corr':'dsr_corr', 'usr_corr':'usr_corr', 'dlr':'dlr', 'ulr':'ulr','cloud_cov':'cloud_cover','t_surf':'t_surf', 'wspd':'wspd', 'z_boom':'z_boom', 'ice_ablation':'ice_ablation' }, inplace = True)
#merged.plot(subplots = True, figsize = (12,12))

cols = ['t_u', 'rh_u_corr', 'p_u', 'dsr_corr', 'usr_corr', 'dlr','ulr', 'wspd', 'z_boom','ice_ablation']
<<drop_head_and_tail_nans_fast>>
merged_slim = drop_nan_rows(merged)

ax = merged[cols].plot(subplots = True, figsize = (6.5,5))

merged.index = pd.to_datetime(merged.index)
years = range(merged.index.year.min(), merged.index.year.max() + 1)

#ax[0].set_xticks([pd.to_datetime(str(year) + '-01-01') for year in years])

#ax[0].set_xticklabels(years)


ax[0].set_ylabel('$\circ$C')
ax[1].set_ylabel('%')
ax[2].set_ylabel('hPa')
ax[3].set_ylabel('W/m²')
ax[4].set_ylabel('W/m²')
ax[5].set_ylabel('W/m²')
ax[6].set_ylabel('W/m²')
ax[7].set_ylabel('m/s')
ax[8].set_ylabel('m')
ax[9].set_ylabel('m')

ax[0].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[1].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[2].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[3].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[4].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[5].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[6].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[7].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[8].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[9].legend(loc='upper left', bbox_to_anchor=(1, 1))

#plt.tight_layout()
plt.savefig('ZAC_L_Measurement_success_rate.png', dpi = 300, bbox_inches='tight')
merged_slim.to_csv('/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'+site+'_2008_2022_preQC.csv', index = True)
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/3627e560f67f1f519357021894d2fd356b85da14.png]]


#+BEGIN_SRC jupyter-python
import pandas as pd
import xarray as xr
import numpy as np

workingfolder = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/preQC/'

#zac_u
site = 'zac_u'
temp = pd.read_csv(workingfolder + site + '_hour_temperature.csv', index_col = 0)
rh = pd.read_csv(workingfolder + site + '_hour_relative_humidity.csv', index_col = 0)
p = pd.read_csv(workingfolder + site + '_hour_pressure.csv', index_col = 0)
rad = pd.read_csv(workingfolder + site + '_hour_radiation.csv', index_col = 0)
ws = pd.read_csv(workingfolder + site + '_hour_wind_speed.csv', index_col = 0)
height = pd.read_csv(workingfolder + site + '_hour_boom_height.csv', index_col = 0)
#surf = pd.read_csv(workingfolder + site + '_hour_lowering.csv', index_col = 0)
ablation = pd.read_csv(workingfolder + site + '_hour_ice_ablation.csv', index_col = 0)

merged = temp.join(rh, how='outer').join(p, how='outer').join(rad, how='outer').join(ws, how='outer').join(height, how='outer').join(ablation, how='outer')

merged.rename(columns = {'t_1':'t_u', 'rh': 'rh_u_corr', 'p': 'p_u', 'dsr':'dsr', 'usr':'usr','dsr_corr':'dsr_corr', 'usr_corr':'usr_corr', 'dlr':'dlr', 'ulr':'ulr','cloud_cov':'cloud_cover','t_surf':'t_surf', 'wspd':'wspd', 'z_boom':'z_boom', 'ice_ablation':'ice_ablation' }, inplace = True)
#merged.plot(subplots = True, figsize = (12,12))

cols = ['t_u', 'rh_u_corr', 'p_u', 'dsr_corr', 'usr_corr', 'dlr','ulr', 'wspd', 'z_boom','ice_ablation']

<<drop_head_and_tail_nans_fast>>

merged_slim = drop_nan_rows(merged)

ax = merged[cols].plot(subplots = True, figsize = (6.5,5))

merged.index = pd.to_datetime(merged.index)
years = range(merged.index.year.min(), merged.index.year.max() + 1)

#ax[0].set_xticks([pd.to_datetime(str(year) + '-01-01') for year in years])

#ax[0].set_xticklabels(years)


ax[0].set_ylabel('$\circ$C')
ax[1].set_ylabel('%')
ax[2].set_ylabel('hPa')
ax[3].set_ylabel('W/m²')
ax[4].set_ylabel('W/m²')
ax[5].set_ylabel('W/m²')
ax[6].set_ylabel('W/m²')
ax[7].set_ylabel('m/s')
ax[8].set_ylabel('m')
ax[9].set_ylabel('m')

ax[0].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[1].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[2].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[3].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[4].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[5].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[6].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[7].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[8].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[9].legend(loc='upper left', bbox_to_anchor=(1, 1))


plt.savefig('ZAC_U_Measurement_success_rate.png', dpi = 300, bbox_inches='tight')

#merged.plot(subplots = True, figsize = (12,12))
merged_slim.to_csv('/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'+site+'_2008_2022_preQC.csv', index = True)
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/087a1c067b6448ddbd3d753c4ccc390f9ec4d63a.png]]

#+BEGIN_SRC jupyter-python
workingfolder = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/preQC/'
#zac_a
site = 'zac_a'
temp = pd.read_csv(workingfolder + site + '_hour_temperature.csv', index_col = 0)
rh = pd.read_csv(workingfolder + site + '_hour_relative_humidity.csv', index_col = 0)
p = pd.read_csv(workingfolder + site + '_hour_pressure.csv', index_col = 0)
rad = pd.read_csv(workingfolder + site + '_hour_radiation.csv', index_col = 0)
ws = pd.read_csv(workingfolder + site + '_hour_wind_speed.csv', index_col = 0)
height = pd.read_csv(workingfolder + site + '_hour_boom_height.csv', index_col = 0)


merged = temp.join(rh, how='outer').join(p, how='outer').join(rad, how='outer').join(ws, how='outer').join(height, how='outer')

merged.rename(columns = {'t_1':'t_u', 'rh': 'rh_u_corr', 'p': 'p_u', 'dsr':'dsr', 'usr':'usr','dsr_corr':'dsr_corr', 'usr_corr':'usr_corr', 'dlr':'dlr', 'ulr':'ulr','cloud_cov':'cloud_cover','t_surf':'t_surf', 'wspd':'wspd', 'z_boom':'z_boom' }, inplace = True)
#merged.plot(subplots = True, figsize = (12,12))

cols = ['t_u', 'rh_u_corr', 'p_u', 'dsr_corr', 'usr_corr', 'dlr','ulr', 'wspd', 'z_boom']

<<drop_head_and_tail_nans_fast>>
merged_slim = drop_nan_rows(merged)

ax = merged[cols].plot(subplots = True, figsize = (6.5,5))

merged.index = pd.to_datetime(merged.index)
years = range(merged.index.year.min(), merged.index.year.max() + 1)

#ax[0].set_xticks([pd.to_datetime(str(year) + '-01-01') for year in years])

#ax[0].set_xticklabels(years)


ax[0].set_ylabel('$\circ$C')
ax[1].set_ylabel('%')
ax[2].set_ylabel('hPa')
ax[3].set_ylabel('W/m²')
ax[4].set_ylabel('W/m²')
ax[5].set_ylabel('W/m²')
ax[6].set_ylabel('W/m²')
ax[7].set_ylabel('m/s')
ax[8].set_ylabel('m')
#ax[9].set_ylabel('m')

ax[0].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[1].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[2].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[3].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[4].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[5].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[6].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[7].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[8].legend(loc='upper left', bbox_to_anchor=(1, 1))
#ax[9].legend(loc='upper left', bbox_to_anchor=(1, 1))


plt.savefig('ZAC_A_Measurement_success_rate.png', dpi = 300, bbox_inches='tight')


#merged.plot(subplots = True, figsize = (12,12))
merged_slim.to_csv('/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'+site+'_2008_2022_preQC.csv', index = True)


#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/25f29905363ddca983fc78c0c7933614dc93a842.png]]


#+NAME: drop_head_and_tail_nans_fast
#+BEGIN_SRC jupyter-python

def drop_nan_rows(df):
    # Create mask of rows where all columns are NaN
    mask = df[cols].isna().all(axis=1)

    # Get the indices where mask is True (all NaN)
    true_indices = np.where(mask)[0]

    # If there are no all-NaN rows, just return the original DataFrame
    if len(true_indices) == 0:
        return df

    # Find the first and last True value in the mask
    first_true = true_indices[0]
    last_true = true_indices[-1]

    # Drop the rows from the start to the first non-NaN row
    # and from the last non-NaN row to the end
    df = df.iloc[first_true+1:last_true]

    return df



#+END_SRC



** Post QC

#+BEGIN_SRC jupyter-python
import pandas as pd
import xarray as xr
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rcParams
import matplotlib.dates as mdates

rcParams.update({'font.size': 8})
workingfolder = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

#zac_l
site = 'zac_l'
temp = pd.read_csv(workingfolder + site + '_hour_temperature.csv', index_col = 0)
rh = pd.read_csv(workingfolder + site + '_hour_relative_humidity.csv', index_col = 0)
p = pd.read_csv(workingfolder + site + '_hour_pressure.csv', index_col = 0)
rad = pd.read_csv(workingfolder + site + '_hour_radiation.csv', index_col = 0)
ws = pd.read_csv(workingfolder + site + '_hour_wind_speed.csv', index_col = 0)
height = pd.read_csv(workingfolder + site + '_hour_boom_height.csv', index_col = 0)
#surf = pd.read_csv(workingfolder + site + '_hour_lowering.csv', index_col = 0)
ablation = pd.read_csv(workingfolder + site + '_hour_ice_ablation.csv', index_col = 0)

merged = temp.join(rh, how='outer').join(p, how='outer').join(rad, how='outer').join(ws, how='outer').join(height, how='outer').join(ablation, how='outer')

merged.rename(columns = {'t_1':'t_u', 'rh': 'rh_u_corr', 'p': 'p_u', 'dsr':'dsr', 'usr':'usr','dsr_corr':'dsr_corr', 'usr_corr':'usr_corr', 'dlr':'dlr', 'ulr':'ulr','cloud_cov':'cloud_cover','t_surf':'t_surf', 'wspd':'wspd', 'z_boom':'z_boom', 'ice_ablation':'ice_ablation' }, inplace = True)
#merged.plot(subplots = True, figsize = (12,12))

cols = ['t_u', 'rh_u_corr', 'p_u', 'dsr_corr', 'usr_corr', 'dlr','ulr', 'wspd', 'z_boom','ice_ablation']
<<drop_head_and_tail_nans_fast>>
merged_slim = drop_nan_rows(merged)

ax = merged[cols].plot(subplots = True, figsize = (6.5,5))

merged.index = pd.to_datetime(merged.index)
years = range(merged.index.year.min(), merged.index.year.max() + 1)

#ax[0].set_xticks([pd.to_datetime(str(year) + '-01-01') for year in years])

#ax[0].set_xticklabels(years)


ax[0].set_ylabel('$\circ$C')
ax[1].set_ylabel('%')
ax[2].set_ylabel('hPa')
ax[3].set_ylabel('W/m²')
ax[4].set_ylabel('W/m²')
ax[5].set_ylabel('W/m²')
ax[6].set_ylabel('W/m²')
ax[7].set_ylabel('m/s')
ax[8].set_ylabel('m')
ax[9].set_ylabel('m')

ax[0].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[1].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[2].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[3].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[4].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[5].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[6].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[7].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[8].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[9].legend(loc='upper left', bbox_to_anchor=(1, 1))

#plt.tight_layout()
plt.savefig('ZAC_L_Measurement_success_rate.png', dpi = 300, bbox_inches='tight')
merged_slim.to_csv('/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'+site+'_2008_2022_QC_final.csv', index = True)
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/02073ee68a3cd38487918e64dcc4fe59cbb442ea.png]]


#+BEGIN_SRC jupyter-python
import pandas as pd
import xarray as xr
import numpy as np

workingfolder = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'

#zac_u
site = 'zac_u'
temp = pd.read_csv(workingfolder + site + '_hour_temperature.csv', index_col = 0)
rh = pd.read_csv(workingfolder + site + '_hour_relative_humidity.csv', index_col = 0)
p = pd.read_csv(workingfolder + site + '_hour_pressure.csv', index_col = 0)
rad = pd.read_csv(workingfolder + site + '_hour_radiation.csv', index_col = 0)
ws = pd.read_csv(workingfolder + site + '_hour_wind_speed.csv', index_col = 0)
height = pd.read_csv(workingfolder + site + '_hour_boom_height.csv', index_col = 0)
#surf = pd.read_csv(workingfolder + site + '_hour_lowering.csv', index_col = 0)
ablation = pd.read_csv(workingfolder + site + '_hour_ice_ablation.csv', index_col = 0)

merged = temp.join(rh, how='outer').join(p, how='outer').join(rad, how='outer').join(ws, how='outer').join(height, how='outer').join(ablation, how='outer')

merged.rename(columns = {'t_1':'t_u', 'rh': 'rh_u_corr', 'p': 'p_u', 'dsr':'dsr', 'usr':'usr','dsr_corr':'dsr_corr', 'usr_corr':'usr_corr', 'dlr':'dlr', 'ulr':'ulr','cloud_cov':'cloud_cover','t_surf':'t_surf', 'wspd':'wspd', 'z_boom':'z_boom', 'ice_ablation':'ice_ablation' }, inplace = True)
#merged.plot(subplots = True, figsize = (12,12))

cols = ['t_u', 'rh_u_corr', 'p_u', 'dsr_corr', 'usr_corr', 'dlr','ulr', 'wspd', 'z_boom','ice_ablation']

<<drop_head_and_tail_nans_fast>>

merged_slim = drop_nan_rows(merged)

ax = merged[cols].plot(subplots = True, figsize = (6.5,5))

merged.index = pd.to_datetime(merged.index)
years = range(merged.index.year.min(), merged.index.year.max() + 1)

#ax[0].set_xticks([pd.to_datetime(str(year) + '-01-01') for year in years])

#ax[0].set_xticklabels(years)


ax[0].set_ylabel('$\circ$C')
ax[1].set_ylabel('%')
ax[2].set_ylabel('hPa')
ax[3].set_ylabel('W/m²')
ax[4].set_ylabel('W/m²')
ax[5].set_ylabel('W/m²')
ax[6].set_ylabel('W/m²')
ax[7].set_ylabel('m/s')
ax[8].set_ylabel('m')
ax[9].set_ylabel('m')

ax[0].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[1].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[2].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[3].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[4].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[5].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[6].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[7].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[8].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[9].legend(loc='upper left', bbox_to_anchor=(1, 1))


plt.savefig('ZAC_U_Measurement_success_rate.png', dpi = 300, bbox_inches='tight')

#merged.plot(subplots = True, figsize = (12,12))
merged_slim.to_csv('/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'+site+'_2008_2022_QC_final.csv', index = True)
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/7af423e309624bc1990d2afa0f255af133530ca2.png]]

#+BEGIN_SRC jupyter-python
workingfolder = '/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'
#zac_a
site = 'zac_a'
temp = pd.read_csv(workingfolder + site + '_hour_temperature.csv', index_col = 0)
rh = pd.read_csv(workingfolder + site + '_hour_relative_humidity.csv', index_col = 0)
p = pd.read_csv(workingfolder + site + '_hour_pressure.csv', index_col = 0)
rad = pd.read_csv(workingfolder + site + '_hour_radiation.csv', index_col = 0)
ws = pd.read_csv(workingfolder + site + '_hour_wind_speed.csv', index_col = 0)
height = pd.read_csv(workingfolder + site + '_hour_boom_height.csv', index_col = 0)


merged = temp.join(rh, how='outer').join(p, how='outer').join(rad, how='outer').join(ws, how='outer').join(height, how='outer')

merged.rename(columns = {'t_1':'t_u', 'rh': 'rh_u_corr', 'p': 'p_u', 'dsr':'dsr', 'usr':'usr','dsr_corr':'dsr_corr', 'usr_corr':'usr_corr', 'dlr':'dlr', 'ulr':'ulr','cloud_cov':'cloud_cover','t_surf':'t_surf', 'wspd':'wspd', 'z_boom':'z_boom' }, inplace = True)
#merged.plot(subplots = True, figsize = (12,12))

cols = ['t_u', 'rh_u_corr', 'p_u', 'dsr_corr', 'usr_corr', 'dlr','ulr', 'wspd', 'z_boom']

<<drop_head_and_tail_nans_fast>>
merged_slim = drop_nan_rows(merged)

ax = merged[cols].plot(subplots = True, figsize = (6.5,5))

merged.index = pd.to_datetime(merged.index)
years = range(merged.index.year.min(), merged.index.year.max() + 1)

#ax[0].set_xticks([pd.to_datetime(str(year) + '-01-01') for year in years])

#ax[0].set_xticklabels(years)


ax[0].set_ylabel('$\circ$C')
ax[1].set_ylabel('%')
ax[2].set_ylabel('hPa')
ax[3].set_ylabel('W/m²')
ax[4].set_ylabel('W/m²')
ax[5].set_ylabel('W/m²')
ax[6].set_ylabel('W/m²')
ax[7].set_ylabel('m/s')
ax[8].set_ylabel('m')
#ax[9].set_ylabel('m')

ax[0].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[1].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[2].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[3].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[4].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[5].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[6].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[7].legend(loc='upper left', bbox_to_anchor=(1, 1))
ax[8].legend(loc='upper left', bbox_to_anchor=(1, 1))
#ax[9].legend(loc='upper left', bbox_to_anchor=(1, 1))


plt.savefig('ZAC_A_Measurement_success_rate.png', dpi = 300, bbox_inches='tight')


#merged.plot(subplots = True, figsize = (12,12))
merged_slim.to_csv('/home/shl@geus.dk/OneDrive/projects/aws_processing_v1.0/data_v1.0/gem_database/2022/'+site+'_2008_2022_QC_final', index = True)


#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/8c922b511c864247818b2907b40f46ee7c1b324d.png]]


